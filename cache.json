{"2024-10-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.11962v3","updated":"2024-10-01T17:50:31Z","published":"2024-08-21T19:31:01Z","title":"Characterizing Online Toxicity During the 2022 Mpox Outbreak: A\n  Computational Analysis of Topical and Network Dynamics","summary":"  Background: Online toxicity, encompassing behaviors such as harassment,\nbullying, hate speech, and the dissemination of misinformation, has become a\npressing social concern in the digital age. The 2022 Mpox outbreak, initially\ntermed \"Monkeypox\" but subsequently renamed to mitigate associated stigmas and\nsocietal concerns, serves as a poignant backdrop to this issue. Objective: In\nthis research, we undertake a comprehensive analysis of the toxic online\ndiscourse surrounding the 2022 Mpox outbreak. Our objective is to dissect its\norigins, characterize its nature and content, trace its dissemination patterns,\nand assess its broader societal implications, with the goal of providing\ninsights that can inform strategies to mitigate such toxicity in future crises.\nMethods: We collected more than 1.6 million unique tweets and analyzed them\nfrom five dimensions, including context, extent, content, speaker, and intent.\nUtilizing BERT-based topic modeling and social network community clustering, we\ndelineated the toxic dynamics on Twitter. Results: We identified five\nhigh-level topic categories in the toxic online discourse on Twitter, including\ndisease (46.6%), health policy and healthcare (19.3%), homophobia (23.9%),\npolitics (6.0%), and racism (4.1%). Through the toxicity diffusion networks of\nmentions, retweets, and the top users, we found that retweets of toxic content\nwere widespread, while influential users rarely engaged with or countered this\ntoxicity through retweets. Conclusions: By tracking topical dynamics, we can\ntrack the changing popularity of toxic content online, providing a better\nunderstanding of societal challenges. Network dynamics spotlight key social\nmedia influencers and their intents, indicating that addressing these central\nfigures in toxic discourse can enhance crisis communication and inform\npolicy-making.\n","authors":["Lizhou Fan","Lingyao Li","Libby Hemphill"],"pdf_url":"https://arxiv.org/pdf/2408.11962v3.pdf","comment":"36 pages, 8 figure, and 12 tables"},{"id":"http://arxiv.org/abs/2409.12618v2","updated":"2024-10-01T17:50:25Z","published":"2024-09-19T09:44:17Z","title":"Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning","summary":"  Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.\n","authors":["Santosh Kumar Radha","Yasamin Nouri Jelyani","Ara Ghukasyan","Oktay Goktas"],"pdf_url":"https://arxiv.org/pdf/2409.12618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18924v2","updated":"2024-10-01T17:49:00Z","published":"2024-09-27T17:17:15Z","title":"AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow","summary":"  Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.\n","authors":["Huizi Yu","Jiayan Zhou","Lingyao Li","Shan Chen","Jack Gallifant","Anye Shi","Xiang Li","Wenyue Hua","Mingyu Jin","Guang Chen","Yang Zhou","Zhao Li","Trisha Gupte","Ming-Li Chen","Zahra Azizi","Yongfeng Zhang","Themistocles L. Assimes","Xin Ma","Danielle S. Bitterman","Lin Lu","Lizhou Fan"],"pdf_url":"https://arxiv.org/pdf/2409.18924v2.pdf","comment":"42 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.17447v2","updated":"2024-10-01T17:39:09Z","published":"2024-07-24T17:23:18Z","title":"FLRT: Fluent Student-Teacher Redteaming","summary":"  Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. To redteam or jailbreak these\nmodels for compliance with toxic requests, users and security analysts have\ndeveloped adversarial prompting techniques. One attack method is to apply\ndiscrete optimization techniques to the prompt. However, the resulting attack\nstrings are often gibberish text, easily filtered by defenders due to high\nmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. In\nthis work, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.\n","authors":["T. Ben Thompson","Michael Sklar"],"pdf_url":"https://arxiv.org/pdf/2407.17447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01247v2","updated":"2024-10-01T17:21:28Z","published":"2024-09-02T13:29:44Z","title":"Conversational Complexity for Assessing Risk in Large Language Models","summary":"  Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm.\n","authors":["John Burden","Manuel Cebrian","Jose Hernandez-Orallo"],"pdf_url":"https://arxiv.org/pdf/2409.01247v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.12857v2","updated":"2024-10-01T17:13:38Z","published":"2024-07-09T15:06:14Z","title":"Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and\n  Analysis","summary":"  In recent years, the rapid increase in scientific papers has overwhelmed\ntraditional review mechanisms, resulting in varying quality of publications.\nAlthough existing methods have explored the capabilities of Large Language\nModels (LLMs) for automated scientific reviewing, their generated contents are\noften generic or partial. To address the issues above, we introduce an\nautomated paper reviewing framework SEA. It comprises of three modules:\nStandardization, Evaluation, and Analysis, which are represented by models\nSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills data\nstandardization capabilities of GPT-4 for integrating multiple reviews for a\npaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it to\ngenerate constructive reviews. Finally, SEA-A introduces a new evaluation\nmetric called mismatch score to assess the consistency between paper contents\nand reviews. Moreover, we design a self-correction strategy to enhance the\nconsistency. Extensive experimental results on datasets collected from eight\nvenues show that SEA can generate valuable insights for authors to improve\ntheir papers.\n","authors":["Jianxiang Yu","Zichen Ding","Jiaqi Tan","Kangyang Luo","Zhenmin Weng","Chenghua Gong","Long Zeng","Renjing Cui","Chengcheng Han","Qiushi Sun","Zhiyong Wu","Yunshi Lan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2407.12857v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.11193v2","updated":"2024-10-01T17:04:22Z","published":"2024-06-17T03:59:44Z","title":"MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in\n  Multimodal Large Language Model","summary":"  Projecting visual features into word embedding space has become a significant\nfusion strategy adopted by Multimodal Large Language Models (MLLMs). However,\nits internal mechanisms have yet to be explored. Inspired by multilingual\nresearch, we identify domain-specific neurons in multimodal large language\nmodels. Specifically, we investigate the distribution of domain-specific\nneurons and the mechanism of how MLLMs process features from diverse domains.\nFurthermore, we propose a three-stage mechanism for language model modules in\nMLLMs when handling projected image features, and verify this hypothesis using\nlogit lens. Extensive experiments indicate that while current MLLMs exhibit\nVisual Question Answering (VQA) capability, they may not fully utilize\ndomain-specific information. Manipulating domain-specific neurons properly will\nresult in a 10% change of accuracy at most, shedding light on the development\nof cross-domain, all-encompassing MLLMs in the future. The source code is\navailable at https://github.com/Z1zs/MMNeuron.\n","authors":["Jiahao Huo","Yibo Yan","Boren Hu","Yutao Yue","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2406.11193v2.pdf","comment":"Accepted by the Main Conference of Empirical Methods in Natural\n  Language Processing (EMNLP) 2024"},{"id":"http://arxiv.org/abs/2406.17328v3","updated":"2024-10-01T16:45:12Z","published":"2024-06-25T07:25:15Z","title":"Dual-Space Knowledge Distillation for Large Language Models","summary":"  Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.\n","authors":["Songming Zhang","Xue Zhang","Zengkui Sun","Yufeng Chen","Jinan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17328v3.pdf","comment":"The camera-ready version for EMNLP 2024 main conference. 17 pages, 11\n  figures, code available at: https://github.com/songmzhang/DSKD"},{"id":"http://arxiv.org/abs/2302.00667v2","updated":"2024-10-01T16:29:14Z","published":"2023-02-01T18:53:42Z","title":"Does Vision Accelerate Hierarchical Generalization in Neural Language\n  Learners?","summary":"  Neural language models (LMs) are arguably less data-efficient than humans\nfrom a language acquisition perspective. One fundamental question is why this\nhuman-LM gap arises. This study explores the advantage of grounded language\nacquisition, specifically the impact of visual information -- which humans can\nusually rely on but LMs largely do not have access to during language\nacquisition -- on syntactic generalization in LMs. Our experiments, following\nthe poverty of stimulus paradigm under two scenarios (using artificial vs.\nnaturalistic images), demonstrate that if the alignments between the linguistic\nand visual components are clear in the input, access to vision data does help\nwith the syntactic generalization of LMs, but if not, visual input does not\nhelp. This highlights the need for additional biases or signals, such as mutual\ngaze, to enhance cross-modal alignment and enable efficient syntactic\ngeneralization in multimodal LMs.\n","authors":["Tatsuki Kuribayashi","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2302.00667v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2407.06917v2","updated":"2024-10-01T15:50:06Z","published":"2024-07-09T14:52:52Z","title":"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models","summary":"  Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.\n","authors":["Zara Siddique","Liam D. Turner","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2407.06917v2.pdf","comment":"Accepted to EMNLP Main 2024"},{"id":"http://arxiv.org/abs/2305.13214v2","updated":"2024-10-01T15:48:32Z","published":"2023-05-22T16:45:50Z","title":"Atomic Inference for NLI with Generated Facts as Atoms","summary":"  With recent advances, neural models can achieve human-level performance on\nvarious natural language tasks. However, there are no guarantees that any\nexplanations from these models are faithful, i.e. that they reflect the inner\nworkings of the model. Atomic inference overcomes this issue, providing\ninterpretable and faithful model decisions. This approach involves making\npredictions for different components (or atoms) of an instance, before using\ninterpretable and deterministic rules to derive the overall prediction based on\nthe individual atom-level predictions. We investigate the effectiveness of\nusing LLM-generated facts as atoms, decomposing Natural Language Inference\npremises into lists of facts. While directly using generated facts in atomic\ninference systems can result in worse performance, with 1) a multi-stage fact\ngeneration process, and 2) a training regime that incorporates the facts, our\nfact-based method outperforms other approaches.\n","authors":["Joe Stacey","Pasquale Minervini","Haim Dubossarsky","Oana-Maria Camburu","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2305.13214v2.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.03354v3","updated":"2024-10-01T15:41:22Z","published":"2024-08-06T09:15:25Z","title":"The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums","summary":"  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence.\n","authors":["Vanessa Clairoux-Trepanier","Isa-May Beauchamp","Estelle Ruellan","Masarah Paquet-Clouston","Serge-Olivier Paquette","Eric Clay"],"pdf_url":"https://arxiv.org/pdf/2408.03354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10774v2","updated":"2024-10-01T15:39:48Z","published":"2024-04-16T17:59:10Z","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","summary":"  Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models.\n","authors":["Liyan Tang","Philippe Laban","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2404.10774v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.19346v3","updated":"2024-10-01T15:28:16Z","published":"2024-03-28T12:04:28Z","title":"Large Language Models Are Unconscious of Unreasonability in Math\n  Problems","summary":"  Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios.\n","authors":["Jingyuan Ma","Damai Dai","Lei Sha","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2403.19346v3.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.12701v2","updated":"2024-10-01T15:03:14Z","published":"2024-05-21T11:50:16Z","title":"OLAPH: Improving Factuality in Biomedical Long-form Question Answering","summary":"  In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.\n","authors":["Minbyul Jeong","Hyeon Hwang","Chanwoong Yoon","Taewhoo Lee","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2405.12701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19177v2","updated":"2024-10-01T14:44:52Z","published":"2024-09-27T23:13:17Z","title":"Evidence Is All You Need: Ordering Imaging Studies via Language Model\n  Alignment with the ACR Appropriateness Criteria","summary":"  Diagnostic imaging studies are an increasingly important component of the\nworkup and management of acutely presenting patients. However, ordering\nappropriate imaging studies according to evidence-based medical guidelines is a\nchallenging task with a high degree of variability between healthcare\nproviders. To address this issue, recent work has investigated if generative AI\nand large language models can be leveraged to help clinicians order relevant\nimaging studies for patients. However, it is challenging to ensure that these\ntools are correctly aligned with medical guidelines, such as the American\nCollege of Radiology's Appropriateness Criteria (ACR AC). In this study, we\nintroduce a framework to intelligently leverage language models by recommending\nimaging studies for patient cases that are aligned with evidence-based\nguidelines. We make available a novel dataset of patient \"one-liner\" scenarios\nto power our experiments, and optimize state-of-the-art language models to\nachieve an accuracy on par with clinicians in image ordering. Finally, we\ndemonstrate that our language model-based pipeline can be used as intelligent\nassistants by clinicians to support image ordering workflows and improve the\naccuracy of imaging study ordering according to the ACR AC. Our work\ndemonstrates and validates a strategy to leverage AI-based software to improve\ntrustworthy clinical decision making in alignment with expert evidence-based\nguidelines.\n","authors":["Michael S. Yao","Allison Chae","Charles E. Kahn Jr.","Walter R. Witschey","James C. Gee","Hersh Sagreiya","Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2409.19177v2.pdf","comment":"15 pages main text, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.09356v3","updated":"2024-10-01T14:09:08Z","published":"2023-11-15T20:33:27Z","title":"LePaRD: A Large-Scale Dataset of Judges Citing Precedents","summary":"  We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive\ncollection of U.S. federal judicial citations to precedent in context. The\ndataset aims to facilitate work on legal passage prediction, a challenging\npractice-oriented legal retrieval and reasoning task. Legal passage prediction\nseeks to predict relevant passages from precedential court decisions given the\ncontext of a legal argument. We extensively evaluate various retrieval\napproaches on LePaRD, and find that classification appears to work best.\nHowever, we note that legal precedent prediction is a difficult task, and there\nremains significant room for improvement. We hope that by publishing LePaRD, we\nwill encourage others to engage with a legal NLP task that promises to help\nexpand access to justice by reducing the burden associated with legal research.\nA subset of the LePaRD dataset is freely available and the whole dataset will\nbe released upon publication.\n","authors":["Robert Mahari","Dominik Stammbach","Elliott Ash","Alex `Sandy' Pentland"],"pdf_url":"https://arxiv.org/pdf/2311.09356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00226v3","updated":"2024-10-01T13:36:38Z","published":"2024-03-30T02:56:54Z","title":"Design as Desired: Utilizing Visual Question Answering for Multimodal\n  Pre-training","summary":"  Multimodal pre-training demonstrates its potential in the medical domain,\nwhich learns medical visual representations from paired medical reports.\nHowever, many pre-training tasks require extra annotations from clinicians, and\nmost of them fail to explicitly guide the model to learn the desired features\nof different pathologies. In this paper, we utilize Visual Question Answering\n(VQA) for multimodal pre-training to guide the framework focusing on targeted\npathological features. We leverage descriptions in medical reports to design\nmulti-granular question-answer pairs associated with different diseases, which\nassist the framework in pre-training without requiring extra annotations from\nexperts. We also propose a novel pre-training framework with a quasi-textual\nfeature transformer, a module designed to transform visual features into a\nquasi-textual space closer to the textual domain via a contrastive learning\nstrategy. This narrows the vision-language gap and facilitates modality\nalignment. Our framework is applied to four downstream tasks: report\ngeneration, classification, segmentation, and detection across five datasets.\nExtensive experiments demonstrate the superiority of our framework compared to\nother state-of-the-art methods. Our code is available at\nhttps://github.com/MoramiSu/QFT-MICCAI2024.\n","authors":["Tongkun Su","Jun Li","Xi Zhang","Haibo Jin","Hao Chen","Qiong Wang","Faqin Lv","Baoliang Zhao","Yin Hu"],"pdf_url":"https://arxiv.org/pdf/2404.00226v3.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2409.16167v2","updated":"2024-10-01T13:16:45Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09864v2","updated":"2024-10-01T13:07:02Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09009v2","updated":"2024-10-01T13:06:20Z","published":"2024-09-13T17:38:03Z","title":"Optimizing Rare Word Accuracy in Direct Speech Translation with a\n  Retrieval-and-Demonstration Approach","summary":"  Direct speech translation (ST) models often struggle with rare words.\nIncorrect translation of these words can have severe consequences, impacting\ntranslation quality and user trust. While rare word translation is inherently\nchallenging for neural models due to sparse learning signals, real-world\nscenarios often allow access to translations of past recordings on similar\ntopics. To leverage these valuable resources, we propose a\nretrieval-and-demonstration approach to enhance rare word translation accuracy\nin direct ST models. First, we adapt existing ST models to incorporate\nretrieved examples for rare word translation, which allows the model to benefit\nfrom prepended examples, similar to in-context learning. We then develop a\ncross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to\nlocate suitable examples. We demonstrate that standard ST models can be\neffectively adapted to leverage examples for rare word translation, improving\nrare word translation accuracy over the baseline by 17.6% with gold examples\nand 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval\napproach outperforms other modalities and exhibits higher robustness to unseen\nspeakers. Our code is publicly available\n(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).\n","authors":["Siqi Li","Danni Liu","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2409.09009v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.19979v2","updated":"2024-10-01T13:04:55Z","published":"2024-09-30T06:07:12Z","title":"Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model","summary":"  Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.\n","authors":["Xinfeng Wang","Jin Cui","Fumiyo Fukumoto","Yoshimi Suzuki"],"pdf_url":"https://arxiv.org/pdf/2409.19979v2.pdf","comment":"Long paper accepted to EMNLP 2024 Main. 16 pages"},{"id":"http://arxiv.org/abs/2409.17946v2","updated":"2024-10-01T13:01:40Z","published":"2024-09-26T15:20:37Z","title":"Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation","summary":"  Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.\n","authors":["Shuai Zhao","Leilei Gan","Zhongliang Guo","Xiaobao Wu","Luwei Xiao","Xiaoyu Xu","Cong-Duy Nguyen","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2409.17946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05949v5","updated":"2024-10-01T12:38:03Z","published":"2024-01-11T14:38:19Z","title":"Universal Vulnerabilities in Large Language Models: Backdoor Attacks for\n  In-context Learning","summary":"  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.\n","authors":["Shuai Zhao","Meihuizi Jia","Luu Anh Tuan","Fengjun Pan","Jinming Wen"],"pdf_url":"https://arxiv.org/pdf/2401.05949v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05904v3","updated":"2024-10-01T12:08:23Z","published":"2024-05-09T17:00:22Z","title":"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?","summary":"  When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.\n","authors":["Zorik Gekhman","Gal Yona","Roee Aharoni","Matan Eyal","Amir Feder","Roi Reichart","Jonathan Herzig"],"pdf_url":"https://arxiv.org/pdf/2405.05904v3.pdf","comment":"Accepted as a long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.12269v2","updated":"2024-10-01T11:26:11Z","published":"2024-06-18T04:55:09Z","title":"Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization","summary":"  Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework.\n","authors":["Kwangwook Seo","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2406.12269v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.17011v2","updated":"2024-10-01T10:38:25Z","published":"2024-02-26T20:35:34Z","title":"DiffuCOMET: Contextual Commonsense Knowledge Diffusion","summary":"  Inferring contextually-relevant and diverse commonsense to understand\nnarratives remains challenging for knowledge models. In this work, we develop a\nseries of knowledge models, DiffuCOMET, that leverage diffusion to learn to\nreconstruct the implicit semantic connections between narrative contexts and\nrelevant commonsense knowledge. Across multiple diffusion steps, our method\nprogressively refines a representation of commonsense facts that is anchored to\na narrative, producing contextually-relevant and diverse commonsense inferences\nfor an input context. To evaluate DiffuCOMET, we introduce new metrics for\ncommonsense inference that more closely measure knowledge diversity and\ncontextual relevance. Our results on two different benchmarks, ComFact and\nWebNLG+, show that knowledge generated by DiffuCOMET achieves a better\ntrade-off between commonsense diversity, contextual relevance and alignment to\nknown gold references, compared to baseline knowledge models.\n","authors":["Silin Gao","Mete Ismayilzada","Mengjie Zhao","Hiromi Wakaki","Yuki Mitsufuji","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2402.17011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04907v3","updated":"2024-10-01T10:36:20Z","published":"2023-01-12T10:03:56Z","title":"Think Twice: A Human-like Two-stage Conversational Agent for Emotional\n  Response Generation","summary":"  Towards human-like dialogue systems, current emotional dialogue approaches\njointly model emotion and semantics with a unified neural network. This\nstrategy tends to generate safe responses due to the mutual restriction between\nemotion and semantics, and requires rare emotion-annotated large-scale dialogue\ncorpus. Inspired by the \"think twice\" behavior in human dialogue, we propose a\ntwo-stage conversational agent for the generation of emotional dialogue.\nFirstly, a dialogue model trained without the emotion-annotated dialogue corpus\ngenerates a prototype response that meets the contextual semantics. Secondly,\nthe first-stage prototype is modified by a controllable emotion refiner with\nthe empathy hypothesis. Experimental results on the DailyDialog and\nEmpatheticDialogues datasets demonstrate that the proposed conversational\noutperforms the comparison models in emotion generation and maintains the\nsemantic performance in automatic and human evaluations.\n","authors":["Yushan Qian","Bo Wang","Shangzhao Ma","Wu Bin","Shuo Zhang","Dongming Zhao","Kun Huang","Yuexian Hou"],"pdf_url":"https://arxiv.org/pdf/2301.04907v3.pdf","comment":"Accepted to AAMAS 2023"},{"id":"http://arxiv.org/abs/2409.19382v2","updated":"2024-10-01T10:28:32Z","published":"2024-09-28T15:13:04Z","title":"Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with\n  Large Language Models","summary":"  Recent advances in large language models (LLMs) have significantly impacted\nthe domain of multi-hop question answering (MHQA), where systems are required\nto aggregate information and infer answers from disparate pieces of text.\nHowever, the autoregressive nature of LLMs inherently poses a challenge as\nerrors may accumulate if mistakes are made in the intermediate reasoning steps.\nThis paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question\nAnswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to\nidentify optimal reasoning paths in MHQA tasks, mitigating the error\npropagation from sequential reasoning processes. Unlike previous works, we\npropose a zero-shot prompting method, which relies solely on instructions\nwithout the support of hand-crafted few-shot examples that typically require\ndomain expertise. We also introduce a behavioral cloning approach (MZQA-BC)\ntrained on self-generated MCTS inference trajectories, achieving an over\n10-fold increase in reasoning speed with bare compromise in performance. The\nefficacy of our method is validated on standard benchmarks such as HotpotQA,\n2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing\nframeworks.\n","authors":["Seongmin Lee","Jaewook Shin","Youngjin Ahn","Seokin Seo","Ohjoon Kwon","Kee-Eung Kim"],"pdf_url":"https://arxiv.org/pdf/2409.19382v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2409.15371v2","updated":"2024-10-01T10:00:49Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04505v2","updated":"2024-10-01T09:54:00Z","published":"2024-02-07T01:18:55Z","title":"Developments in Sheaf-Theoretic Models of Natural Language Ambiguities","summary":"  Sheaves are mathematical objects consisting of a base which constitutes a\ntopological space and the data associated with each open set thereof, e.g.\ncontinuous functions defined on the open sets. Sheaves have originally been\nused in algebraic topology and logic. Recently, they have also modelled events\nsuch as physical experiments and natural language disambiguation processes. We\nextend the latter models from lexical ambiguities to discourse ambiguities\narising from anaphora. To begin, we calculated a new measure of contextuality\nfor a dataset of basic anaphoric discourses, resulting in a higher proportion\nof contextual models-82.9%-compared to previous work which only yielded 3.17%\ncontextual models. Then, we show how an extension of the natural language\nprocessing challenge, known as the Winograd Schema, which involves anaphoric\nambiguities can be modelled on the Bell-CHSH scenario with a contextual\nfraction of 0.096.\n","authors":["Kin Ian Lo","Mehrnoosh Sadrzadeh","Shane Mansfield"],"pdf_url":"https://arxiv.org/pdf/2402.04505v2.pdf","comment":"In Proceedings DCM 2023, arXiv:2409.19298"},{"id":"http://arxiv.org/abs/2407.04615v2","updated":"2024-10-01T09:23:32Z","published":"2024-07-05T16:11:03Z","title":"Efficient Controlled Language Generation with Low-Rank Autoregressive\n  Reward Models","summary":"  Language models trained on large amounts of data are known to produce\ninappropriate content in some cases and require careful tuning to be used in\nthe real world. We revisit the reward augmented decoding (RAD) approach to\ncontrol the generation from a language model using the scores from a\ntask-specific reward model. We investigate the training objective of RAD, and\nreformulate it as a task of learning a reward matrix. We show that RAD is\ndesigned to support high flexibility when representing the reward matrices,\nwhich leads to a higher computational costs during decoding. However, we\ndemonstrate that RAD does not use its full flexibility. Motivated by this, we\npropose a simpler but more efficient low-rank parametrization of the reward\nmodel enabling fast and effective guided decoding. For the detoxification and\nsentiment control tasks, we show that our low-rank reward model performs on par\nwith the more flexible RAD parametrization, while requiring only a single\nreward model call per generated token.\n","authors":["Sergey Troshin","Vlad Niculae","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2407.04615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18943v2","updated":"2024-10-01T09:20:58Z","published":"2024-09-27T17:44:58Z","title":"Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models","summary":"  The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.\n","authors":["Jiaming Li","Lei Zhang","Yunshui Li","Ziqiang Liu","yuelin bai","Run Luo","Longze Chen","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18708v3","updated":"2024-10-01T08:50:01Z","published":"2024-09-27T12:54:13Z","title":"Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity","summary":"  We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2409.18708v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19541v2","updated":"2024-10-01T08:30:13Z","published":"2024-09-29T03:56:50Z","title":"Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance\n  Regularization","summary":"  Language models frequently inherit societal biases from their training data.\nNumerous techniques have been proposed to mitigate these biases during both the\npre-training and fine-tuning stages. However, fine-tuning a pre-trained\ndebiased language model on a downstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing methods for downstream tasks either (i)\nrequire labels of protected attributes (e.g., age, race, or political views)\nthat are often not available or (ii) rely on indicators of bias, which\nrestricts their applicability to gender debiasing since they rely on\ngender-specific words. To address this, we introduce a novel debiasing\nregularization technique based on the class-wise variance of embeddings.\nCrucially, our method does not require attribute labels and targets any\nattribute, thus addressing the shortcomings of existing debiasing methods. Our\nexperiments on encoder language models and three datasets demonstrate that our\nmethod outperforms existing strong debiasing baselines that rely on target\nattribute labels while maintaining performance on the target task.\n","authors":["Shahed Masoudian","Markus Frohman","Navid Rekabsaz","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2409.19541v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.18618v2","updated":"2024-10-01T07:29:06Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.19898v2","updated":"2024-10-01T07:11:44Z","published":"2024-09-30T02:56:35Z","title":"UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs","summary":"  Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.\n","authors":["Yuho Lee","Taewon Yun","Jason Cai","Hang Su","Hwanjun Song"],"pdf_url":"https://arxiv.org/pdf/2409.19898v2.pdf","comment":"Accepted at EMNLP-Findings 2024"},{"id":"http://arxiv.org/abs/2409.14119v2","updated":"2024-10-01T07:10:02Z","published":"2024-09-21T12:20:18Z","title":"Obliviate: Neutralizing Task-agnostic Backdoors within the\n  Parameter-efficient Fine-tuning Paradigm","summary":"  Parameter-efficient fine-tuning (PEFT) has become a key training strategy for\nlarge language models. However, its reliance on fewer trainable parameters\nposes security risks, such as task-agnostic backdoors. Despite their severe\nimpact on a wide range of tasks, there is no practical defense solution\navailable that effectively counters task-agnostic backdoors within the context\nof PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor\ndefense. We develop two techniques aimed at amplifying benign neurons within\nPEFT layers and penalizing the influence of trigger tokens. Our evaluations\nacross three major PEFT architectures show that our method can significantly\nreduce the attack success rate of the state-of-the-art task-agnostic backdoors\n(83.6%$\\downarrow$). Furthermore, our method exhibits robust defense\ncapabilities against both task-specific backdoors and adaptive attacks. Source\ncode will be obtained at https://github.com/obliviateARR/Obliviate.\n","authors":["Jaehan Kim","Minkyoo Song","Seung Ho Na","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2409.14119v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.19663v2","updated":"2024-10-01T06:35:24Z","published":"2024-09-29T11:29:57Z","title":"Identifying Knowledge Editing Types in Large Language Models","summary":"  Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text.\n","authors":["Xiaopeng Li","Shangwen Wang","Shezheng Song","Bin Ji","Huijun Liu","Shasha Li","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2409.19663v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.18384v2","updated":"2024-10-01T06:17:52Z","published":"2024-04-29T02:43:23Z","title":"Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions","summary":"  Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions.\n","authors":["Jordan Meadows","Tamsin James","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2404.18384v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2409.20441v2","updated":"2024-10-01T06:03:22Z","published":"2024-09-30T16:00:34Z","title":"Instance-adaptive Zero-shot Chain-of-Thought Prompting","summary":"  Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective\nstrategy for enhancing the performance of large language models (LLMs) in\nreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level\nprompt uniformly applied across the whole of instances is inherently limited\nsince one prompt cannot be a good partner for all, a more appropriate approach\nshould consider the interaction between the prompt and each instance\nmeticulously. This work introduces an instance-adaptive prompting algorithm as\nan alternative zero-shot CoT reasoning scheme by adaptively differentiating\ngood and bad prompts. Concretely, we first employ analysis on LLMs through the\nlens of information flow to detect the mechanism under zero-shot CoT reasoning,\nin which we discover that information flows from question to prompt and\nquestion to rationale jointly influence the reasoning results most. We notice\nthat a better zero-shot CoT reasoning needs the prompt to obtain semantic\ninformation from the question then the rationale aggregates sufficient\ninformation from the question directly and via the prompt indirectly. On the\ncontrary, lacking any of those would probably lead to a bad one. Stem from\nthat, we further propose an instance-adaptive prompting strategy (IAP) for\nzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen\non math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal\nJudgement) obtain consistent improvement, demonstrating that the\ninstance-adaptive zero-shot CoT prompting performs better than other task-level\nmethods with some curated prompts or sophisticated procedures, showing the\nsignificance of our findings in the zero-shot CoT reasoning mechanism.\n","authors":["Xiaosong Yuan","Chen Shen","Shaotian Yan","Xiaofeng Zhang","Liang Xie","Wenxiao Wang","Renchu Guan","Ying Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2409.20441v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.19014v2","updated":"2024-10-01T05:55:33Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v2.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2409.20135v2","updated":"2024-10-01T05:37:07Z","published":"2024-09-30T09:34:31Z","title":"Federated Instruction Tuning of LLMs with Domain Coverage Augmentation","summary":"  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.\n","authors":["Zezhou Wang","Yaxin Du","Zhuzhong Qian","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13647v2","updated":"2024-10-01T05:28:54Z","published":"2024-07-18T16:25:17Z","title":"Weak-to-Strong Reasoning","summary":"  When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.\n","authors":["Yuqing Yang","Yan Ma","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13647v2.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.12566v3","updated":"2024-10-01T04:42:48Z","published":"2024-06-18T12:52:51Z","title":"RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users.\n","authors":["Shuting Wang","Xin Yu","Mang Wang","Weipeng Chen","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.12566v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04732v3","updated":"2024-10-01T04:41:53Z","published":"2024-03-07T18:35:54Z","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","summary":"  Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.\n","authors":["Yizhe Zhang","He Bai","Ruixiang Zhang","Jiatao Gu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2403.04732v3.pdf","comment":"COLM 2024. https://github.com/apple/ml-rpm-bench"},{"id":"http://arxiv.org/abs/2311.02262v2","updated":"2024-10-01T04:10:34Z","published":"2023-11-03T22:56:43Z","title":"Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs","summary":"  In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .\n","authors":["Qingru Zhang","Chandan Singh","Liyuan Liu","Xiaodong Liu","Bin Yu","Jianfeng Gao","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.02262v2.pdf","comment":"The 12th International Conference on Learning Representations (ICLR\n  2024)"},{"id":"http://arxiv.org/abs/2409.15355v3","updated":"2024-10-01T03:40:08Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15868v3","updated":"2024-10-01T03:12:35Z","published":"2024-09-24T08:41:26Z","title":"Privacy Evaluation Benchmarks for NLP Models","summary":"  By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.\n","authors":["Wei Huang","Yinggui Wang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.15868v3.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2312.01314v2","updated":"2024-10-01T02:56:30Z","published":"2023-12-03T08:09:45Z","title":"NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian","summary":"  Norwegian, spoken by only 5 million population, is under-representative\nwithin the most impressive breakthroughs in NLP tasks. To the best of our\nknowledge, there has not yet been a comprehensive evaluation of the existing\nlanguage models (LMs) on Norwegian generation tasks during the article writing\nprocess. To fill this gap, we 1) compiled the existing Norwegian dataset and\npre-trained 4 Norwegian Open Language Models varied from parameter scales and\narchitectures, collectively called NorGLM; 2) introduced a comprehensive\nbenchmark, NLEBench, for evaluating natural language generation capabilities in\nNorwegian, encompassing translation and human annotation. Based on the\ninvestigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5\nhas limited capability in understanding the Norwegian context; 2) the increase\nin model parameter scales demonstrates limited impact on the performance of\ndownstream tasks when the pre-training dataset is constrained in size; 3)\nsmaller models also demonstrate the reasoning capability through\nChain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be\nused to verify the generalizability of LLMs on natural language understanding\nand, meanwhile, test the interconnectedness of these NLP tasks. We share our\nresources and code for reproducibility under a CC BY-NC 4.0 license.\n","authors":["Peng Liu","Lemei Zhang","Terje Farup","Even W. Lauvrak","Jon Espen Ingvaldsen","Simen Eide","Jon Atle Gulla","Zhirong Yang"],"pdf_url":"https://arxiv.org/pdf/2312.01314v2.pdf","comment":"Accepted at EMNLP 2024 Main Conference. Code available at\n  https://github.com/Smartmedia-AI/NorGLM/"},{"id":"http://arxiv.org/abs/2408.03247v3","updated":"2024-10-01T01:48:58Z","published":"2024-08-06T15:07:08Z","title":"Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons","summary":"  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n","authors":["Yifei Wang","Yuheng Chen","Wanting Wen","Yu Sheng","Linjing Li","Daniel Dajun Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.03247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08978v2","updated":"2024-10-01T01:40:14Z","published":"2024-08-16T19:01:52Z","title":"See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses","summary":"  The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation.\n","authors":["Yulong Chen","Yang Liu","Jianhao Yan","Xuefeng Bai","Ming Zhong","Yinghao Yang","Ziyi Yang","Chenguang Zhu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.08978v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2311.08369v4","updated":"2024-10-01T01:24:05Z","published":"2023-11-14T18:32:52Z","title":"How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection","summary":"  To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance.\n","authors":["Ryuto Koike","Masahiro Kaneko","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2311.08369v4.pdf","comment":"EMNLP 2024 Findings camera ready. Dataset available at\n  https://github.com/ryuryukke/HowYouPromptMatters"},{"id":"http://arxiv.org/abs/2405.17633v2","updated":"2024-10-01T00:17:41Z","published":"2024-05-27T20:00:38Z","title":"HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal\n  Stories with LLMs","summary":"  Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights.\n","authors":["Jocelyn Shen","Joel Mire","Hae Won Park","Cynthia Breazeal","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2405.17633v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.17146v2","updated":"2024-10-01T00:09:49Z","published":"2024-03-25T19:44:06Z","title":"Outcome-Constrained Large Language Models for Countering Hate Speech","summary":"  Automatic counterspeech generation methods have been developed to assist\nefforts in combating hate speech. Existing research focuses on generating\ncounterspeech with linguistic attributes such as being polite, informative, and\nintent-driven. However, the real impact of counterspeech in online environments\nis seldom considered. This study aims to develop methods for generating\ncounterspeech constrained by conversation outcomes and evaluate their\neffectiveness. We experiment with large language models (LLMs) to incorporate\ninto the text generation process two desired conversation outcomes: low\nconversation incivility and non-hateful hater reentry. Specifically, we\nexperiment with instruction prompts, LLM finetuning, and LLM reinforcement\nlearning (RL). Evaluation results show that our methods effectively steer the\ngeneration of counterspeech toward the desired outcomes. Our analyses, however,\nshow that there are differences in the quality and style depending on the\nmodel.\n","authors":["Lingzi Hong","Pengcheng Luo","Eduardo Blanco","Xiaoying Song"],"pdf_url":"https://arxiv.org/pdf/2403.17146v2.pdf","comment":"Accepted for presentation at the EMNLP 2024 main conference"}],"Sound":[{"id":"http://arxiv.org/abs/2402.18085v3","updated":"2024-10-01T16:54:49Z","published":"2024-02-28T06:17:55Z","title":"PITCH: AI-assisted Tagging of Deepfake Audio Calls using\n  Challenge-Response","summary":"  The rise of AI voice-cloning technology, particularly audio Real-time\nDeepfakes (RTDFs), has intensified social engineering attacks by enabling\nreal-time voice impersonation that bypasses conventional enrollment-based\nauthentication. To address this, we propose PITCH, a robust challenge-response\nmethod to detect and tag interactive deepfake audio calls. We developed a\ncomprehensive taxonomy of audio challenges based on the human auditory system,\nlinguistics, and environmental factors, yielding 20 prospective challenges.\nThese were tested against leading voice-cloning systems using a novel dataset\ncomprising 18,600 original and 1.6 million deepfake samples from 100 users.\nPITCH's prospective challenges enhanced machine detection capabilities to 88.7%\nAUROC score on the full unbalanced dataset, enabling us to shortlist 10\nfunctional challenges that balance security and usability.\n  For human evaluation and subsequent analyses, we filtered a challenging,\nbalanced subset. On this subset, human evaluators independently scored 72.6%\naccuracy, while machines achieved 87.7%. Acknowledging that call environments\nrequire higher human control, we aided call receivers in making decisions with\nthem using machines. Our solution uses an early warning system to tag\nsuspicious incoming calls as \"Deepfake-likely.\" Contrary to prior findings, we\ndiscovered that integrating human intuition with machine precision offers\ncomplementary advantages. Our solution gave users maximum control and boosted\ndetection accuracy to 84.5%. Evidenced by this jump in accuracy, PITCH\ndemonstrated the potential for AI-assisted pre-screening in call verification\nprocesses, offering an adaptable and usable approach to combat real-time\nvoice-cloning attacks. Code to reproduce and access data at\n\\url{https://github.com/mittalgovind/PITCH-Deepfakes}.\n","authors":["Govind Mittal","Arthur Jakobsson","Kelly O. Marshall","Chinmay Hegde","Nasir Memon"],"pdf_url":"https://arxiv.org/pdf/2402.18085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12957v3","updated":"2024-10-01T13:18:54Z","published":"2024-05-17T07:46:05Z","title":"Enhancing the analysis of murine neonatal ultrasonic vocalizations:\n  Development, evaluation, and application of different mathematical models","summary":"  Rodents employ a broad spectrum of ultrasonic vocalizations (USVs) for social\ncommunication. As these vocalizations offer valuable insights into affective\nstates, social interactions, and developmental stages of animals, various deep\nlearning approaches have aimed to automate both the quantitative (detection)\nand qualitative (classification) analysis of USVs. Here, we present the first\nsystematic evaluation of different types of neural networks for USV\nclassification. We assessed various feedforward networks, including a\ncustom-built, fully-connected network and convolutional neural network,\ndifferent residual neural networks (ResNets), an EfficientNet, and a Vision\nTransformer (ViT). Paired with a refined, entropy-based detection algorithm\n(achieving recall of 94.9% and precision of 99.3%), the best architecture\n(achieving 86.79% accuracy) was integrated into a fully automated pipeline\ncapable of analyzing extensive USV datasets with high reliability.\nAdditionally, users can specify an individual minimum accuracy threshold based\non their research needs. In this semi-automated setup, the pipeline selectively\nclassifies calls with high pseudo-probability, leaving the rest for manual\ninspection. Our study focuses exclusively on neonatal USVs. As part of an\nongoing phenotyping study, our pipeline has proven to be a valuable tool for\nidentifying key differences in USVs produced by mice with autism-like\nbehaviors.\n","authors":["Rudolf Herdt","Louisa Kinzel","Johann Georg Maaß","Marvin Walther","Henning Fröhlich","Tim Schubert","Peter Maass","Christian Patrick Schaaf"],"pdf_url":"https://arxiv.org/pdf/2405.12957v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06310v4","updated":"2024-10-01T13:11:33Z","published":"2024-06-10T14:26:09Z","title":"Unsupervised Improved MVDR Beamforming for Sound Enhancement","summary":"  Neural networks have recently become the dominant approach to sound\nseparation. Their good performance relies on large datasets of isolated\nrecordings. For speech and music, isolated single channel data are readily\navailable; however the same does not hold in the multi-channel case, and with\nmost other sound classes. Multi-channel methods have the potential to\noutperform single channel approaches as they can exploit both spatial and\nspectral features, but the lack of training data remains a challenge. We\npropose unsupervised improved minimum variation distortionless response\n(UIMVDR), which enables multi-channel separation to leverage in-the-wild\nsingle-channel data through unsupervised training and beamforming. Results show\nthat UIMVDR generalizes well and improves separation performance compared to\nsupervised models, particularly in cases with limited supervised data. By using\ndata available online, it also reduces the effort required to gather data for\nmulti-channel approaches.\n","authors":["Jacob Kealey","John Hershey","François Grondin"],"pdf_url":"https://arxiv.org/pdf/2406.06310v4.pdf","comment":"Proceedings of INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.14021v2","updated":"2024-10-01T10:54:54Z","published":"2024-07-19T04:44:16Z","title":"GE2E-AC: Generalized End-to-End Loss Training for Accent Classification","summary":"  Accent classification or AC is a task to predict the accent type of an input\nutterance, and it can be used as a preliminary step toward accented speech\nrecognition and accent conversion. Existing studies have often achieved such\nclassification by training a neural network model to minimize the\nclassification error of the predicted accent label, which can be obtained as a\nmodel output. Since we optimize the entire model only from the perspective of\nclassification loss during training time in this approach, the model might\nlearn to predict the accent type from irrelevant features, such as individual\nspeaker identity, which are not informative during test time. To address this\nproblem, we propose a GE2E-AC, in which we train a model to extract accent\nembedding or AE of an input utterance such that the AEs of the same accent\nclass get closer, instead of directly minimizing the classification loss. We\nexperimentally show the effectiveness of the proposed GE2E-AC, compared to the\nbaseline model trained with the conventional cross-entropy-based loss.\n","authors":["Chihiro Watanabe","Hirokazu Kameoka"],"pdf_url":"https://arxiv.org/pdf/2407.14021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05791v4","updated":"2024-10-01T05:09:44Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous microphone array calibration is a prerequisite for many audition\nrobot applications. A popular solution to the above calibration problem is the\nbatch form of Simultaneous Localisation and Mapping (SLAM), using the time\ndifference of arrival measurements between two microphones (TDOA-M), and the\nrobot (which serves as a moving sound source during calibration) odometry\ninformation. In this paper, we introduce a new form of measurement for\nmicrophone array calibration, i.e. the time difference of arrival between\nadjacent sound events (TDOA-S) with respect to the microphone channels. We\npropose to use TDOA-S and TDOA-M, called hybrid TDOA, together with odometry\nmeasurements for bath SLAM-based calibration of asynchronous microphone arrays.\nExtensive simulation and real-world experiments show that our method is more\nindependent of microphone number, less sensitive to initial values (when using\noff-the-shelf algorithms such as Gauss-Newton iterations), and has better\ncalibration accuracy and robustness under various TDOA noises. Simulation\nresults also demonstrate that our method has a lower Cram\\'er-Rao lower bound\n(CRLB) for microphone parameters. To benefit the community, we open-source our\ncode and data at https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19959v2","updated":"2024-10-01T04:18:34Z","published":"2024-06-28T14:47:13Z","title":"RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for\n  Dynamic Speech Enhancement and Localization","summary":"  The training of deep learning-based multichannel speech enhancement and\nsource localization systems relies heavily on the simulation of room impulse\nresponse and multichannel diffuse noise, due to the lack of large-scale\nreal-recorded datasets. However, the acoustic mismatch between simulated and\nreal-world data could degrade the model performance when applying in real-world\nscenarios. To bridge this simulation-to-real gap, this paper presents a new\nrelatively large-scale Real-recorded and annotated Microphone Array\nspeech&Noise (RealMAN) dataset. The proposed dataset is valuable in two\naspects: 1) benchmarking speech enhancement and localization algorithms in real\nscenarios; 2) offering a substantial amount of real-world training data for\npotentially improving the performance of real-world applications. Specifically,\na 32-channel array with high-fidelity microphones is used for recording. A\nloudspeaker is used for playing source speech signals (about 35 hours of\nMandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for\nstatic speaker and 35.4 hours for moving speaker) are recorded in 32 different\nscenes, and 144.5 hours of background noise are recorded in 31 different\nscenes. Both speech and noise recording scenes cover various common indoor,\noutdoor, semi-outdoor and transportation environments, which enables the\ntraining of general-purpose speech enhancement and source localization\nnetworks. To obtain the task-specific annotations, speaker location is\nannotated with an omni-directional fisheye camera by automatically detecting\nthe loudspeaker. The direct-path signal is set as the target clean speech for\nspeech enhancement, which is obtained by filtering the source speech signal\nwith an estimated direct-path propagation filter.\n","authors":["Bing Yang","Changsheng Quan","Yabo Wang","Pengyu Wang","Yujie Yang","Ying Fang","Nian Shao","Hui Bu","Xin Xu","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2406.19959v2.pdf","comment":"accepted by NIPS 2024"},{"id":"http://arxiv.org/abs/2410.01108v1","updated":"2024-10-01T22:34:51Z","published":"2024-10-01T22:34:51Z","title":"Augmentation through Laundering Attacks for Audio Spoof Detection","summary":"  Recent text-to-speech (TTS) developments have made voice cloning (VC) more\nrealistic, affordable, and easily accessible. This has given rise to many\npotential abuses of this technology, including Joe Biden's New Hampshire\ndeepfake robocall. Several methodologies have been proposed to detect such\nclones. However, these methodologies have been trained and evaluated on\nrelatively clean databases. Recently, ASVspoof 5 Challenge introduced a new\ncrowd-sourced database of diverse acoustic conditions including various\nspoofing attacks and codec conditions. This paper is our submission to the\nASVspoof 5 Challenge and aims to investigate the performance of Audio Spoof\nDetection, trained using data augmentation through laundering attacks, on the\nASVSpoof 5 database. The results demonstrate that our system performs worst on\nA18, A19, A20, A26, and A30 spoofing attacks and in the codec and compression\nconditions of C08, C09, and C10.\n","authors":["Hashim Ali","Surya Subramani","Hafiz Malik"],"pdf_url":"https://arxiv.org/pdf/2410.01108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01036v1","updated":"2024-10-01T19:54:10Z","published":"2024-10-01T19:54:10Z","title":"MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation\n  Model Training on EU Languages","summary":"  The rise of foundation models (FMs), coupled with regulatory efforts\naddressing their risks and impacts, has sparked significant interest in\nopen-source models. However, existing speech FMs (SFMs) fall short of full\ncompliance with the open-source principles, even if claimed otherwise, as no\nexisting SFM has model weights, code, and training data publicly available\nunder open-source terms. In this work, we take the first step toward filling\nthis gap by focusing on the 24 official languages of the European Union (EU).\nWe collect suitable training data by surveying automatic speech recognition\ndatasets and unlabeled speech corpora under open-source compliant licenses, for\na total of 950k hours. Additionally, we release automatic transcripts for 441k\nhours of unlabeled data under the permissive CC-BY license, thereby\nfacilitating the creation of open-source SFMs for the EU languages.\n","authors":["Marco Gaido","Sara Papi","Luisa Bentivogli","Alessio Brutti","Mauro Cettolo","Roberto Gretter","Marco Matassoni","Mohamed Nabih","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2410.01036v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.01020v1","updated":"2024-10-01T19:28:45Z","published":"2024-10-01T19:28:45Z","title":"A Critical Assessment of Visual Sound Source Localization Models\n  Including Negative Audio","summary":"  The task of Visual Sound Source Localization (VSSL) involves identifying the\nlocation of sound sources in visual scenes, integrating audio-visual data for\nenhanced scene understanding. Despite advancements in state-of-the-art (SOTA)\nmodels, we observe three critical flaws: i) The evaluation of the models is\nmainly focused in sounds produced by objects that are visible in the image, ii)\nThe evaluation often assumes a prior knowledge of the size of the sounding\nobject, and iii) No universal threshold for localization in real-world\nscenarios is established, as previous approaches only consider positive\nexamples without accounting for both positive and negative cases. In this\npaper, we introduce a novel test set and metrics designed to complete the\ncurrent standard evaluation of VSSL models by testing them in scenarios where\nnone of the objects in the image corresponds to the audio input, i.e. a\nnegative audio. We consider three types of negative audio: silence, noise and\noffscreen. Our analysis reveals that numerous SOTA models fail to appropriately\nadjust their predictions based on audio input, suggesting that these models may\nnot be leveraging audio information as intended. Additionally, we provide a\ncomprehensive analysis of the range of maximum values in the estimated\naudio-visual similarity maps, in both positive and negative audio cases, and\nshow that most of the models are not discriminative enough, making them unfit\nto choose a universal threshold appropriate to perform sound localization\nwithout any a priori information of the sounding object, that is, object size\nand visibility.\n","authors":["Xavier Juanola","Gloria Haro","Magdalena Fuentes"],"pdf_url":"https://arxiv.org/pdf/2410.01020v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.00980v1","updated":"2024-10-01T18:09:02Z","published":"2024-10-01T18:09:02Z","title":"Heterogeneous sound classification with the Broad Sound Taxonomy and\n  Dataset","summary":"  Automatic sound classification has a wide range of applications in machine\nlistening, enabling context-aware sound processing and understanding. This\npaper explores methodologies for automatically classifying heterogeneous sounds\ncharacterized by high intra-class variability. Our study evaluates the\nclassification task using the Broad Sound Taxonomy, a two-level taxonomy\ncomprising 28 classes designed to cover a heterogeneous range of sounds with\nsemantic distinctions tailored for practical user applications. We construct a\ndataset through manual annotation to ensure accuracy, diverse representation\nwithin each class and relevance in real-world scenarios. We compare a variety\nof both traditional and modern machine learning approaches to establish a\nbaseline for the task of heterogeneous sound classification. We investigate the\nrole of input features, specifically examining how acoustically derived sound\nrepresentations compare to embeddings extracted with pre-trained deep neural\nnetworks that capture both acoustic and semantic information about sounds.\nExperimental results illustrate that audio embeddings encoding acoustic and\nsemantic information achieve higher accuracy in the classification task. After\ncareful analysis of classification errors, we identify some underlying reasons\nfor failure and propose actions to mitigate them. The paper highlights the need\nfor deeper exploration of all stages of classification, understanding the data\nand adopting methodologies capable of effectively handling data complexity and\ngeneralizing in real-world sound environments.\n","authors":["Panagiota Anastasopoulou","Jessica Torrey","Xavier Serra","Frederic Font"],"pdf_url":"https://arxiv.org/pdf/2410.00980v1.pdf","comment":"DCASE2024, post-print, 5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.00872v1","updated":"2024-10-01T17:06:30Z","published":"2024-10-01T17:06:30Z","title":"Do Music Generation Models Encode Music Theory?","summary":"  Music foundation models possess impressive music generation capabilities.\nWhen people compose music, they may infuse their understanding of music into\ntheir work, by using notes and intervals to craft melodies, chords to build\nprogressions, and tempo to create a rhythmic feel. To what extent is this true\nof music generation models? More specifically, are fundamental Western music\ntheory concepts observable within the \"inner workings\" of these models? Recent\nwork proposed leveraging latent audio representations from music generation\nmodels towards music information retrieval tasks (e.g. genre classification,\nemotion recognition), which suggests that high-level musical characteristics\nare encoded within these models. However, probing individual music theory\nconcepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus,\nwe introduce SynTheory, a synthetic MIDI and audio music theory dataset,\nconsisting of tempos, time signatures, notes, intervals, scales, chords, and\nchord progressions concepts. We then propose a framework to probe for these\nmusic theory concepts in music foundation models (Jukebox and MusicGen) and\nassess how strongly they encode these concepts within their internal\nrepresentations. Our findings suggest that music theory concepts are\ndiscernible within foundation models and that the degree to which they are\ndetectable varies by model size and layer.\n","authors":["Megan Wei","Michael Freeman","Chris Donahue","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00872v1.pdf","comment":"Accepted at ISMIR 2024. Dataset:\n  https://huggingface.co/datasets/meganwei/syntheory Code:\n  https://github.com/brown-palm/syntheory Website:\n  https://brown-palm.github.io/music-theory"},{"id":"http://arxiv.org/abs/2410.00822v1","updated":"2024-10-01T16:06:02Z","published":"2024-10-01T16:06:02Z","title":"VHASR: A Multimodal Speech Recognition System With Vision Hotwords","summary":"  The image-based multimodal automatic speech recognition (ASR) model enhances\nspeech recognition performance by incorporating audio-related image. However,\nsome works suggest that introducing image information to model does not help\nimproving ASR performance. In this paper, we propose a novel approach\neffectively utilizing audio-related image information and set up VHASR, a\nmultimodal speech recognition system that uses vision as hotwords to strengthen\nthe model's speech recognition capability. Our system utilizes a dual-stream\narchitecture, which firstly transcribes the text on the two streams separately,\nand then combines the outputs. We evaluate the proposed model on four datasets:\nFlickr8k, ADE20k, COCO, and OpenImages. The experimental results show that\nVHASR can effectively utilize key information in images to enhance the model's\nspeech recognition ability. Its performance not only surpasses unimodal ASR,\nbut also achieves SOTA among existing image-based multimodal ASR.\n","authors":["Jiliang Hu","Zuchao Li","Ping Wang","Haojun Ai","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.00822v1.pdf","comment":"14 pages, 6 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.00811v1","updated":"2024-10-01T15:57:35Z","published":"2024-10-01T15:57:35Z","title":"Improving curriculum learning for target speaker extraction with\n  synthetic speakers","summary":"  Target speaker extraction (TSE) aims to isolate individual speaker voices\nfrom complex speech environments. The effectiveness of TSE systems is often\ncompromised when the speaker characteristics are similar to each other. Recent\nresearch has introduced curriculum learning (CL), in which TSE models are\ntrained incrementally on speech samples of increasing complexity. In CL\ntraining, the model is first trained on samples with low speaker similarity\nbetween the target and interference speakers, and then on samples with high\nspeaker similarity. To further improve CL, this paper uses a $k$-nearest\nneighbor-based voice conversion method to simulate and generate speech of\ndiverse interference speakers, and then uses the generated data as part of the\nCL. Experiments demonstrate that training data based on synthetic speakers can\neffectively enhance the model's capabilities and significantly improve the\nperformance of multiple TSE systems.\n","authors":["Yun Liu","Xuechen Liu","Junichi Ymagishi"],"pdf_url":"https://arxiv.org/pdf/2410.00811v1.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.00767v1","updated":"2024-10-01T15:04:21Z","published":"2024-10-01T15:04:21Z","title":"Zero-Shot Text-to-Speech from Continuous Text Streams","summary":"  Existing zero-shot text-to-speech (TTS) systems are typically designed to\nprocess complete sentences and are constrained by the maximum duration for\nwhich they have been trained. However, in many streaming applications, texts\narrive continuously in short chunks, necessitating instant responses from the\nsystem. We identify the essential capabilities required for chunk-level\nstreaming and introduce LiveSpeech 2, a stream-aware model that supports\ninfinitely long speech generation, text-audio stream synchronization, and\nseamless transitions between short speech chunks. To achieve these, we propose\n(1) adopting Mamba, a class of sequence modeling distinguished by linear-time\ndecoding, which is augmented by cross-attention mechanisms for conditioning,\n(2) utilizing rotary positional embeddings in the computation of\ncross-attention, enabling the model to process an infinite text stream by\nsliding a window, and (3) decoding with semantic guidance, a technique that\naligns speech with the transcript during inference with minimal overhead.\nExperimental results demonstrate that our models are competitive with\nstate-of-the-art language model-based zero-shot TTS models, while also\nproviding flexibility to support a wide range of streaming scenarios.\n","authors":["Trung Dang","David Aponte","Dung Tran","Tianyi Chen","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2410.00767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00680v1","updated":"2024-10-01T13:39:05Z","published":"2024-10-01T13:39:05Z","title":"The Conformer Encoder May Reverse the Time Dimension","summary":"  We sometimes observe monotonically decreasing cross-attention weights in our\nConformer-based global attention-based encoder-decoder (AED) models. Further\ninvestigation shows that the Conformer encoder internally reverses the sequence\nin the time dimension. We analyze the initial behavior of the decoder\ncross-attention mechanism and find that it encourages the Conformer encoder\nself-attention to build a connection between the initial frames and all other\ninformative frames. Furthermore, we show that, at some point in training, the\nself-attention module of the Conformer starts dominating the output over the\npreceding feed-forward module, which then only allows the reversed information\nto pass through. We propose several methods and ideas of how this flipping can\nbe avoided. Additionally, we investigate a novel method to obtain\nlabel-frame-position alignments by using the gradients of the label log\nprobabilities w.r.t. the encoder input frames.\n","authors":["Robin Schmitt","Albert Zeyer","Mohammad Zeineldeen","Ralf Schlüter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2410.00680v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.00667v1","updated":"2024-10-01T13:22:51Z","published":"2024-10-01T13:22:51Z","title":"Contribution of soundscape appropriateness to soundscape quality\n  assessment in space: a mediating variable affecting acoustic comfort","summary":"  Soundscape appropriateness (SA) provides supplemental information on the\nmatching degree between auditory information and the surrounding scene in\nsoundscape perception. This indicator has been integrated into the standard ISO\nprocess for collecting soundscape data, forming a component of the sound\nquality assessment questionnaire. However, its role in soundscape quality\nassessment has not been fully understood. Herein, we present the findings from\nsoundscape data collected from Beiling Park in Shenyang, China. A method was\ndeveloped that integrates mediation effect models with multiscale\ngeographically weighted regression (MGWR) models to explore the mediating role\nof SA in the impact of sound source types on soundscape quality, as well as the\nspatial heterogeneity of this mediation effect. The results confirm that SA\ndoes mediates the influence of sound source types on acoustics comfort (AC).\nSpecifically, natural sounds (indirect effect / total effect = 0.19 / 0.19),\ntraffic sounds (indirect effect / total effect = -0.46 / -0.65), and commercial\nsounds (indirect effect / total effect = -0.25 / -0.12) impact the perception\nof AC by either enhancing or reducing SA. Moreover, the relationships among\nvariables depicted in this model demonstrate spatial heterogeneity,\ndemonstrating that in urban open spaces with complex constructures, local\nspatial models may be needed for soundscape assessment. The research reaffirms\nthe significance of SA in urban open spaces. In terms of practical implications\nfor urban and landscape planners, when sound sources cannot be controlled or\naltered, coordinating between the sound and the surrounding environment through\nlandscape optimisation could also improve the quality of the soundscape through\nenhancing SA and help achieve the goal of creating healthy urban open spaces.\n","authors":["Xinhao Yang","Guangyu Zhang","Xiaodong Lu","Yuan Zhang","Jian Kang"],"pdf_url":"https://arxiv.org/pdf/2410.00667v1.pdf","comment":"Revision submitted to Journal of Environmental Management"},{"id":"http://arxiv.org/abs/2410.00316v1","updated":"2024-10-01T01:29:54Z","published":"2024-10-01T01:29:54Z","title":"EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control","summary":"  While recent advances in Text-to-Speech (TTS) technology produce natural and\nexpressive speech, they lack the option for users to select emotion and control\nintensity. We propose EmoKnob, a framework that allows fine-grained emotion\ncontrol in speech synthesis with few-shot demonstrative samples of arbitrary\nemotion. Our framework leverages the expressive speaker representation space\nmade possible by recent advances in foundation voice cloning models. Based on\nthe few-shot capability of our emotion control framework, we propose two\nmethods to apply emotion control on emotions described by open-ended text,\nenabling an intuitive interface for controlling a diverse array of nuanced\nemotions. To facilitate a more systematic emotional speech synthesis field, we\nintroduce a set of evaluation metrics designed to rigorously assess the\nfaithfulness and recognizability of emotion control frameworks. Through\nobjective and subjective evaluations, we show that our emotion control\nframework effectively embeds emotions into speech and surpasses emotion\nexpressiveness of commercial TTS services.\n","authors":["Haozhe Chen","Run Chen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2410.00316v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03751v1","updated":"2024-10-01T21:48:12Z","published":"2024-10-01T21:48:12Z","title":"Recent Advances in Speech Language Models: A Survey","summary":"  Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion\nand error accumulation across the three stages. To address these issues, Speech\nLanguage Models (SpeechLMs) -- end-to-end models that generate speech without\nconverting from text -- have emerged as a promising alternative. This survey\npaper provides the first comprehensive overview of recent methodologies for\nconstructing SpeechLMs, detailing the key components of their architecture and\nthe various training recipes integral to their development. Additionally, we\nsystematically survey the various capabilities of SpeechLMs, categorize the\nevaluation metrics for SpeechLMs, and discuss the challenges and future\nresearch directions in this rapidly evolving field.\n","authors":["Wenqian Cui","Dianzhi Yu","Xiaoqi Jiao","Ziqiao Meng","Guangyan Zhang","Qichao Wang","Yiwen Guo","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2410.03751v1.pdf","comment":"Work in progress"}],"Speech Processing":[{"id":"http://arxiv.org/abs/2402.18085v3","updated":"2024-10-01T16:54:49Z","published":"2024-02-28T06:17:55Z","title":"PITCH: AI-assisted Tagging of Deepfake Audio Calls using\n  Challenge-Response","summary":"  The rise of AI voice-cloning technology, particularly audio Real-time\nDeepfakes (RTDFs), has intensified social engineering attacks by enabling\nreal-time voice impersonation that bypasses conventional enrollment-based\nauthentication. To address this, we propose PITCH, a robust challenge-response\nmethod to detect and tag interactive deepfake audio calls. We developed a\ncomprehensive taxonomy of audio challenges based on the human auditory system,\nlinguistics, and environmental factors, yielding 20 prospective challenges.\nThese were tested against leading voice-cloning systems using a novel dataset\ncomprising 18,600 original and 1.6 million deepfake samples from 100 users.\nPITCH's prospective challenges enhanced machine detection capabilities to 88.7%\nAUROC score on the full unbalanced dataset, enabling us to shortlist 10\nfunctional challenges that balance security and usability.\n  For human evaluation and subsequent analyses, we filtered a challenging,\nbalanced subset. On this subset, human evaluators independently scored 72.6%\naccuracy, while machines achieved 87.7%. Acknowledging that call environments\nrequire higher human control, we aided call receivers in making decisions with\nthem using machines. Our solution uses an early warning system to tag\nsuspicious incoming calls as \"Deepfake-likely.\" Contrary to prior findings, we\ndiscovered that integrating human intuition with machine precision offers\ncomplementary advantages. Our solution gave users maximum control and boosted\ndetection accuracy to 84.5%. Evidenced by this jump in accuracy, PITCH\ndemonstrated the potential for AI-assisted pre-screening in call verification\nprocesses, offering an adaptable and usable approach to combat real-time\nvoice-cloning attacks. Code to reproduce and access data at\n\\url{https://github.com/mittalgovind/PITCH-Deepfakes}.\n","authors":["Govind Mittal","Arthur Jakobsson","Kelly O. Marshall","Chinmay Hegde","Nasir Memon"],"pdf_url":"https://arxiv.org/pdf/2402.18085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12957v3","updated":"2024-10-01T13:18:54Z","published":"2024-05-17T07:46:05Z","title":"Enhancing the analysis of murine neonatal ultrasonic vocalizations:\n  Development, evaluation, and application of different mathematical models","summary":"  Rodents employ a broad spectrum of ultrasonic vocalizations (USVs) for social\ncommunication. As these vocalizations offer valuable insights into affective\nstates, social interactions, and developmental stages of animals, various deep\nlearning approaches have aimed to automate both the quantitative (detection)\nand qualitative (classification) analysis of USVs. Here, we present the first\nsystematic evaluation of different types of neural networks for USV\nclassification. We assessed various feedforward networks, including a\ncustom-built, fully-connected network and convolutional neural network,\ndifferent residual neural networks (ResNets), an EfficientNet, and a Vision\nTransformer (ViT). Paired with a refined, entropy-based detection algorithm\n(achieving recall of 94.9% and precision of 99.3%), the best architecture\n(achieving 86.79% accuracy) was integrated into a fully automated pipeline\ncapable of analyzing extensive USV datasets with high reliability.\nAdditionally, users can specify an individual minimum accuracy threshold based\non their research needs. In this semi-automated setup, the pipeline selectively\nclassifies calls with high pseudo-probability, leaving the rest for manual\ninspection. Our study focuses exclusively on neonatal USVs. As part of an\nongoing phenotyping study, our pipeline has proven to be a valuable tool for\nidentifying key differences in USVs produced by mice with autism-like\nbehaviors.\n","authors":["Rudolf Herdt","Louisa Kinzel","Johann Georg Maaß","Marvin Walther","Henning Fröhlich","Tim Schubert","Peter Maass","Christian Patrick Schaaf"],"pdf_url":"https://arxiv.org/pdf/2405.12957v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06310v4","updated":"2024-10-01T13:11:33Z","published":"2024-06-10T14:26:09Z","title":"Unsupervised Improved MVDR Beamforming for Sound Enhancement","summary":"  Neural networks have recently become the dominant approach to sound\nseparation. Their good performance relies on large datasets of isolated\nrecordings. For speech and music, isolated single channel data are readily\navailable; however the same does not hold in the multi-channel case, and with\nmost other sound classes. Multi-channel methods have the potential to\noutperform single channel approaches as they can exploit both spatial and\nspectral features, but the lack of training data remains a challenge. We\npropose unsupervised improved minimum variation distortionless response\n(UIMVDR), which enables multi-channel separation to leverage in-the-wild\nsingle-channel data through unsupervised training and beamforming. Results show\nthat UIMVDR generalizes well and improves separation performance compared to\nsupervised models, particularly in cases with limited supervised data. By using\ndata available online, it also reduces the effort required to gather data for\nmulti-channel approaches.\n","authors":["Jacob Kealey","John Hershey","François Grondin"],"pdf_url":"https://arxiv.org/pdf/2406.06310v4.pdf","comment":"Proceedings of INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.14021v2","updated":"2024-10-01T10:54:54Z","published":"2024-07-19T04:44:16Z","title":"GE2E-AC: Generalized End-to-End Loss Training for Accent Classification","summary":"  Accent classification or AC is a task to predict the accent type of an input\nutterance, and it can be used as a preliminary step toward accented speech\nrecognition and accent conversion. Existing studies have often achieved such\nclassification by training a neural network model to minimize the\nclassification error of the predicted accent label, which can be obtained as a\nmodel output. Since we optimize the entire model only from the perspective of\nclassification loss during training time in this approach, the model might\nlearn to predict the accent type from irrelevant features, such as individual\nspeaker identity, which are not informative during test time. To address this\nproblem, we propose a GE2E-AC, in which we train a model to extract accent\nembedding or AE of an input utterance such that the AEs of the same accent\nclass get closer, instead of directly minimizing the classification loss. We\nexperimentally show the effectiveness of the proposed GE2E-AC, compared to the\nbaseline model trained with the conventional cross-entropy-based loss.\n","authors":["Chihiro Watanabe","Hirokazu Kameoka"],"pdf_url":"https://arxiv.org/pdf/2407.14021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05791v4","updated":"2024-10-01T05:09:44Z","published":"2024-03-09T04:37:34Z","title":"Asynchronous Microphone Array Calibration using Hybrid TDOA Information","summary":"  Asynchronous microphone array calibration is a prerequisite for many audition\nrobot applications. A popular solution to the above calibration problem is the\nbatch form of Simultaneous Localisation and Mapping (SLAM), using the time\ndifference of arrival measurements between two microphones (TDOA-M), and the\nrobot (which serves as a moving sound source during calibration) odometry\ninformation. In this paper, we introduce a new form of measurement for\nmicrophone array calibration, i.e. the time difference of arrival between\nadjacent sound events (TDOA-S) with respect to the microphone channels. We\npropose to use TDOA-S and TDOA-M, called hybrid TDOA, together with odometry\nmeasurements for bath SLAM-based calibration of asynchronous microphone arrays.\nExtensive simulation and real-world experiments show that our method is more\nindependent of microphone number, less sensitive to initial values (when using\noff-the-shelf algorithms such as Gauss-Newton iterations), and has better\ncalibration accuracy and robustness under various TDOA noises. Simulation\nresults also demonstrate that our method has a lower Cram\\'er-Rao lower bound\n(CRLB) for microphone parameters. To benefit the community, we open-source our\ncode and data at https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib.\n","authors":["Chengjie Zhang","Jiang Wang","He Kong"],"pdf_url":"https://arxiv.org/pdf/2403.05791v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19959v2","updated":"2024-10-01T04:18:34Z","published":"2024-06-28T14:47:13Z","title":"RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for\n  Dynamic Speech Enhancement and Localization","summary":"  The training of deep learning-based multichannel speech enhancement and\nsource localization systems relies heavily on the simulation of room impulse\nresponse and multichannel diffuse noise, due to the lack of large-scale\nreal-recorded datasets. However, the acoustic mismatch between simulated and\nreal-world data could degrade the model performance when applying in real-world\nscenarios. To bridge this simulation-to-real gap, this paper presents a new\nrelatively large-scale Real-recorded and annotated Microphone Array\nspeech&Noise (RealMAN) dataset. The proposed dataset is valuable in two\naspects: 1) benchmarking speech enhancement and localization algorithms in real\nscenarios; 2) offering a substantial amount of real-world training data for\npotentially improving the performance of real-world applications. Specifically,\na 32-channel array with high-fidelity microphones is used for recording. A\nloudspeaker is used for playing source speech signals (about 35 hours of\nMandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for\nstatic speaker and 35.4 hours for moving speaker) are recorded in 32 different\nscenes, and 144.5 hours of background noise are recorded in 31 different\nscenes. Both speech and noise recording scenes cover various common indoor,\noutdoor, semi-outdoor and transportation environments, which enables the\ntraining of general-purpose speech enhancement and source localization\nnetworks. To obtain the task-specific annotations, speaker location is\nannotated with an omni-directional fisheye camera by automatically detecting\nthe loudspeaker. The direct-path signal is set as the target clean speech for\nspeech enhancement, which is obtained by filtering the source speech signal\nwith an estimated direct-path propagation filter.\n","authors":["Bing Yang","Changsheng Quan","Yabo Wang","Pengyu Wang","Yujie Yang","Ying Fang","Nian Shao","Hui Bu","Xin Xu","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2406.19959v2.pdf","comment":"accepted by NIPS 2024"},{"id":"http://arxiv.org/abs/2410.01108v1","updated":"2024-10-01T22:34:51Z","published":"2024-10-01T22:34:51Z","title":"Augmentation through Laundering Attacks for Audio Spoof Detection","summary":"  Recent text-to-speech (TTS) developments have made voice cloning (VC) more\nrealistic, affordable, and easily accessible. This has given rise to many\npotential abuses of this technology, including Joe Biden's New Hampshire\ndeepfake robocall. Several methodologies have been proposed to detect such\nclones. However, these methodologies have been trained and evaluated on\nrelatively clean databases. Recently, ASVspoof 5 Challenge introduced a new\ncrowd-sourced database of diverse acoustic conditions including various\nspoofing attacks and codec conditions. This paper is our submission to the\nASVspoof 5 Challenge and aims to investigate the performance of Audio Spoof\nDetection, trained using data augmentation through laundering attacks, on the\nASVSpoof 5 database. The results demonstrate that our system performs worst on\nA18, A19, A20, A26, and A30 spoofing attacks and in the codec and compression\nconditions of C08, C09, and C10.\n","authors":["Hashim Ali","Surya Subramani","Hafiz Malik"],"pdf_url":"https://arxiv.org/pdf/2410.01108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10177v2","updated":"2024-10-01T21:13:39Z","published":"2024-06-14T16:56:40Z","title":"Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised\n  Learning with Targeted Fine-Tuning and Data Augmentation","summary":"  Automatic speech recognition (ASR) systems often falter while processing\nstuttering-related disfluencies -- such as involuntary blocks and word\nrepetitions -- yielding inaccurate transcripts. A critical barrier to progress\nis the scarcity of large, annotated disfluent speech datasets. Therefore, we\npresent an inclusive ASR design approach, leveraging large-scale\nself-supervised learning on standard speech followed by targeted fine-tuning\nand data augmentation on a smaller, curated dataset of disfluent speech. Our\ndata augmentation technique enriches training datasets with various\ndisfluencies, enhancing ASR processing of these speech patterns. Results show\nthat fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset,\nalongside data augmentation, can significantly reduce word error rates for\ndisfluent speech. Our approach not only advances ASR inclusivity for people who\nstutter, but also paves the way for ASRs that can accommodate wider speech\nvariations.\n","authors":["Dena Mujtaba","Nihar R. Mahapatra","Megan Arney","J. Scott Yaruss","Caryn Herring","Jia Bin"],"pdf_url":"https://arxiv.org/pdf/2406.10177v2.pdf","comment":"Included in 2024 Proceedings of INTERSPEECH"},{"id":"http://arxiv.org/abs/2410.01036v1","updated":"2024-10-01T19:54:10Z","published":"2024-10-01T19:54:10Z","title":"MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation\n  Model Training on EU Languages","summary":"  The rise of foundation models (FMs), coupled with regulatory efforts\naddressing their risks and impacts, has sparked significant interest in\nopen-source models. However, existing speech FMs (SFMs) fall short of full\ncompliance with the open-source principles, even if claimed otherwise, as no\nexisting SFM has model weights, code, and training data publicly available\nunder open-source terms. In this work, we take the first step toward filling\nthis gap by focusing on the 24 official languages of the European Union (EU).\nWe collect suitable training data by surveying automatic speech recognition\ndatasets and unlabeled speech corpora under open-source compliant licenses, for\na total of 950k hours. Additionally, we release automatic transcripts for 441k\nhours of unlabeled data under the permissive CC-BY license, thereby\nfacilitating the creation of open-source SFMs for the EU languages.\n","authors":["Marco Gaido","Sara Papi","Luisa Bentivogli","Alessio Brutti","Mauro Cettolo","Roberto Gretter","Marco Matassoni","Mohamed Nabih","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2410.01036v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.01020v1","updated":"2024-10-01T19:28:45Z","published":"2024-10-01T19:28:45Z","title":"A Critical Assessment of Visual Sound Source Localization Models\n  Including Negative Audio","summary":"  The task of Visual Sound Source Localization (VSSL) involves identifying the\nlocation of sound sources in visual scenes, integrating audio-visual data for\nenhanced scene understanding. Despite advancements in state-of-the-art (SOTA)\nmodels, we observe three critical flaws: i) The evaluation of the models is\nmainly focused in sounds produced by objects that are visible in the image, ii)\nThe evaluation often assumes a prior knowledge of the size of the sounding\nobject, and iii) No universal threshold for localization in real-world\nscenarios is established, as previous approaches only consider positive\nexamples without accounting for both positive and negative cases. In this\npaper, we introduce a novel test set and metrics designed to complete the\ncurrent standard evaluation of VSSL models by testing them in scenarios where\nnone of the objects in the image corresponds to the audio input, i.e. a\nnegative audio. We consider three types of negative audio: silence, noise and\noffscreen. Our analysis reveals that numerous SOTA models fail to appropriately\nadjust their predictions based on audio input, suggesting that these models may\nnot be leveraging audio information as intended. Additionally, we provide a\ncomprehensive analysis of the range of maximum values in the estimated\naudio-visual similarity maps, in both positive and negative audio cases, and\nshow that most of the models are not discriminative enough, making them unfit\nto choose a universal threshold appropriate to perform sound localization\nwithout any a priori information of the sounding object, that is, object size\nand visibility.\n","authors":["Xavier Juanola","Gloria Haro","Magdalena Fuentes"],"pdf_url":"https://arxiv.org/pdf/2410.01020v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.00980v1","updated":"2024-10-01T18:09:02Z","published":"2024-10-01T18:09:02Z","title":"Heterogeneous sound classification with the Broad Sound Taxonomy and\n  Dataset","summary":"  Automatic sound classification has a wide range of applications in machine\nlistening, enabling context-aware sound processing and understanding. This\npaper explores methodologies for automatically classifying heterogeneous sounds\ncharacterized by high intra-class variability. Our study evaluates the\nclassification task using the Broad Sound Taxonomy, a two-level taxonomy\ncomprising 28 classes designed to cover a heterogeneous range of sounds with\nsemantic distinctions tailored for practical user applications. We construct a\ndataset through manual annotation to ensure accuracy, diverse representation\nwithin each class and relevance in real-world scenarios. We compare a variety\nof both traditional and modern machine learning approaches to establish a\nbaseline for the task of heterogeneous sound classification. We investigate the\nrole of input features, specifically examining how acoustically derived sound\nrepresentations compare to embeddings extracted with pre-trained deep neural\nnetworks that capture both acoustic and semantic information about sounds.\nExperimental results illustrate that audio embeddings encoding acoustic and\nsemantic information achieve higher accuracy in the classification task. After\ncareful analysis of classification errors, we identify some underlying reasons\nfor failure and propose actions to mitigate them. The paper highlights the need\nfor deeper exploration of all stages of classification, understanding the data\nand adopting methodologies capable of effectively handling data complexity and\ngeneralizing in real-world sound environments.\n","authors":["Panagiota Anastasopoulou","Jessica Torrey","Xavier Serra","Frederic Font"],"pdf_url":"https://arxiv.org/pdf/2410.00980v1.pdf","comment":"DCASE2024, post-print, 5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.00872v1","updated":"2024-10-01T17:06:30Z","published":"2024-10-01T17:06:30Z","title":"Do Music Generation Models Encode Music Theory?","summary":"  Music foundation models possess impressive music generation capabilities.\nWhen people compose music, they may infuse their understanding of music into\ntheir work, by using notes and intervals to craft melodies, chords to build\nprogressions, and tempo to create a rhythmic feel. To what extent is this true\nof music generation models? More specifically, are fundamental Western music\ntheory concepts observable within the \"inner workings\" of these models? Recent\nwork proposed leveraging latent audio representations from music generation\nmodels towards music information retrieval tasks (e.g. genre classification,\nemotion recognition), which suggests that high-level musical characteristics\nare encoded within these models. However, probing individual music theory\nconcepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus,\nwe introduce SynTheory, a synthetic MIDI and audio music theory dataset,\nconsisting of tempos, time signatures, notes, intervals, scales, chords, and\nchord progressions concepts. We then propose a framework to probe for these\nmusic theory concepts in music foundation models (Jukebox and MusicGen) and\nassess how strongly they encode these concepts within their internal\nrepresentations. Our findings suggest that music theory concepts are\ndiscernible within foundation models and that the degree to which they are\ndetectable varies by model size and layer.\n","authors":["Megan Wei","Michael Freeman","Chris Donahue","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00872v1.pdf","comment":"Accepted at ISMIR 2024. Dataset:\n  https://huggingface.co/datasets/meganwei/syntheory Code:\n  https://github.com/brown-palm/syntheory Website:\n  https://brown-palm.github.io/music-theory"},{"id":"http://arxiv.org/abs/2410.00822v1","updated":"2024-10-01T16:06:02Z","published":"2024-10-01T16:06:02Z","title":"VHASR: A Multimodal Speech Recognition System With Vision Hotwords","summary":"  The image-based multimodal automatic speech recognition (ASR) model enhances\nspeech recognition performance by incorporating audio-related image. However,\nsome works suggest that introducing image information to model does not help\nimproving ASR performance. In this paper, we propose a novel approach\neffectively utilizing audio-related image information and set up VHASR, a\nmultimodal speech recognition system that uses vision as hotwords to strengthen\nthe model's speech recognition capability. Our system utilizes a dual-stream\narchitecture, which firstly transcribes the text on the two streams separately,\nand then combines the outputs. We evaluate the proposed model on four datasets:\nFlickr8k, ADE20k, COCO, and OpenImages. The experimental results show that\nVHASR can effectively utilize key information in images to enhance the model's\nspeech recognition ability. Its performance not only surpasses unimodal ASR,\nbut also achieves SOTA among existing image-based multimodal ASR.\n","authors":["Jiliang Hu","Zuchao Li","Ping Wang","Haojun Ai","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.00822v1.pdf","comment":"14 pages, 6 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.00811v1","updated":"2024-10-01T15:57:35Z","published":"2024-10-01T15:57:35Z","title":"Improving curriculum learning for target speaker extraction with\n  synthetic speakers","summary":"  Target speaker extraction (TSE) aims to isolate individual speaker voices\nfrom complex speech environments. The effectiveness of TSE systems is often\ncompromised when the speaker characteristics are similar to each other. Recent\nresearch has introduced curriculum learning (CL), in which TSE models are\ntrained incrementally on speech samples of increasing complexity. In CL\ntraining, the model is first trained on samples with low speaker similarity\nbetween the target and interference speakers, and then on samples with high\nspeaker similarity. To further improve CL, this paper uses a $k$-nearest\nneighbor-based voice conversion method to simulate and generate speech of\ndiverse interference speakers, and then uses the generated data as part of the\nCL. Experiments demonstrate that training data based on synthetic speakers can\neffectively enhance the model's capabilities and significantly improve the\nperformance of multiple TSE systems.\n","authors":["Yun Liu","Xuechen Liu","Junichi Ymagishi"],"pdf_url":"https://arxiv.org/pdf/2410.00811v1.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.00767v1","updated":"2024-10-01T15:04:21Z","published":"2024-10-01T15:04:21Z","title":"Zero-Shot Text-to-Speech from Continuous Text Streams","summary":"  Existing zero-shot text-to-speech (TTS) systems are typically designed to\nprocess complete sentences and are constrained by the maximum duration for\nwhich they have been trained. However, in many streaming applications, texts\narrive continuously in short chunks, necessitating instant responses from the\nsystem. We identify the essential capabilities required for chunk-level\nstreaming and introduce LiveSpeech 2, a stream-aware model that supports\ninfinitely long speech generation, text-audio stream synchronization, and\nseamless transitions between short speech chunks. To achieve these, we propose\n(1) adopting Mamba, a class of sequence modeling distinguished by linear-time\ndecoding, which is augmented by cross-attention mechanisms for conditioning,\n(2) utilizing rotary positional embeddings in the computation of\ncross-attention, enabling the model to process an infinite text stream by\nsliding a window, and (3) decoding with semantic guidance, a technique that\naligns speech with the transcript during inference with minimal overhead.\nExperimental results demonstrate that our models are competitive with\nstate-of-the-art language model-based zero-shot TTS models, while also\nproviding flexibility to support a wide range of streaming scenarios.\n","authors":["Trung Dang","David Aponte","Dung Tran","Tianyi Chen","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2410.00767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00680v1","updated":"2024-10-01T13:39:05Z","published":"2024-10-01T13:39:05Z","title":"The Conformer Encoder May Reverse the Time Dimension","summary":"  We sometimes observe monotonically decreasing cross-attention weights in our\nConformer-based global attention-based encoder-decoder (AED) models. Further\ninvestigation shows that the Conformer encoder internally reverses the sequence\nin the time dimension. We analyze the initial behavior of the decoder\ncross-attention mechanism and find that it encourages the Conformer encoder\nself-attention to build a connection between the initial frames and all other\ninformative frames. Furthermore, we show that, at some point in training, the\nself-attention module of the Conformer starts dominating the output over the\npreceding feed-forward module, which then only allows the reversed information\nto pass through. We propose several methods and ideas of how this flipping can\nbe avoided. Additionally, we investigate a novel method to obtain\nlabel-frame-position alignments by using the gradients of the label log\nprobabilities w.r.t. the encoder input frames.\n","authors":["Robin Schmitt","Albert Zeyer","Mohammad Zeineldeen","Ralf Schlüter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2410.00680v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.00667v1","updated":"2024-10-01T13:22:51Z","published":"2024-10-01T13:22:51Z","title":"Contribution of soundscape appropriateness to soundscape quality\n  assessment in space: a mediating variable affecting acoustic comfort","summary":"  Soundscape appropriateness (SA) provides supplemental information on the\nmatching degree between auditory information and the surrounding scene in\nsoundscape perception. This indicator has been integrated into the standard ISO\nprocess for collecting soundscape data, forming a component of the sound\nquality assessment questionnaire. However, its role in soundscape quality\nassessment has not been fully understood. Herein, we present the findings from\nsoundscape data collected from Beiling Park in Shenyang, China. A method was\ndeveloped that integrates mediation effect models with multiscale\ngeographically weighted regression (MGWR) models to explore the mediating role\nof SA in the impact of sound source types on soundscape quality, as well as the\nspatial heterogeneity of this mediation effect. The results confirm that SA\ndoes mediates the influence of sound source types on acoustics comfort (AC).\nSpecifically, natural sounds (indirect effect / total effect = 0.19 / 0.19),\ntraffic sounds (indirect effect / total effect = -0.46 / -0.65), and commercial\nsounds (indirect effect / total effect = -0.25 / -0.12) impact the perception\nof AC by either enhancing or reducing SA. Moreover, the relationships among\nvariables depicted in this model demonstrate spatial heterogeneity,\ndemonstrating that in urban open spaces with complex constructures, local\nspatial models may be needed for soundscape assessment. The research reaffirms\nthe significance of SA in urban open spaces. In terms of practical implications\nfor urban and landscape planners, when sound sources cannot be controlled or\naltered, coordinating between the sound and the surrounding environment through\nlandscape optimisation could also improve the quality of the soundscape through\nenhancing SA and help achieve the goal of creating healthy urban open spaces.\n","authors":["Xinhao Yang","Guangyu Zhang","Xiaodong Lu","Yuan Zhang","Jian Kang"],"pdf_url":"https://arxiv.org/pdf/2410.00667v1.pdf","comment":"Revision submitted to Journal of Environmental Management"},{"id":"http://arxiv.org/abs/2410.00528v1","updated":"2024-10-01T09:14:31Z","published":"2024-10-01T09:14:31Z","title":"End-to-End Speech Recognition with Pre-trained Masked Language Model","summary":"  We present a novel approach to end-to-end automatic speech recognition (ASR)\nthat utilizes pre-trained masked language models (LMs) to facilitate the\nextraction of linguistic information. The proposed models, BERT-CTC and BECTRA,\nare specifically designed to effectively integrate pre-trained LMs (e.g., BERT)\ninto end-to-end ASR models. BERT-CTC adapts BERT for connectionist temporal\nclassification (CTC) by addressing the constraint of the conditional\nindependence assumption between output tokens. This enables explicit\nconditioning of BERT's contextualized embeddings in the ASR process, seamlessly\nmerging audio and linguistic information through an iterative refinement\nalgorithm. BECTRA extends BERT-CTC to the transducer framework and trains the\ndecoder network using a vocabulary suitable for ASR training. This aims to\nbridge the gap between the text processed in end-to-end ASR and BERT, as these\nmodels have distinct vocabularies with varying text formats and styles, such as\nthe presence of punctuation. Experimental results on various ASR tasks\ndemonstrate that the proposed models improve over both the CTC and\ntransducer-based baselines, owing to the incorporation of BERT knowledge.\nMoreover, our in-depth analysis and investigation verify the effectiveness of\nthe proposed formulations and architectural designs.\n","authors":["Yosuke Higuchi","Tetsuji Ogawa","Tetsunori Kobayashi","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.00528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00527v1","updated":"2024-10-01T09:11:03Z","published":"2024-10-01T09:11:03Z","title":"Wanna Hear Your Voice: Adaptive, Effective, and Language-Agnostic\n  Approach in Voice Extraction","summary":"  The research on audio clue-based target speaker extraction (TSE) has mostly\nfocused on modeling the mixture and reference speech, achieving high\nperformance in English due to the availability of large datasets. However, less\nattention has been given to the consistent properties of human speech across\nlanguages. To bridge this gap, we introduce WHYV (Wanna Hear Your Voice), which\naddresses the challenge of transferring TSE models from one language to another\nwithout fine-tuning. In this work, we proposed a gating mechanism that be able\nto modify specific frequencies based on the speaker's acoustic features. The\nmodel achieves an SI-SDR of 17.3544 on clean English speech and 13.2032 on\nclean speech mixed with Wham! noise, outperforming all other models in its\nability to adapt to different languages.\n","authors":["The Hieu Pham","Phuong Thanh Tran Nguyen","Xuan Tho Nguyen","Duc Dung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.00527v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.00511v1","updated":"2024-10-01T08:52:35Z","published":"2024-10-01T08:52:35Z","title":"Pre-training with Synthetic Patterns for Audio","summary":"  In this paper, we propose to pre-train audio encoders using synthetic\npatterns instead of real audio data. Our proposed framework consists of two key\nelements. The first one is Masked Autoencoder (MAE), a self-supervised learning\nframework that learns from reconstructing data from randomly masked\ncounterparts. MAEs tend to focus on low-level information such as visual\npatterns and regularities within data. Therefore, it is unimportant what is\nportrayed in the input, whether it be images, audio mel-spectrograms, or even\nsynthetic patterns. This leads to the second key element, which is synthetic\ndata. Synthetic data, unlike real audio, is free from privacy and licensing\ninfringement issues. By combining MAEs and synthetic patterns, our framework\nenables the model to learn generalized feature representations without real\ndata, while addressing the issues related to real audio. To evaluate the\nefficacy of our framework, we conduct extensive experiments across a total of\n13 audio tasks and 17 synthetic datasets. The experiments provide insights into\nwhich types of synthetic patterns are effective for audio. Our results\ndemonstrate that our framework achieves performance comparable to models\npre-trained on AudioSet-2M and partially outperforms image-based pre-training\nmethods.\n","authors":["Yuchi Ishikawa","Tatsuya Komatsu","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2410.00511v1.pdf","comment":"Submitted to ICASSP'25"},{"id":"http://arxiv.org/abs/2410.00390v1","updated":"2024-10-01T04:22:10Z","published":"2024-10-01T04:22:10Z","title":"Multi-Scale Temporal Transformer For Speech Emotion Recognition","summary":"  Speech emotion recognition plays a crucial role in human-machine interaction\nsystems. Recently various optimized Transformers have been successfully applied\nto speech emotion recognition. However, the existing Transformer architectures\nfocus more on global information and require large computation. On the other\nhand, abundant speech emotional representations exist locally on different\nparts of the input speech. To tackle these problems, we propose a Multi-Scale\nTRansfomer (MSTR) for speech emotion recognition. It comprises of three main\ncomponents: (1) a multi-scale temporal feature operator, (2) a fractal\nself-attention module, and (3) a scale mixer module. These three components can\neffectively enhance the transformer's ability to learn multi-scale local\nemotion representations. Experimental results demonstrate that the proposed\nMSTR model significantly outperforms a vanilla Transformer and other\nstate-of-the-art methods across three speech emotion datasets: IEMOCAP, MELD\nand, CREMAD. In addition, it can greatly reduce the computational cost.\n","authors":["Zhipeng Li","Xiaofen Xing","Yuanbo Fang","Weibin Zhang","Hengsheng Fan","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2410.00390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00316v1","updated":"2024-10-01T01:29:54Z","published":"2024-10-01T01:29:54Z","title":"EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control","summary":"  While recent advances in Text-to-Speech (TTS) technology produce natural and\nexpressive speech, they lack the option for users to select emotion and control\nintensity. We propose EmoKnob, a framework that allows fine-grained emotion\ncontrol in speech synthesis with few-shot demonstrative samples of arbitrary\nemotion. Our framework leverages the expressive speaker representation space\nmade possible by recent advances in foundation voice cloning models. Based on\nthe few-shot capability of our emotion control framework, we propose two\nmethods to apply emotion control on emotions described by open-ended text,\nenabling an intuitive interface for controlling a diverse array of nuanced\nemotions. To facilitate a more systematic emotional speech synthesis field, we\nintroduce a set of evaluation metrics designed to rigorously assess the\nfaithfulness and recognizability of emotion control frameworks. Through\nobjective and subjective evaluations, we show that our emotion control\nframework effectively embeds emotions into speech and surpasses emotion\nexpressiveness of commercial TTS services.\n","authors":["Haozhe Chen","Run Chen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2410.00316v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03751v1","updated":"2024-10-01T21:48:12Z","published":"2024-10-01T21:48:12Z","title":"Recent Advances in Speech Language Models: A Survey","summary":"  Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion\nand error accumulation across the three stages. To address these issues, Speech\nLanguage Models (SpeechLMs) -- end-to-end models that generate speech without\nconverting from text -- have emerged as a promising alternative. This survey\npaper provides the first comprehensive overview of recent methodologies for\nconstructing SpeechLMs, detailing the key components of their architecture and\nthe various training recipes integral to their development. Additionally, we\nsystematically survey the various capabilities of SpeechLMs, categorize the\nevaluation metrics for SpeechLMs, and discuss the challenges and future\nresearch directions in this rapidly evolving field.\n","authors":["Wenqian Cui","Dianzhi Yu","Xiaoqi Jiao","Ziqiao Meng","Guangyan Zhang","Qichao Wang","Yiwen Guo","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2410.03751v1.pdf","comment":"Work in progress"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2409.12618v2","updated":"2024-10-01T17:50:25Z","published":"2024-09-19T09:44:17Z","title":"Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning","summary":"  Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.\n","authors":["Santosh Kumar Radha","Yasamin Nouri Jelyani","Ara Ghukasyan","Oktay Goktas"],"pdf_url":"https://arxiv.org/pdf/2409.12618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18924v2","updated":"2024-10-01T17:49:00Z","published":"2024-09-27T17:17:15Z","title":"AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow","summary":"  Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.\n","authors":["Huizi Yu","Jiayan Zhou","Lingyao Li","Shan Chen","Jack Gallifant","Anye Shi","Xiang Li","Wenyue Hua","Mingyu Jin","Guang Chen","Yang Zhou","Zhao Li","Trisha Gupte","Ming-Li Chen","Zahra Azizi","Yongfeng Zhang","Themistocles L. Assimes","Xin Ma","Danielle S. Bitterman","Lin Lu","Lizhou Fan"],"pdf_url":"https://arxiv.org/pdf/2409.18924v2.pdf","comment":"42 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.17447v2","updated":"2024-10-01T17:39:09Z","published":"2024-07-24T17:23:18Z","title":"FLRT: Fluent Student-Teacher Redteaming","summary":"  Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. To redteam or jailbreak these\nmodels for compliance with toxic requests, users and security analysts have\ndeveloped adversarial prompting techniques. One attack method is to apply\ndiscrete optimization techniques to the prompt. However, the resulting attack\nstrings are often gibberish text, easily filtered by defenders due to high\nmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. In\nthis work, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.\n","authors":["T. Ben Thompson","Michael Sklar"],"pdf_url":"https://arxiv.org/pdf/2407.17447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19300v3","updated":"2024-10-01T17:39:02Z","published":"2024-05-29T17:27:08Z","title":"Measuring and Mitigating Bias for Tabular Datasets with Multiple\n  Protected Attributes","summary":"  Motivated by the recital (67) of the current corrigendum of the AI Act in the\nEuropean Union, we propose and present measures and mitigation strategies for\ndiscrimination in tabular datasets. We specifically focus on datasets that\ncontain multiple protected attributes, such as nationality, age, and sex. This\nmakes measuring and mitigating bias more challenging, as many existing methods\nare designed for a single protected attribute. This paper comes with a twofold\ncontribution: Firstly, new discrimination measures are introduced. These\nmeasures are categorized in our framework along with existing ones, guiding\nresearchers and practitioners in choosing the right measure to assess the\nfairness of the underlying dataset. Secondly, a novel application of an\nexisting bias mitigation method, FairDo, is presented. We show that this\nstrategy can mitigate any type of discrimination, including intersectional\ndiscrimination, by transforming the dataset. By conducting experiments on\nreal-world datasets (Adult, Bank, COMPAS), we demonstrate that de-biasing\ndatasets with multiple protected attributes is possible. All transformed\ndatasets show a reduction in discrimination, on average by 28%. Further, these\ndatasets do not compromise any of the tested machine learning models'\nperformances significantly compared to the original datasets. Conclusively,\nthis study demonstrates the effectiveness of the mitigation strategy used and\ncontributes to the ongoing discussion on the implementation of the European\nUnion's AI Act.\n","authors":["Manh Khoi Duong","Stefan Conrad"],"pdf_url":"https://arxiv.org/pdf/2405.19300v3.pdf","comment":"Submission accepted in AEQUITAS'24 (co-located with ECAI 2024)"},{"id":"http://arxiv.org/abs/2403.18079v2","updated":"2024-10-01T17:33:13Z","published":"2024-03-26T19:58:39Z","title":"Paths to Equilibrium in Games","summary":"  In multi-agent reinforcement learning (MARL) and game theory, agents\nrepeatedly interact and revise their strategies as new data arrives, producing\na sequence of strategy profiles. This paper studies sequences of strategies\nsatisfying a pairwise constraint inspired by policy updating in reinforcement\nlearning, where an agent who is best responding in one period does not switch\nits strategy in the next period. This constraint merely requires that\noptimizing agents do not switch strategies, but does not constrain the\nnon-optimizing agents in any way, and thus allows for exploration. Sequences\nwith this property are called satisficing paths, and arise naturally in many\nMARL algorithms. A fundamental question about strategic dynamics is such: for a\ngiven game and initial strategy profile, is it always possible to construct a\nsatisficing path that terminates at an equilibrium? The resolution of this\nquestion has implications about the capabilities or limitations of a class of\nMARL algorithms. We answer this question in the affirmative for normal-form\ngames. Our analysis reveals a counterintuitive insight that reward\ndeteriorating strategic updates are key to driving play to equilibrium along a\nsatisficing path.\n","authors":["Bora Yongacoglu","Gürdal Arslan","Lacra Pavel","Serdar Yüksel"],"pdf_url":"https://arxiv.org/pdf/2403.18079v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.01247v2","updated":"2024-10-01T17:21:28Z","published":"2024-09-02T13:29:44Z","title":"Conversational Complexity for Assessing Risk in Large Language Models","summary":"  Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case was Kevin Roose's notable conversation with\nBing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily,\nraising the question: How much conversational effort is needed to elicit\nharmful information from LLMs? We propose two measures: Conversational Length\n(CL), which quantifies the conversation length used to obtain a specific\nresponse, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the response. To\naddress the incomputability of Kolmogorov complexity, we approximate CC using a\nreference LLM to estimate the compressibility of user instructions. Applying\nthis approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless\nconversational lengths and complexities. Our empirical findings suggest that\nthis distributional analysis and the minimisation of CC serve as valuable tools\nfor understanding AI safety, offering insights into the accessibility of\nharmful information. This work establishes a foundation for a new perspective\non LLM safety, centered around the algorithmic complexity of pathways to harm.\n","authors":["John Burden","Manuel Cebrian","Jose Hernandez-Orallo"],"pdf_url":"https://arxiv.org/pdf/2409.01247v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.09111v2","updated":"2024-10-01T17:10:07Z","published":"2024-07-12T09:24:34Z","title":"Inference Optimization of Foundation Models on AI Accelerators","summary":"  Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.\n","authors":["Youngsuk Park","Kailash Budhathoki","Liangfu Chen","Jonas Kübler","Jiaji Huang","Matthäus Kleindessner","Jun Huan","Volkan Cevher","Yida Wang","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2407.09111v2.pdf","comment":"[v2] Tutorial website added [v1] Tutorial published at KDD 2024.\n  Camera-ready version"},{"id":"http://arxiv.org/abs/2406.17328v3","updated":"2024-10-01T16:45:12Z","published":"2024-06-25T07:25:15Z","title":"Dual-Space Knowledge Distillation for Large Language Models","summary":"  Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.\n","authors":["Songming Zhang","Xue Zhang","Zengkui Sun","Yufeng Chen","Jinan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17328v3.pdf","comment":"The camera-ready version for EMNLP 2024 main conference. 17 pages, 11\n  figures, code available at: https://github.com/songmzhang/DSKD"},{"id":"http://arxiv.org/abs/2409.20252v2","updated":"2024-10-01T16:34:13Z","published":"2024-09-30T12:42:25Z","title":"What is the Role of Large Language Models in the Evolution of Astronomy\n  Research?","summary":"  ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.\n","authors":["Morgan Fouesneau","Ivelina G. Momcheva","Urmila Chadayammuri","Mariia Demianenko","Antoine Dumont","Raphael E. Hviding","K. Angelique Kahle","Nadiia Pulatova","Bhavesh Rajpoot","Marten B. Scheuck","Rhys Seeburger","Dmitry Semenov","Jaime I. Villaseñor"],"pdf_url":"https://arxiv.org/pdf/2409.20252v2.pdf","comment":"Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback"},{"id":"http://arxiv.org/abs/2311.16176v4","updated":"2024-10-01T15:50:57Z","published":"2023-11-23T15:47:33Z","title":"Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse\n  Ensembles","summary":"  Spurious correlations in the data, where multiple cues are predictive of the\ntarget labels, often lead to a phenomenon known as shortcut learning, where a\nmodel relies on erroneous, easy-to-learn cues while ignoring reliable ones. In\nthis work, we propose DiffDiv an ensemble diversification framework exploiting\nDiffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show\nthat at particular training intervals, DPMs can generate images with novel\nfeature combinations, even when trained on samples displaying correlated input\nfeatures. We leverage this crucial property to generate synthetic\ncounterfactuals to increase model diversity via ensemble disagreement. We show\nthat DPM-guided diversification is sufficient to remove dependence on shortcut\ncues, without a need for additional supervised signals. We further empirically\nquantify its efficacy on several diversification objectives, and finally show\nimproved generalization and diversification on par with prior work that relies\non auxiliary data collection.\n","authors":["Luca Scimeca","Alexander Rubinstein","Damien Teney","Seong Joon Oh","Armand Mihai Nicolicioiu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2311.16176v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.02230"},{"id":"http://arxiv.org/abs/2409.17055v2","updated":"2024-10-01T15:47:14Z","published":"2024-09-25T16:13:57Z","title":"DRIM: Learning Disentangled Representations from Incomplete Multimodal\n  Healthcare Data","summary":"  Real-life medical data is often multimodal and incomplete, fueling the\ngrowing need for advanced deep learning models capable of integrating them\nefficiently. The use of diverse modalities, including histopathology slides,\nMRI, and genetic data, offers unprecedented opportunities to improve prognosis\nprediction and to unveil new treatment pathways. Contrastive learning, widely\nused for deriving representations from paired data in multimodal tasks, assumes\nthat different views contain the same task-relevant information and leverages\nonly shared information. This assumption becomes restrictive when handling\nmedical data since each modality also harbors specific knowledge relevant to\ndownstream tasks. We introduce DRIM, a new multimodal method for capturing\nthese shared and unique representations, despite data sparsity. More\nspecifically, given a set of modalities, we aim to encode a representation for\neach one that can be divided into two components: one encapsulating\npatient-related information common across modalities and the other,\nencapsulating modality-specific details. This is achieved by increasing the\nshared information among different patient modalities while minimizing the\noverlap between shared and unique components within each modality. Our method\noutperforms state-of-the-art algorithms on glioma patients survival prediction\ntasks, while being robust to missing modalities. To promote reproducibility,\nthe code is made publicly available at https://github.com/Lucas-rbnt/DRIM\n","authors":["Lucas Robinet","Ahmad Berjaoui","Ziad Kheil","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2409.17055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03354v3","updated":"2024-10-01T15:41:22Z","published":"2024-08-06T09:15:25Z","title":"The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums","summary":"  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence.\n","authors":["Vanessa Clairoux-Trepanier","Isa-May Beauchamp","Estelle Ruellan","Masarah Paquet-Clouston","Serge-Olivier Paquette","Eric Clay"],"pdf_url":"https://arxiv.org/pdf/2408.03354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10774v2","updated":"2024-10-01T15:39:48Z","published":"2024-04-16T17:59:10Z","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","summary":"  Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models.\n","authors":["Liyan Tang","Philippe Laban","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2404.10774v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.17510v2","updated":"2024-10-01T15:23:56Z","published":"2024-09-26T03:40:12Z","title":"NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human\n  Connectomes","summary":"  Although modern imaging technologies allow us to study connectivity between\ntwo distinct brain regions in-vivo, an in-depth understanding of how anatomical\nstructure supports brain function and how spontaneous functional fluctuations\nemerge remarkable cognition is still elusive. Meanwhile, tremendous efforts\nhave been made in the realm of machine learning to establish the nonlinear\nmapping between neuroimaging data and phenotypic traits. However, the absence\nof neuroscience insight in the current approaches poses significant challenges\nin understanding cognitive behavior from transient neural activities. To\naddress this challenge, we put the spotlight on the coupling mechanism of\nstructural connectivity (SC) and functional connectivity (FC) by formulating\nsuch network neuroscience question into an expressive graph representation\nlearning problem for high-order topology. Specifically, we introduce the\nconcept of topological detour to characterize how a ubiquitous instance of FC\n(direct link) is supported by neural pathways (detour) physically wired by SC,\nwhich forms a cyclic loop interacted by brain structure and function. In the\nclich\\'e of machine learning, the multi-hop detour pathway underlying SC-FC\ncoupling allows us to devise a novel multi-head self-attention mechanism within\nTransformer to capture multi-modal feature representation from paired graphs of\nSC and FC. Taken together, we propose a biological-inspired deep model, coined\nas NeuroPath, to find putative connectomic feature representations from the\nunprecedented amount of neuroimages, which can be plugged into various\ndownstream applications such as task recognition and disease diagnosis. We have\nevaluated NeuroPath on large-scale public datasets including HCP and UK Biobank\nunder supervised and zero-shot learning, where the state-of-the-art performance\nby our NeuroPath indicates great potential in network neuroscience.\n","authors":["Ziquan Wei","Tingting Dan","Jiaqi Ding","Guorong Wu"],"pdf_url":"https://arxiv.org/pdf/2409.17510v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.10932v2","updated":"2024-10-01T15:21:05Z","published":"2024-09-17T07:08:39Z","title":"Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine\n  Learning Approach","summary":"  Coronary heart disease (CHD) is a severe cardiac disease, and hence, its\nearly diagnosis is essential as it improves treatment results and saves money\non medical care. The prevailing development of quantum computing and machine\nlearning (ML) technologies may bring practical improvement to the performance\nof CHD diagnosis. Quantum machine learning (QML) is receiving tremendous\ninterest in various disciplines due to its higher performance and capabilities.\nA quantum leap in the healthcare industry will increase processing power and\noptimise multiple models. Techniques for QML have the potential to forecast\ncardiac disease and help in early detection. To predict the risk of coronary\nheart disease, a hybrid approach utilizing an ensemble machine learning model\nbased on QML classifiers is presented in this paper. Our approach, with its\nunique ability to address multidimensional healthcare data, reassures the\nmethod's robustness by fusing quantum and classical ML algorithms in a\nmulti-step inferential framework. The marked rise in heart disease and death\nrates impacts worldwide human health and the global economy. Reducing cardiac\nmorbidity and mortality requires early detection of heart disease. In this\nresearch, a hybrid approach utilizes techniques with quantum computing\ncapabilities to tackle complex problems that are not amenable to conventional\nmachine learning algorithms and to minimize computational expenses. The\nproposed method has been developed in the Raspberry Pi 5 Graphics Processing\nUnit (GPU) platform and tested on a broad dataset that integrates clinical and\nimaging data from patients suffering from CHD and healthy controls. Compared to\nclassical machine learning models, the accuracy, sensitivity, F1 score, and\nspecificity of the proposed hybrid QML model used with CHD are manifold higher.\n","authors":["Mehroush Banday","Sherin Zafar","Parul Agarwal","M Afshar Alam","Abubeker K M"],"pdf_url":"https://arxiv.org/pdf/2409.10932v2.pdf","comment":"I found a mistake in methodology presentation. Also I have observed\n  more precised results with new dataset. So my research guide ask me to modify\n  the current version"},{"id":"http://arxiv.org/abs/2407.00568v4","updated":"2024-10-01T15:19:42Z","published":"2024-06-30T02:50:28Z","title":"Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep\n  Penalty Neural Ordinary Differential Equations","summary":"  Forecasting high-dimensional dynamical systems is a fundamental challenge in\nvarious fields, such as geosciences and engineering. Neural Ordinary\nDifferential Equations (NODEs), which combine the power of neural networks and\nnumerical solvers, have emerged as a promising algorithm for forecasting\ncomplex nonlinear dynamical systems. However, classical techniques used for\nNODE training are ineffective for learning chaotic dynamical systems. In this\nwork, we propose a novel NODE-training approach that allows for robust learning\nof chaotic dynamical systems. Our method addresses the challenges of\nnon-convexity and exploding gradients associated with underlying chaotic\ndynamics. Training data trajectories from such systems are split into multiple,\nnon-overlapping time windows. In addition to the deviation from the training\ndata, the optimization loss term further penalizes the discontinuities of the\npredicted trajectory between the time windows. The window size is selected\nbased on the fastest Lyapunov time scale of the system. Multi-step penalty(MP)\nmethod is first demonstrated on Lorenz equation, to illustrate how it improves\nthe loss landscape and thereby accelerates the optimization convergence. MP\nmethod can optimize chaotic systems in a manner similar to least-squares\nshadowing with significantly lower computational costs. Our proposed algorithm,\ndenoted the Multistep Penalty NODE, is applied to chaotic systems such as the\nKuramoto-Sivashinsky equation, the two-dimensional Kolmogorov flow, and ERA5\nreanalysis data for the atmosphere. It is observed that MP-NODE provide viable\nperformance for such chaotic systems, not only for short-term trajectory\npredictions but also for invariant statistics that are hallmarks of the chaotic\nnature of these dynamics.\n","authors":["Dibyajyoti Chakraborty","Seung Whan Chung","Troy Arcomano","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2407.00568v4.pdf","comment":"25 pages, 17 Figures, submitted to Computer Methods in Applied\n  Mechanics and Engineering"},{"id":"http://arxiv.org/abs/2405.03869v3","updated":"2024-10-01T15:07:09Z","published":"2024-05-06T21:34:46Z","title":"Outlier Gradient Analysis: Efficiently Improving Deep Learning Model\n  Performance via Hessian-Free Influence Functions","summary":"  A core data-centric learning challenge is the identification of training\nsamples that are detrimental to model performance. Influence functions serve as\na prominent tool for this task and offer a robust framework for assessing\ntraining data influence on model predictions. Despite their widespread use,\ntheir high computational cost associated with calculating the inverse of the\nHessian matrix pose constraints, particularly when analyzing large-sized deep\nmodels. In this paper, we establish a bridge between identifying detrimental\ntraining samples via influence functions and outlier gradient detection. This\ntransformation not only presents a straightforward and Hessian-free formulation\nbut also provides insights into the role of the gradient in sample impact.\nThrough systematic empirical evaluations, we first validate the hypothesis of\nour proposed outlier gradient analysis approach on synthetic datasets. We then\ndemonstrate its effectiveness in detecting mislabeled samples in vision models\nand selecting data samples for improving performance of natural language\nprocessing transformer models. We also extend its use to influential sample\nidentification for fine-tuning Large Language Models.\n","authors":["Anshuman Chhabra","Bo Li","Jian Chen","Prasant Mohapatra","Hongfu Liu"],"pdf_url":"https://arxiv.org/pdf/2405.03869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20330v3","updated":"2024-10-01T15:04:23Z","published":"2024-05-30T17:59:02Z","title":"OmniHands: Towards Robust 4D Hand Mesh Recovery via A Versatile\n  Transformer","summary":"  In this paper, we introduce OmniHands, a universal approach to recovering\ninteractive hand meshes and their relative movement from monocular or\nmulti-view inputs. Our approach addresses two major limitations of previous\nmethods: lacking a unified solution for handling various hand image inputs and\nneglecting the positional relationship of two hands within images. To overcome\nthese challenges, we develop a universal architecture with novel tokenization\nand contextual feature fusion strategies, capable of adapting to a variety of\ntasks. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT)\nmethod to embed positional relation information into the hand tokens. In this\nway, our network can handle both single-hand and two-hand inputs and explicitly\nleverage relative hand positions, facilitating the reconstruction of intricate\nhand interactions in real-world scenarios. As such tokenization indicates the\nrelative relationship of two hands, it also supports more effective feature\nfusion. To this end, we further develop a 4D Interaction Reasoning (FIR) module\nto fuse hand tokens in 4D with attention and decode them into 3D hand meshes\nand relative temporal movements. The efficacy of our approach is validated on\nseveral benchmark datasets. The results on in-the-wild videos and real-world\nscenarios demonstrate the superior performances of our approach for interactive\nhand reconstruction. More video results can be found on the project page:\nhttps://OmniHand.github.io.\n","authors":["Dixuan Lin","Yuxiang Zhang","Mengcheng Li","Yebin Liu","Wei Jing","Qi Yan","Qianying Wang","Hongwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20330v3.pdf","comment":"An extended journal version of 4DHands, featured with versatile\n  module that can adapt to temporal task and multi-view task. Additional\n  detailed comparison experiments and results presentation have been added.\n  More demo videos can be seen at our project page: https://OmniHand.github.io"},{"id":"http://arxiv.org/abs/2405.12701v2","updated":"2024-10-01T15:03:14Z","published":"2024-05-21T11:50:16Z","title":"OLAPH: Improving Factuality in Biomedical Long-form Question Answering","summary":"  In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.\n","authors":["Minbyul Jeong","Hyeon Hwang","Chanwoong Yoon","Taewhoo Lee","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2405.12701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08767v2","updated":"2024-10-01T14:46:42Z","published":"2024-09-13T12:20:04Z","title":"HOLA-Drone: Hypergraphic Open-ended Learning for Zero-Shot Multi-Drone\n  Cooperative Pursuit","summary":"  Zero-shot coordination (ZSC) is a significant challenge in multi-agent\ncollaboration, aiming to develop agents that can coordinate with unseen\npartners they have not encountered before. Recent cutting-edge ZSC methods have\nprimarily focused on two-player video games such as OverCooked!2 and Hanabi. In\nthis paper, we extend the scope of ZSC research to the multi-drone cooperative\npursuit scenario, exploring how to construct a drone agent capable of\ncoordinating with multiple unseen partners to capture multiple evaders. We\npropose a novel Hypergraphic Open-ended Learning Algorithm (HOLA-Drone) that\ncontinuously adapts the learning objective based on our hypergraphic-form game\nmodeling, aiming to improve cooperative abilities with multiple unknown drone\nteammates. To empirically verify the effectiveness of HOLA-Drone, we build two\ndifferent unseen drone teammate pools to evaluate their performance in\ncoordination with various unseen partners. The experimental results demonstrate\nthat HOLA-Drone outperforms the baseline methods in coordination with unseen\ndrone teammates. Furthermore, real-world experiments validate the feasibility\nof HOLA-Drone in physical systems. Videos can be found on the project\nhomepage~\\url{https://sites.google.com/view/hola-drone}.\n","authors":["Yang Li","Dengyu Zhang","Junfan Chen","Ying Wen","Qingrui Zhang","Shaoshuai Mou","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2409.08767v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2310.14069v2","updated":"2024-10-01T14:35:59Z","published":"2023-10-21T17:20:20Z","title":"Ladder Bottom-up Convolutional Bidirectional Variational Autoencoder for\n  Image Translation of Dotted Arabic Expiration Dates","summary":"  This paper proposes an approach of Ladder Bottom-up Convolutional\nBidirectional Variational Autoencoder (LCBVAE) architecture for the encoder and\ndecoder, which is trained on the image translation of the dotted Arabic\nexpiration dates by reconstructing the Arabic dotted expiration dates into\nfilled-in expiration dates. We employed a customized and adapted version of\nConvolutional Recurrent Neural Network CRNN model to meet our specific\nrequirements and enhance its performance in our context, and then trained the\ncustom CRNN model with the filled-in images from the year of 2019 to 2027 to\nextract the expiration dates and assess the model performance of LCBVAE on the\nexpiration date recognition. The pipeline of (LCBVAE+CRNN) can be then\nintegrated into an automated sorting systems for extracting the expiry dates\nand sorting the products accordingly during the manufacture stage.\nAdditionally, it can overcome the manual entry of expiration dates that can be\ntime-consuming and inefficient at the merchants. Due to the lack of the\navailability of the dotted Arabic expiration date images, we created an Arabic\ndot-matrix True Type Font (TTF) for the generation of the synthetic images. We\ntrained the model with unrealistic synthetic dates of 60,000 images and\nperformed the testing on a realistic synthetic date of 3000 images from the\nyear of 2019 to 2027, represented as yyyy/mm/dd. In our study, we demonstrated\nthe significance of latent bottleneck layer with improving the generalization\nwhen the size is increased up to 1024 in downstream transfer learning tasks as\nfor image translation. The proposed approach achieved an accuracy of 97% on the\nimage translation with using the LCBVAE architecture that can be generalized\nfor any downstream learning tasks as for image translation and reconstruction.\n","authors":["Ahmed Zidane","Ghada Soliman"],"pdf_url":"https://arxiv.org/pdf/2310.14069v2.pdf","comment":"* Corresponding author. E-mail address: ghada.soliman@orange.com\n  https://doi.org/10.1117/1.JEI.33.5.053024 Received: 14 April 2024; Accepted:\n  28 August 2024; Published: 30 September 2024"},{"id":"http://arxiv.org/abs/2409.20340v2","updated":"2024-10-01T14:14:32Z","published":"2024-09-30T14:39:56Z","title":"Enhancing GANs with Contrastive Learning-Based Multistage Progressive\n  Finetuning SNN and RL-Based External Optimization","summary":"  The application of deep learning in cancer research, particularly in early\ndiagnosis, case understanding, and treatment strategy design, emphasizes the\nneed for high-quality data. Generative AI, especially Generative Adversarial\nNetworks (GANs), has emerged as a leading solution to challenges like class\nimbalance, robust learning, and model training, while addressing issues\nstemming from patient privacy and the scarcity of real data. Despite their\npromise, GANs face several challenges, both inherent and specific to\nhistopathology data. Inherent issues include training imbalance, mode collapse,\nlinear learning from insufficient discriminator feedback, and hard boundary\nconvergence due to stringent feedback. Histopathology data presents a unique\nchallenge with its complex representation, high spatial resolution, and\nmultiscale features. To address these challenges, we propose a framework\nconsisting of two components. First, we introduce a contrastive learning-based\nMultistage Progressive Finetuning Siamese Neural Network (MFT-SNN) for\nassessing the similarity between histopathology patches. Second, we implement a\nReinforcement Learning-based External Optimizer (RL-EO) within the GAN training\nloop, serving as a reward signal generator. The modified discriminator loss\nfunction incorporates a weighted reward, guiding the GAN to maximize this\nreward while minimizing loss. This approach offers an external optimization\nguide to the discriminator, preventing generator overfitting and ensuring\nsmooth convergence. Our proposed solution has been benchmarked against\nstate-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,\noutperforming previous SOTA across various metrics, including FID score, KID\nscore, Perceptual Path Length, and downstream classification tasks.\n","authors":["Osama Mustafa"],"pdf_url":"https://arxiv.org/pdf/2409.20340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01453v3","updated":"2024-10-01T13:46:04Z","published":"2024-05-02T16:36:26Z","title":"Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?","summary":"  We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs\n","authors":["Lakshmi Nair","Evana Gizzi","Jivko Sinapov"],"pdf_url":"https://arxiv.org/pdf/2405.01453v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16167v2","updated":"2024-10-01T13:16:45Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06890v2","updated":"2024-10-01T13:10:40Z","published":"2024-08-13T13:36:48Z","title":"BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning","summary":"  Developing models with robust group fairness properties is paramount,\nparticularly in ethically sensitive domains such as medical diagnosis. Recent\napproaches to achieving fairness in machine learning require a substantial\namount of training data and depend on model retraining, which may not be\npractical in real-world scenarios. To mitigate these challenges, we propose\nBias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method\nthat enhances the fairness of a trained model in significantly fewer epochs\nwithout requiring access to the original training data. BMFT produces a mask\nover model parameters, which efficiently identifies the weights contributing\nthe most towards biased predictions. Furthermore, we propose a two-step\ndebiasing strategy, wherein the feature extractor undergoes initial fine-tuning\non the identified bias-influenced weights, succeeded by a fine-tuning phase on\na reinitialised classification layer to uphold discriminative performance.\nExtensive experiments across four dermatological datasets and two sensitive\nattributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA)\ntechniques in both diagnostic accuracy and fairness metrics. Our findings\nunderscore the efficacy and robustness of BMFT in advancing fairness across\nvarious out-of-distribution (OOD) settings. Our code is available at:\nhttps://github.com/vios-s/BMFT\n","authors":["Yuyang Xue","Junyu Yan","Raman Dutt","Fasih Haider","Jingshuai Liu","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2408.06890v2.pdf","comment":"Accepted by MICCAI 2024 FAIMI Workshop Oral"},{"id":"http://arxiv.org/abs/2406.09864v2","updated":"2024-10-01T13:07:02Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09706v2","updated":"2024-10-01T13:02:24Z","published":"2024-09-15T11:58:07Z","title":"Exploring Utility in a Real-World Warehouse Optimization Problem:\n  Formulation Based on Quantum Annealers and Preliminary Results","summary":"  In the current NISQ-era, one of the major challenges faced by researchers and\npractitioners lies in figuring out how to combine quantum and classical\ncomputing in the most efficient and innovative way. In this paper, we present a\nmechanism coined as Quantum Initialization for Warehouse Optimization Problem\nthat resorts to D-Wave's Quantum Annealer. The module has been specifically\ndesigned to be embedded into already existing classical software dedicated to\nthe optimization of a real-world industrial problem. We preliminary tested the\nimplemented mechanism through a two-phase experiment against the classical\nversion of the software.\n","authors":["Eneko Osaba","Esther Villar-Rodriguez","Antón Asla"],"pdf_url":"https://arxiv.org/pdf/2409.09706v2.pdf","comment":"2 pages, 2 figures. Paper presented at the 5th IEEE International\n  Conference on Quantum Computing and Engineering (IEEE QCE 2024)"},{"id":"http://arxiv.org/abs/2409.17946v2","updated":"2024-10-01T13:01:40Z","published":"2024-09-26T15:20:37Z","title":"Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation","summary":"  Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.\n","authors":["Shuai Zhao","Leilei Gan","Zhongliang Guo","Xiaobao Wu","Luwei Xiao","Xiaoyu Xu","Cong-Duy Nguyen","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2409.17946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00677v3","updated":"2024-10-01T12:45:01Z","published":"2023-07-02T22:30:08Z","title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary\n  Directed Differential with Normalized Density and Self-Adaption","summary":"  Density-based clustering could be the most popular clustering algorithm since\nit can identify clusters of arbitrary shape as long as they are separated by\nlow-density regions. However, a high-density region that is not separated by\nlow-density ones might also have different structures belonging to multiple\nclusters. As far as we know, all previous density-based clustering algorithms\nfail to detect such structures. In this paper, we provide a novel density-based\nclustering scheme that can not only detect clusters separated by low-density\nregions but also detect structures in high-density regions not separated by\nlow-density ones. The algorithm employs secondary directed differential,\nhierarchy, normalized density, as well as the self-adaption coefficient, and\nthus is called Structure Detecting Cluster by Hierarchical Secondary Directed\nDifferential with Normalized Density and Self-Adaption, dubbed by\nSDC-HSDD-NDSA. The algorithm is run on several datasets to verify its\neffectiveness, robustness, as well as granularity independence, and results\ndemonstrate that it has the ability that previous ones do not have. The Python\ncode is on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2307.00677v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2409.19924v2","updated":"2024-10-01T12:43:09Z","published":"2024-09-30T03:58:43Z","title":"On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability","summary":"  Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.\n","authors":["Kevin Wang","Junbo Li","Neel P. Bhatt","Yihan Xi","Qiang Liu","Ufuk Topcu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2409.19924v2.pdf","comment":"Updated link to code repository"},{"id":"http://arxiv.org/abs/2401.05949v5","updated":"2024-10-01T12:38:03Z","published":"2024-01-11T14:38:19Z","title":"Universal Vulnerabilities in Large Language Models: Backdoor Attacks for\n  In-context Learning","summary":"  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.\n","authors":["Shuai Zhao","Meihuizi Jia","Luu Anh Tuan","Fengjun Pan","Jinming Wen"],"pdf_url":"https://arxiv.org/pdf/2401.05949v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03728v2","updated":"2024-10-01T12:26:24Z","published":"2024-07-04T08:21:54Z","title":"Measuring Orthogonality in Representations of Generative Models","summary":"  In unsupervised representation learning, models aim to distill essential\nfeatures from high-dimensional data into lower-dimensional learned\nrepresentations, guided by inductive biases. Understanding the characteristics\nthat make a good representation remains a topic of ongoing research.\nDisentanglement of independent generative processes has long been credited with\nproducing high-quality representations. However, focusing solely on\nrepresentations that adhere to the stringent requirements of most\ndisentanglement metrics, may result in overlooking many high-quality\nrepresentations, well suited for various downstream tasks. These metrics often\ndemand that generative factors be encoded in distinct, single dimensions\naligned with the canonical basis of the representation space.\n  Motivated by these observations, we propose two novel metrics:\nImportance-Weighted Orthogonality (IWO) and Importance-Weighted Rank (IWR).\nThese metrics evaluate the mutual orthogonality and rank of generative factor\nsubspaces. Throughout extensive experiments on common downstream tasks, over\nseveral benchmark datasets and models, IWO and IWR consistently show stronger\ncorrelations with downstream task performance than traditional disentanglement\nmetrics. Our findings suggest that representation quality is closer related to\nthe orthogonality of independent generative processes rather than their\ndisentanglement, offering a new direction for evaluating and improving\nunsupervised learning models.\n","authors":["Robin C. Geyer","Alessandro Torcinovich","João B. Carvalho","Alexander Meyer","Joachim M. Buhmann"],"pdf_url":"https://arxiv.org/pdf/2407.03728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13947v2","updated":"2024-10-01T12:07:57Z","published":"2024-03-20T19:41:05Z","title":"BlendScape: Enabling End-User Customization of Video-Conferencing\n  Environments through Generative AI","summary":"  Today's video-conferencing tools support a rich range of professional and\nsocial activities, but their generic meeting environments cannot be dynamically\nadapted to align with distributed collaborators' needs. To enable end-user\ncustomization, we developed BlendScape, a rendering and composition system for\nvideo-conferencing participants to tailor environments to their meeting context\nby leveraging AI image generation techniques. BlendScape supports flexible\nrepresentations of task spaces by blending users' physical or digital\nbackgrounds into unified environments and implements multimodal interaction\ntechniques to steer the generation. Through an exploratory study with 15\nend-users, we investigated whether and how they would find value in using\ngenerative AI to customize video-conferencing environments. Participants\nenvisioned using a system like BlendScape to facilitate collaborative\nactivities in the future, but required further controls to mitigate distracting\nor unrealistic visual elements. We implemented scenarios to demonstrate\nBlendScape's expressiveness for supporting environment design strategies from\nprior work and propose composition techniques to improve the quality of\nenvironments.\n","authors":["Shwetha Rajaram","Nels Numan","Balasaravanan Thoravi Kumaravel","Nicolai Marquardt","Andrew D. Wilson"],"pdf_url":"https://arxiv.org/pdf/2403.13947v2.pdf","comment":"ACM UIST 2024"},{"id":"http://arxiv.org/abs/2409.09808v2","updated":"2024-10-01T12:03:49Z","published":"2024-09-15T18:02:26Z","title":"Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion","summary":"  Mamba and Vision Mamba (Vim) models have shown their potential as an\nalternative to methods based on Transformer architecture. This work introduces\nFast Mamba for Vision (Famba-V), a cross-layer token fusion technique to\nenhance the training efficiency of Vim models. The key idea of Famba-V is to\nidentify and fuse similar tokens across different Vim layers based on a suit of\ncross-layer strategies instead of simply applying token fusion uniformly across\nall the layers that existing works propose. We evaluate the performance of\nFamba-V on CIFAR-100. Our results show that Famba-V is able to enhance the\ntraining efficiency of Vim models by reducing both training time and peak\nmemory usage during training. Moreover, the proposed cross-layer strategies\nallow Famba-V to deliver superior accuracy-efficiency trade-offs. These results\nall together demonstrate Famba-V as a promising efficiency enhancement\ntechnique for Vim models.\n","authors":["Hui Shen","Zhongwei Wan","Xin Wang","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09808v2.pdf","comment":"Camera ready version of ECCV 2024 The Fourth Workshop on\n  Computational Aspects of Deep Learning (Best Paper Award)"},{"id":"http://arxiv.org/abs/2408.17198v2","updated":"2024-10-01T11:35:49Z","published":"2024-08-30T10:52:18Z","title":"Towards Symbolic XAI -- Explanation Through Human Understandable Logical\n  Relationships Between Features","summary":"  Explainable Artificial Intelligence (XAI) plays a crucial role in fostering\ntransparency and trust in AI systems, where traditional XAI approaches\ntypically offer one level of abstraction for explanations, often in the form of\nheatmaps highlighting single or multiple input features. However, we ask\nwhether abstract reasoning or problem-solving strategies of a model may also be\nrelevant, as these align more closely with how humans approach solutions to\nproblems. We propose a framework, called Symbolic XAI, that attributes\nrelevance to symbolic queries expressing logical relationships between input\nfeatures, thereby capturing the abstract reasoning behind a model's\npredictions. The methodology is built upon a simple yet general multi-order\ndecomposition of model predictions. This decomposition can be specified using\nhigher-order propagation-based relevance methods, such as GNN-LRP, or\nperturbation-based explanation methods commonly used in XAI. The effectiveness\nof our framework is demonstrated in the domains of natural language processing\n(NLP), vision, and quantum chemistry (QC), where abstract symbolic domain\nknowledge is abundant and of significant interest to users. The Symbolic XAI\nframework provides an understanding of the model's decision-making process that\nis both flexible for customization by the user and human-readable through\nlogical formulas.\n","authors":["Thomas Schnake","Farnoush Rezaei Jafari","Jonas Lederer","Ping Xiong","Shinichi Nakajima","Stefan Gugler","Grégoire Montavon","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2408.17198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06494v2","updated":"2024-10-01T11:10:59Z","published":"2024-07-09T01:56:23Z","title":"A Generative Approach to Control Complex Physical Systems","summary":"  Controlling the evolution of complex physical systems is a fundamental task\nacross science and engineering. Classical techniques suffer from limited\napplicability or huge computational costs. On the other hand, recent deep\nlearning and reinforcement learning-based approaches often struggle to optimize\nlong-term control sequences under the constraints of system dynamics. In this\nwork, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class\nof method to address the physical systems control problem. DiffPhyCon excels by\nsimultaneously minimizing both the learned generative energy function and the\npredefined control objectives across the entire trajectory and control\nsequence. Thus, it can explore globally and plan near-optimal control\nsequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the\ndiscovery of control sequences that significantly deviate from the training\ndistribution. We test our method on three tasks: 1D Burgers' equation, 2D\njellyfish movement control, and 2D high-dimensional smoke control, where our\ngenerated jellyfish dataset is released as a benchmark for complex physical\nsystem control research. Our method outperforms widely applied classical\napproaches and state-of-the-art deep learning and reinforcement learning\nmethods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern\nobserved in the jellyfish, aligning with established findings in the field of\nfluid dynamics. The project website, jellyfish dataset, and code can be found\nat https://github.com/AI4Science-WestlakeU/diffphycon.\n","authors":["Long Wei","Peiyan Hu","Ruiqi Feng","Haodong Feng","Yixuan Du","Tao Zhang","Rui Wang","Yue Wang","Zhi-Ming Ma","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2407.06494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07867v6","updated":"2024-10-01T10:46:26Z","published":"2023-10-11T20:16:38Z","title":"Cheap Talking Algorithms","summary":"  We simulate behaviour of two independent reinforcement learning algorithms\nplaying the Crawford and Sobel (1982) game of strategic information\ntransmission. We adopt memoryless algorithms to capture learning in a static\ngame where a large population interacts anonymously. We show that sender and\nreceiver converge to Nash equilibrium play. The level of informativeness of the\nsender's cheap talk decreases as the bias increases and, at intermediate level\nof the bias, it matches the level predicted by the Pareto optimal equilibrium\nor by the second best one. Conclusions are robust to alternative specifications\nof the learning hyperparameters and of the game.\n","authors":["Daniele Condorelli","Massimiliano Furlan"],"pdf_url":"https://arxiv.org/pdf/2310.07867v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19676v2","updated":"2024-10-01T10:42:32Z","published":"2024-09-29T12:08:20Z","title":"See Detail Say Clear: Towards Brain CT Report Generation via\n  Pathological Clue-driven Representation Learning","summary":"  Brain CT report generation is significant to aid physicians in diagnosing\ncranial diseases. Recent studies concentrate on handling the consistency\nbetween visual and textual pathological features to improve the coherence of\nreport. However, there exist some challenges: 1) Redundant visual representing:\nMassive irrelevant areas in 3D scans distract models from representing salient\nvisual contexts. 2) Shifted semantic representing: Limited medical corpus\ncauses difficulties for models to transfer the learned textual representations\nto generative layers. This study introduces a Pathological Clue-driven\nRepresentation Learning (PCRL) model to build cross-modal representations based\non pathological clues and naturally adapt them for accurate report generation.\nSpecifically, we construct pathological clues from perspectives of segmented\nregions, pathological entities, and report themes, to fully grasp visual\npathological patterns and learn cross-modal feature representations. To adapt\nthe representations for the text generation task, we bridge the gap between\nrepresentation learning and report generation by using a unified large language\nmodel (LLM) with task-tailored instructions. These crafted instructions enable\nthe LLM to be flexibly fine-tuned across tasks and smoothly transfer the\nsemantic representation for report generation. Experiments demonstrate that our\nmethod outperforms previous methods and achieves SoTA performance. Our code is\navailable at \"https://github.com/Chauncey-Jheng/PCRL-MRG\".\n","authors":["Chengxin Zheng","Junzhong Ji","Yanzhao Shi","Xiaodan Zhang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2409.19676v2.pdf","comment":"Our work has been accepted by EMNLP2024 findings"},{"id":"http://arxiv.org/abs/2409.15371v2","updated":"2024-10-01T10:00:49Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20138v2","updated":"2024-10-01T09:11:53Z","published":"2024-05-30T15:13:46Z","title":"Separation and Collapse of Equilibria Inequalities on AND-OR Trees\n  without Shape Constraints","summary":"  Herein, we investigate the zero-error randomized complexity, which is the\nleast cost against the worst input, of AND-OR tree computation by imposing\nvarious restrictions on the algorithm to find the Boolean value of the root of\nthat tree and no restrictions on the tree shape. When a tree satisfies a\ncertain condition regarding its symmetry, directional algorithms proposed by\nSaks and Wigderson (1986), special randomized algorithms, are known to achieve\nthe randomized complexity. Furthermore, there is a known example of a tree that\nis so unbalanced that no directional algorithm achieves the randomized\ncomplexity (Vereshchagin 1998). In this study, we aim to identify where\ndeviations arise between the general randomized Boolean decision tree and its\nspecial case, directional algorithms. In this paper, we show that for any\nAND-OR tree, randomized depth-first algorithms, which form a broader class\ncompared with directional algorithms, have the same equilibrium as that of the\ndirectional algorithms. Thus, we get the collapse result on equilibria\ninequalities that holds for an arbitrary AND-OR tree. This implies that there\nexists a case where even depth-first algorithms cannot be the fastest, leading\nto the separation result on equilibria inequality. Additionally, a new\nalgorithm is introduced as a key concept for proof of the separation result.\n","authors":["Fuki Ito","Toshio Suzuki"],"pdf_url":"https://arxiv.org/pdf/2405.20138v2.pdf","comment":"42 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.15301v2","updated":"2024-10-01T09:05:45Z","published":"2024-08-27T15:03:01Z","title":"The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization","summary":"  We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts.\n","authors":["Minghai Qin"],"pdf_url":"https://arxiv.org/pdf/2408.15301v2.pdf","comment":"27 pages, 41 figures"},{"id":"http://arxiv.org/abs/2405.12514v4","updated":"2024-10-01T09:00:57Z","published":"2024-05-21T06:00:51Z","title":"Future You: A Conversation with an AI-Generated Future Self Reduces\n  Anxiety, Negative Emotions, and Increases Future Self-Continuity","summary":"  We introduce \"Future You,\" an interactive, brief, single-session, digital\nchat intervention designed to improve future self-continuity--the degree of\nconnection an individual feels with a temporally distant future self--a\ncharacteristic that is positively related to mental health and wellbeing. Our\nsystem allows users to chat with a relatable yet AI-powered virtual version of\ntheir future selves that is tuned to their future goals and personal qualities.\nTo make the conversation realistic, the system generates a \"synthetic\nmemory\"--a unique backstory for each user--that creates a throughline between\nthe user's present age (between 18-30) and their life at age 60. The \"Future\nYou\" character also adopts the persona of an age-progressed image of the user's\npresent self. After a brief interaction with the \"Future You\" character, users\nreported decreased anxiety, and increased future self-continuity. This is the\nfirst study successfully demonstrating the use of personalized AI-generated\ncharacters to improve users' future self-continuity and wellbeing.\n","authors":["Pat Pataranutaporn","Kavin Winson","Peggy Yin","Auttasak Lapapirojn","Pichayoot Ouppaphan","Monchai Lertsutthiwong","Pattie Maes","Hal Hershfield"],"pdf_url":"https://arxiv.org/pdf/2405.12514v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08782v3","updated":"2024-10-01T08:54:53Z","published":"2023-12-14T10:02:55Z","title":"Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis","summary":"  Building general-purpose robots that operate seamlessly in any environment,\nwith any object, and utilizing various skills to complete diverse tasks has\nbeen a long-standing goal in Artificial Intelligence. However, as a community,\nwe have been constraining most robotic systems by designing them for specific\ntasks, training them on specific datasets, and deploying them within specific\nenvironments. These systems require extensively-labeled data and task-specific\nmodels. When deployed in real-world scenarios, such systems face several\ngeneralization issues and struggle to remain robust to distribution shifts.\nMotivated by the impressive open-set performance and content generation\ncapabilities of web-scale, large-capacity pre-trained models (i.e., foundation\nmodels) in research fields such as Natural Language Processing (NLP) and\nComputer Vision (CV), we devote this survey to exploring (i) how these existing\nfoundation models from NLP and CV can be applied to the field of\ngeneral-purpose robotics, and also exploring (ii) what a robotics-specific\nfoundation model would look like. We begin by providing a generalized\nformulation of how foundation models are used in robotics, and the fundamental\nbarriers to making generalist robots universally applicable. Next, we establish\na taxonomy to discuss current work exploring ways to leverage existing\nfoundation models for robotics and develop ones catered to robotics. Finally,\nwe discuss key challenges and promising future directions in using foundation\nmodels for enabling general-purpose robotic systems. We encourage readers to\nview our living GitHub repository 2 of resources, including papers reviewed in\nthis survey, as well as related projects and repositories for developing\nfoundation models for robotics.\n","authors":["Yafei Hu","Quanting Xie","Vidhi Jain","Jonathan Francis","Jay Patrikar","Nikhil Keetha","Seungchan Kim","Yaqi Xie","Tianyi Zhang","Hao-Shu Fang","Shibo Zhao","Shayegan Omidshafiei","Dong-Ki Kim","Ali-akbar Agha-mohammadi","Katia Sycara","Matthew Johnson-Roberson","Dhruv Batra","Xiaolong Wang","Sebastian Scherer","Chen Wang","Zsolt Kira","Fei Xia","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2312.08782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18708v3","updated":"2024-10-01T08:50:01Z","published":"2024-09-27T12:54:13Z","title":"Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity","summary":"  We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2409.18708v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10691v2","updated":"2024-10-01T08:48:34Z","published":"2024-08-20T09:42:17Z","title":"Fine-Tuning and Deploying Large Language Models Over Edges: Issues and\n  Approaches","summary":"  Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have\ntransitioned from specialized models to versatile foundation models. The LLMs\nexhibit impressive zero-shot ability, however, require fine-tuning on local\ndatasets and significant resources for deployment. Traditional fine-tuning\ntechniques with the first-order optimizers require substantial GPU memory that\nexceeds mainstream hardware capability. Therefore, memory-efficient methods are\nmotivated to be investigated. Model compression techniques can reduce energy\nconsumption, operational costs, and environmental impact so that to support\nsustainable artificial intelligence advancements. Additionally, large-scale\nfoundation models have expanded to create images, audio, videos, and\nmulti-modal contents, further emphasizing the need for efficient deployment.\nTherefore, we are motivated to present a comprehensive overview of the\nprevalent memory-efficient fine-tuning methods over the network edge. We also\nreview the state-of-the-art literatures on model compression to provide a\nvision on deploying LLMs over the network edge.\n","authors":["Yanjie Dong","Haijun Zhang","Chengming Li","Song Guo","Victor C. M. Leung","Xiping Hu"],"pdf_url":"https://arxiv.org/pdf/2408.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19541v2","updated":"2024-10-01T08:30:13Z","published":"2024-09-29T03:56:50Z","title":"Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance\n  Regularization","summary":"  Language models frequently inherit societal biases from their training data.\nNumerous techniques have been proposed to mitigate these biases during both the\npre-training and fine-tuning stages. However, fine-tuning a pre-trained\ndebiased language model on a downstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing methods for downstream tasks either (i)\nrequire labels of protected attributes (e.g., age, race, or political views)\nthat are often not available or (ii) rely on indicators of bias, which\nrestricts their applicability to gender debiasing since they rely on\ngender-specific words. To address this, we introduce a novel debiasing\nregularization technique based on the class-wise variance of embeddings.\nCrucially, our method does not require attribute labels and targets any\nattribute, thus addressing the shortcomings of existing debiasing methods. Our\nexperiments on encoder language models and three datasets demonstrate that our\nmethod outperforms existing strong debiasing baselines that rely on target\nattribute labels while maintaining performance on the target task.\n","authors":["Shahed Masoudian","Markus Frohman","Navid Rekabsaz","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2409.19541v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.05395v2","updated":"2024-10-01T08:29:53Z","published":"2024-09-09T07:49:09Z","title":"Shaking Up VLMs: Comparing Transformers and Structured State Space\n  Models for Vision & Language Modeling","summary":"  This study explores replacing Transformers in Visual Language Models (VLMs)\nwith Mamba, a recent structured state space model (SSM) that demonstrates\npromising performance in sequence modeling. We test models up to 3B parameters\nunder controlled conditions, showing that Mamba-based VLMs outperforms\nTransformers-based VLMs in captioning, question answering, and reading\ncomprehension. However, we find that Transformers achieve greater performance\nin visual grounding and the performance gap widens with scale. We explore two\nhypotheses to explain this phenomenon: 1) the effect of task-agnostic visual\nencoding on the updates of the hidden states, and 2) the difficulty in\nperforming visual grounding from the perspective of in-context multimodal\nretrieval. Our results indicate that a task-aware encoding yields minimal\nperformance gains on grounding, however, Transformers significantly outperform\nMamba at in-context multimodal retrieval. Overall, Mamba shows promising\nperformance on tasks where the correct output relies on a summary of the image\nbut struggles when retrieval of explicit information from the context is\nrequired.\n","authors":["Georgios Pantazopoulos","Malvina Nikandrou","Alessandro Suglia","Oliver Lemon","Arash Eshghi"],"pdf_url":"https://arxiv.org/pdf/2409.05395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17985v2","updated":"2024-10-01T08:25:14Z","published":"2024-01-31T16:44:20Z","title":"Individual mapping of large polymorphic shrubs in high mountains using\n  satellite images and deep learning","summary":"  Monitoring the distribution and size of long-living large shrubs, such as\njunipers, is crucial for assessing the long-term impacts of global change on\nhigh-mountain ecosystems. While deep learning models have shown remarkable\nsuccess in object segmentation, adapting these models to detect shrub species\nwith polymorphic nature remains challenging. In this research, we release a\nlarge dataset of individual shrub delineations on freely available satellite\nimagery and use an instance segmentation model to map all junipers over the\ntreeline for an entire biosphere reserve (Sierra Nevada, Spain). To optimize\nperformance, we introduced a novel dual data construction approach: using\nphoto-interpreted (PI) data for model development and fieldwork (FW) data for\nvalidation. To account for the polymorphic nature of junipers during model\nevaluation, we developed a soft version of the Intersection over Union metric.\nFinally, we assessed the uncertainty of the resulting map in terms of canopy\ncover and density of shrubs per size class. Our model achieved an F1-score in\nshrub delineation of 87.87% on the PI data and 76.86% on the FW data. The R2\nand RMSE of the observed versus predicted relationship were 0.63 and 6.67% for\ncanopy cover, and 0.90 and 20.62 for shrub density. The greater density of\nlarger shrubs in lower altitudes and smaller shrubs in higher altitudes\nobserved in the model outputs was also present in the PI and FW data,\nsuggesting an altitudinal uplift in the optimal performance of the species.\nThis study demonstrates that deep learning applied on freely available\nhigh-resolution satellite imagery is useful to detect medium to large shrubs of\nhigh ecological value at the regional scale, which could be expanded to other\nhigh-mountains worldwide and to historical and forthcoming imagery.\n","authors":["Rohaifa Khaldi","Siham Tabik","Sergio Puertas-Ruiz","Julio Peñas de Giles","José Antonio Hódar Correa","Regino Zamora","Domingo Alcaraz Segura"],"pdf_url":"https://arxiv.org/pdf/2401.17985v2.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2210.05222v2","updated":"2024-10-01T08:15:02Z","published":"2022-10-11T07:40:45Z","title":"Stochastic Direct Search Method for Blind Resource Allocation","summary":"  Motivated by programmatic advertising optimization, we consider the task of\nsequentially allocating budget across a set of resources. At every time step, a\nfeasible allocation is chosen and only a corresponding random return is\nobserved. The goal is to maximize the cumulative expected sum of returns. This\nis a realistic model for budget allocation across subdivisions of marketing\ncampaigns, with the objective of maximizing the number of conversions. We study\ndirect search (also known as pattern search) methods for linearly constrained\nand derivative-free optimization in the presence of noise, which apply in\nparticular to sequential budget allocation. These algorithms, which do not rely\non hierarchical partitioning of the resource space, are easy to implement; they\nrespect the operational constraints of resource allocation by avoiding\nevaluation outside of the feasible domain; and they are also compatible with\nwarm start by being (approximate) descent algorithms. However, they have not\nyet been analyzed from the perspective of cumulative regret. We show that\ndirect search methods achieves finite regret in the deterministic and\nunconstrained case. In the presence of evaluation noise and linear constraints,\nwe propose a simple extension of direct search that achieves a regret\nupper-bound of the order of $T^{2/3}$. We also propose an accelerated version\nof the algorithm, relying on repeated sequential testing, that significantly\nimproves the practical behavior of the approach.\n","authors":["Juliette Achddou","Olivier Cappe","Aurélien Garivier"],"pdf_url":"https://arxiv.org/pdf/2210.05222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17518v2","updated":"2024-10-01T08:08:42Z","published":"2024-09-26T04:01:15Z","title":"Multi-Designated Detector Watermarking for Language Models","summary":"  In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.\n","authors":["Zhengan Huang","Gongxian Zeng","Xin Mu","Yu Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2409.17518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09839v2","updated":"2024-10-01T07:50:41Z","published":"2024-08-19T09:35:51Z","title":"Segment-Anything Models Achieve Zero-shot Robustness in Autonomous\n  Driving","summary":"  Semantic segmentation is a significant perception task in autonomous driving.\nIt suffers from the risks of adversarial examples. In the past few years, deep\nlearning has gradually transitioned from convolutional neural network (CNN)\nmodels with a relatively small number of parameters to foundation models with a\nhuge number of parameters. The segment-anything model (SAM) is a generalized\nimage segmentation framework that is capable of handling various types of\nimages and is able to recognize and segment arbitrary objects in an image\nwithout the need to train on a specific object. It is a unified model that can\nhandle diverse downstream tasks, including semantic segmentation, object\ndetection, and tracking. In the task of semantic segmentation for autonomous\ndriving, it is significant to study the zero-shot adversarial robustness of\nSAM. Therefore, we deliver a systematic empirical study on the robustness of\nSAM without additional training. Based on the experimental results, the\nzero-shot adversarial robustness of the SAM under the black-box corruptions and\nwhite-box adversarial attacks is acceptable, even without the need for\nadditional training. The finding of this study is insightful in that the\ngigantic model parameters and huge amounts of training data lead to the\nphenomenon of emergence, which builds a guarantee of adversarial robustness.\nSAM is a vision foundation model that can be regarded as an early prototype of\nan artificial general intelligence (AGI) pipeline. In such a pipeline, a\nunified model can handle diverse tasks. Therefore, this research not only\ninspects the impact of vision foundation models on safe autonomous driving but\nalso provides a perspective on developing trustworthy AGI. The code is\navailable at: https://github.com/momo1986/robust_sam_iv.\n","authors":["Jun Yan","Pengyu Wang","Danni Wang","Weiquan Huang","Daniel Watzenig","Huilin Yin"],"pdf_url":"https://arxiv.org/pdf/2408.09839v2.pdf","comment":"Accepted to IAVVC 2024"},{"id":"http://arxiv.org/abs/2402.15898v4","updated":"2024-10-01T07:45:38Z","published":"2024-02-13T09:22:45Z","title":"Transductive Active Learning: Theory and Applications","summary":"  We generalize active learning to address real-world settings with concrete\nprediction targets where sampling is restricted to an accessible region of the\ndomain, while prediction targets may lie outside this region. We analyze a\nfamily of decision rules that sample adaptively to minimize uncertainty about\nprediction targets. We are the first to show, under general regularity\nassumptions, that such decision rules converge uniformly to the smallest\npossible uncertainty obtainable from the accessible data. We demonstrate their\nstrong sample efficiency in two key applications: Active few-shot fine-tuning\nof large neural networks and safe Bayesian optimization, where they improve\nsignificantly upon the state-of-the-art.\n","authors":["Jonas Hübotter","Bhavya Sukhija","Lenart Treven","Yarden As","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2402.15898v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.15441"},{"id":"http://arxiv.org/abs/2409.18618v2","updated":"2024-10-01T07:29:06Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.04813v2","updated":"2024-10-01T07:28:39Z","published":"2024-09-07T12:53:44Z","title":"Generalized Learning of Coefficients in Spectral Graph Convolutional\n  Networks","summary":"  Spectral Graph Convolutional Networks (GCNs) have gained popularity in graph\nmachine learning applications due, in part, to their flexibility in\nspecification of network propagation rules. These propagation rules are often\nconstructed as polynomial filters whose coefficients are learned using label\ninformation during training. In contrast to learned polynomial filters,\nexplicit filter functions are useful in capturing relationships between network\ntopology and distribution of labels across the network. A number of algorithms\nincorporating either approach have been proposed; however the relationship\nbetween filter functions and polynomial approximations is not fully resolved.\nThis is largely due to the ill-conditioned nature of the linear systems that\nmust be solved to derive polynomial approximations of filter functions. To\naddress this challenge, we propose a novel Arnoldi orthonormalization-based\nalgorithm, along with a unifying approach, called G-Arnoldi-GCN that can\nefficiently and effectively approximate a given filter function with a\npolynomial. We evaluate G-Arnoldi-GCN in the context of multi-class node\nclassification across ten datasets with diverse topological characteristics.\nOur experiments show that G-Arnoldi-GCN consistently outperforms\nstate-of-the-art methods when suitable filter functions are employed. Overall,\nG-Arnoldi-GCN opens important new directions in graph machine learning by\nenabling the explicit design and application of diverse filter functions. Code\nlink: https://github.com/mustafaCoskunAgu/GArnoldi-GCN\n","authors":["Mustafa Coşkun","Ananth Grama","Mehmet Koyutürk"],"pdf_url":"https://arxiv.org/pdf/2409.04813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19898v2","updated":"2024-10-01T07:11:44Z","published":"2024-09-30T02:56:35Z","title":"UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional\n  Summarization Evaluation for LLMs","summary":"  Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.\n","authors":["Yuho Lee","Taewon Yun","Jason Cai","Hang Su","Hwanjun Song"],"pdf_url":"https://arxiv.org/pdf/2409.19898v2.pdf","comment":"Accepted at EMNLP-Findings 2024"},{"id":"http://arxiv.org/abs/2301.10874v2","updated":"2024-10-01T07:10:39Z","published":"2023-01-25T23:47:34Z","title":"Recursive deep learning framework for forecasting the decadal world\n  economic outlook","summary":"  The gross domestic product (GDP) is the most widely used indicator in\nmacroeconomics and the main tool for measuring a country's economic output. Due\nto the diversity and complexity of the world economy, a wide range of models\nhave been used, but there are challenges in making decadal GDP forecasts given\nunexpected changes such as emergence of catastrophic world events including\npandemics and wars. Deep learning models are well suited for modelling temporal\nsequences and time series forecasting. In this paper, we develop a deep\nlearning framework to forecast the GDP growth rate of the world economy over a\ndecade. We use the Penn World Table as the data source featuring 13 countries\nprior to the COVID-19 pandemic, such as Australia, China, India, and the United\nStates. We present a recursive deep learning framework to predict the GDP\ngrowth rate in the next ten years. We test prominent deep learning models and\ncompare their results with traditional econometric models for selected\ndeveloped and developing countries. Our decadal forecasts reveal that that most\nof the developed countries would experience economic growth slowdown,\nstagnation and even recession within five years (2020-2024). Furthermore, our\nmodel forecasts show that only China, France, and India would experience stable\nGDP growth.\n","authors":["Tianyi Wang","Rodney Beard","John Hawkins","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2301.10874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14119v2","updated":"2024-10-01T07:10:02Z","published":"2024-09-21T12:20:18Z","title":"Obliviate: Neutralizing Task-agnostic Backdoors within the\n  Parameter-efficient Fine-tuning Paradigm","summary":"  Parameter-efficient fine-tuning (PEFT) has become a key training strategy for\nlarge language models. However, its reliance on fewer trainable parameters\nposes security risks, such as task-agnostic backdoors. Despite their severe\nimpact on a wide range of tasks, there is no practical defense solution\navailable that effectively counters task-agnostic backdoors within the context\nof PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor\ndefense. We develop two techniques aimed at amplifying benign neurons within\nPEFT layers and penalizing the influence of trigger tokens. Our evaluations\nacross three major PEFT architectures show that our method can significantly\nreduce the attack success rate of the state-of-the-art task-agnostic backdoors\n(83.6%$\\downarrow$). Furthermore, our method exhibits robust defense\ncapabilities against both task-specific backdoors and adaptive attacks. Source\ncode will be obtained at https://github.com/obliviateARR/Obliviate.\n","authors":["Jaehan Kim","Minkyoo Song","Seung Ho Na","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2409.14119v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.19422v2","updated":"2024-10-01T07:04:04Z","published":"2024-09-28T17:43:17Z","title":"Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures","summary":"  A core task in multi-modal learning is to integrate information from multiple\nfeature spaces (e.g., text and audio), offering modality-invariant essential\nrepresentations of data. Recent research showed that, classical tools such as\n{\\it canonical correlation analysis} (CCA) provably identify the shared\ncomponents up to minor ambiguities, when samples in each modality are generated\nfrom a linear mixture of shared and private components. Such identifiability\nresults were obtained under the condition that the cross-modality samples are\naligned/paired according to their shared information. This work takes a step\nfurther, investigating shared component identifiability from multi-modal linear\nmixtures where cross-modality samples are unaligned. A distribution divergence\nminimization-based loss is proposed, under which a suite of sufficient\nconditions ensuring identifiability of the shared components are derived. Our\nconditions are based on cross-modality distribution discrepancy\ncharacterization and density-preserving transform removal, which are much\nmilder than existing studies relying on independent component analysis. More\nrelaxed conditions are also provided via adding reasonable structural\nconstraints, motivated by available side information in various applications.\nThe identifiability claims are thoroughly validated using synthetic and\nreal-world data.\n","authors":["Subash Timilsina","Sagar Shrestha","Xiao Fu"],"pdf_url":"https://arxiv.org/pdf/2409.19422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13208v3","updated":"2024-10-01T06:42:29Z","published":"2024-09-20T04:32:54Z","title":"Redefining Data Pairing for Motion Retargeting Leveraging a Human Body\n  Prior","summary":"  We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a\ncost-effective and convenient method to collect high-quality upper body paired\n<robot, human> pose data, which is essential for data-driven motion retargeting\nmethods. Unlike existing approaches which collect <robot, human> pose data by\nconverting human MoCap poses into robot poses, our method goes in reverse. We\nfirst sample diverse random robot poses, and then convert them into human\nposes. However, since random robot poses can result in extreme and infeasible\nhuman poses, we propose an additional technique to sort out extreme poses by\nexploiting a human body prior trained from a large amount of human pose data.\nOur data collection method can be used for any humanoid robots, if one designs\nor optimizes the system's hyperparameters which include a size scale factor and\nthe joint angle ranges for sampling. In addition to this data collection\nmethod, we also present a two-stage motion retargeting neural network that can\nbe trained via supervised learning on a large amount of paired data. Compared\nto other learning-based methods trained via unsupervised learning, we found\nthat our deep neural network trained with ample high-quality paired data\nachieved notable performance. Our experiments also show that our data filtering\nmethod yields better retargeting results than training the model with raw and\nnoisy data. Our code and video results are available on\nhttps://sites.google.com/view/mr-hubo/\n","authors":["Xiyana Figuera","Soogeun Park","Hyemin Ahn"],"pdf_url":"https://arxiv.org/pdf/2409.13208v3.pdf","comment":"8 pages, 5 Figures, Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2409.19663v2","updated":"2024-10-01T06:35:24Z","published":"2024-09-29T11:29:57Z","title":"Identifying Knowledge Editing Types in Large Language Models","summary":"  Knowledge editing has emerged as an efficient technology for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technology, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, Knowledge Editing Type Identification (KETI), aimed at\nidentifying different types of edits in LLMs, thereby providing timely alerts\nto users when encountering illicit edits. As part of this task, we propose\nKETIBench, which includes five types of harmful edits covering most popular\ntoxic types, as well as one benign factual edit. We develop four classical\nclassification models and three BERT-based models as baseline identifiers for\nboth open-source and closed-source LLMs. Our experimental results, across 42\ntrials involving two models and three knowledge editing methods, demonstrate\nthat all seven baseline identifiers achieve decent identification performance,\nhighlighting the feasibility of identifying malicious edits in LLMs. Additional\nanalyses reveal that the performance of the identifiers is independent of the\nreliability of the knowledge editing methods and exhibits cross-domain\ngeneralization, enabling the identification of edits from unknown sources. All\ndata and code are available in https://github.com/xpq-tech/KETI. Warning: This\npaper contains examples of toxic text.\n","authors":["Xiaopeng Li","Shangwen Wang","Shezheng Song","Bin Ji","Huijun Liu","Shasha Li","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2409.19663v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.07198v2","updated":"2024-10-01T05:52:11Z","published":"2024-04-10T17:56:07Z","title":"A Foundation Model for Zero-shot Logical Query Reasoning","summary":"  Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond\nsimple KG completion and aims at answering compositional queries comprised of\nmultiple projections and logical operations. Existing CLQA methods that learn\nparameters bound to certain entity or relation vocabularies can only be applied\nto the graph they are trained on which requires substantial training time\nbefore being deployed on a new graph. Here we present UltraQuery, the first\nfoundation model for inductive reasoning that can zero-shot answer logical\nqueries on any KG. The core idea of UltraQuery is to derive both projections\nand logical operations as vocabulary-independent functions which generalize to\nnew entities and relations in any KG. With the projection operation initialized\nfrom a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on\nany KG after finetuning on a single dataset. Experimenting on 23 datasets,\nUltraQuery in the zero-shot inference mode shows competitive or better query\nanswering performance than best available baselines and sets a new state of the\nart on 15 of them.\n","authors":["Mikhail Galkin","Jincheng Zhou","Bruno Ribeiro","Jian Tang","Zhaocheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.07198v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.08642v2","updated":"2024-10-01T05:42:12Z","published":"2024-09-13T08:59:31Z","title":"CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks","summary":"  Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%).\n","authors":["Tianlong Wang","Junzhe Chen","Xueting Han","Jing Bai"],"pdf_url":"https://arxiv.org/pdf/2409.08642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13647v2","updated":"2024-10-01T05:28:54Z","published":"2024-07-18T16:25:17Z","title":"Weak-to-Strong Reasoning","summary":"  When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervision for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.\n","authors":["Yuqing Yang","Yan Ma","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13647v2.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2403.04732v3","updated":"2024-10-01T04:41:53Z","published":"2024-03-07T18:35:54Z","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","summary":"  Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.\n","authors":["Yizhe Zhang","He Bai","Ruixiang Zhang","Jiatao Gu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2403.04732v3.pdf","comment":"COLM 2024. https://github.com/apple/ml-rpm-bench"},{"id":"http://arxiv.org/abs/2312.02186v2","updated":"2024-10-01T04:39:14Z","published":"2023-12-01T20:16:02Z","title":"Identifying Spurious Correlations using Counterfactual Alignment","summary":"  Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual (CF) alignment method to detect and\nquantify spurious correlations of black box classifiers. Our methodology is\nbased on counterfactual images generated with respect to one classifier being\ninput into other classifiers to see if they also induce changes in the outputs\nof these classifiers. The relationship between these responses can be\nquantified and used to identify specific instances where a spurious correlation\nexists. This is validated by observing intuitive trends in a face-attribute\nface-attribute and waterbird classifiers, as well as by fabricating spurious\ncorrelations and detecting their presence, both visually and quantitatively.\nFurthermore, utilizing the CF alignment method, we demonstrate that we can\nevaluate robust optimization methods (GroupDRO, JTT, and FLAC) by detecting a\nreduction in spurious correlations.\n","authors":["Joseph Paul Cohen","Louis Blankemeier","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2312.02186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19894v2","updated":"2024-10-01T04:35:05Z","published":"2024-09-30T02:53:03Z","title":"TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation","summary":"  Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.\n","authors":["Zhiqiang Yuan","Weitong Chen","Hanlin Wang","Kai Yu","Xin Peng","Yiling Lou"],"pdf_url":"https://arxiv.org/pdf/2409.19894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12784v3","updated":"2024-10-01T04:19:55Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing text-to-image models can\ncorrectly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K\ndiverse image-text pairs across nine categories with 1,000 rigorously curated\nquestions covering various compositional challenges. We evaluate five\ntext-to-image models using I-HallA and reveal that these state-of-the-art\nmodels often fail to accurately convey factual information. Moreover, we\nvalidate the reliability of our metric by demonstrating a strong Spearman\ncorrelation (rho=0.95) with human judgments. We believe our benchmark dataset\nand metric can serve as a foundation for developing factually accurate\ntext-to-image generation models.\n","authors":["Youngsun Lim","Hojun Choi","Pin-Yu Chen","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2401.12261v4","updated":"2024-10-01T03:41:26Z","published":"2024-01-22T00:37:01Z","title":"Cloud-based XAI Services for Assessing Open Repository Models Under\n  Adversarial Attacks","summary":"  The opacity of AI models necessitates both validation and evaluation before\ntheir integration into services. To investigate these models, explainable AI\n(XAI) employs methods that elucidate the relationship between input features\nand output predictions. The operations of XAI extend beyond the execution of a\nsingle algorithm, involving a series of activities that include preprocessing\ndata, adjusting XAI to align with model parameters, invoking the model to\ngenerate predictions, and summarizing the XAI results. Adversarial attacks are\nwell-known threats that aim to mislead AI models. The assessment complexity,\nespecially for XAI, increases when open-source AI models are subject to\nadversarial attacks, due to various combinations. To automate the numerous\nentities and tasks involved in XAI-based assessments, we propose a cloud-based\nservice framework that encapsulates computing components as microservices and\norganizes assessment tasks into pipelines. The current XAI tools are not\ninherently service-oriented. This framework also integrates open XAI tool\nlibraries as part of the pipeline composition. We demonstrate the application\nof XAI services for assessing five quality attributes of AI models: (1)\ncomputational cost, (2) performance, (3) robustness, (4) explanation deviation,\nand (5) explanation resilience across computer vision and tabular cases. The\nservice framework generates aggregated analysis that showcases the quality\nattributes for more than a hundred combination scenarios.\n","authors":["Zerui Wang","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.12261v4.pdf","comment":"2024 IEEE International Conference on Software Services Engineering\n  (SSE)"},{"id":"http://arxiv.org/abs/2409.15355v3","updated":"2024-10-01T03:40:08Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17411v3","updated":"2024-10-01T02:54:41Z","published":"2024-09-25T22:48:14Z","title":"Exploring Semantic Clustering in Deep Reinforcement Learning for Video\n  Games","summary":"  In this paper, we investigate the semantic clustering properties of deep\nreinforcement learning (DRL) for video games, enriching our understanding of\nthe internal dynamics of DRL and advancing its interpretability. In this\ncontext, semantic clustering refers to the inherent capacity of neural networks\nto internally group video inputs based on semantic similarity. To achieve this,\nwe propose a novel DRL architecture that integrates a semantic clustering\nmodule featuring both feature dimensionality reduction and online clustering.\nThis module seamlessly integrates into the DRL training pipeline, addressing\ninstability issues observed in previous t-SNE-based analysis methods and\neliminating the necessity for extensive manual annotation of semantic analysis.\nThrough experiments, we validate the effectiveness of the proposed module and\nthe semantic clustering properties in DRL for video games. Additionally, based\non these properties, we introduce new analytical methods to help understand the\nhierarchical structure of policies and the semantic distribution within the\nfeature space.\n","authors":["Liang Zhang","Justin Lieffers","Adarsh Pyarelal"],"pdf_url":"https://arxiv.org/pdf/2409.17411v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19546v2","updated":"2024-10-01T02:33:09Z","published":"2024-09-29T04:16:24Z","title":"Almost Sure Convergence of Average Reward Temporal Difference Learning","summary":"  Tabular average reward Temporal Difference (TD) learning is perhaps the\nsimplest and the most fundamental policy evaluation algorithm in average reward\nreinforcement learning. After at least 25 years since its discovery, we are\nfinally able to provide a long-awaited almost sure convergence analysis.\nNamely, we are the first to prove that, under very mild conditions, tabular\naverage reward TD converges almost surely to a sample path dependent fixed\npoint. Key to this success is a new general stochastic approximation result\nconcerning nonexpansive mappings with Markovian and additive noise, built on\nrecent advances in stochastic Krasnoselskii-Mann iterations.\n","authors":["Ethan Blaser","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13659v3","updated":"2024-10-01T02:12:41Z","published":"2024-08-24T19:19:33Z","title":"ReactZyme: A Benchmark for Enzyme-Reaction Prediction","summary":"  Enzymes, with their specific catalyzed reactions, are necessary for all\naspects of life, enabling diverse biological processes and adaptations.\nPredicting enzyme functions is essential for understanding biological pathways,\nguiding drug development, enhancing bioproduct yields, and facilitating\nevolutionary studies. Addressing the inherent complexities, we introduce a new\napproach to annotating enzymes based on their catalyzed reactions. This method\nprovides detailed insights into specific reactions and is adaptable to newly\ndiscovered reactions, diverging from traditional classifications by protein\nfamily or expert-derived reaction classes. We employ machine learning\nalgorithms to analyze enzyme reaction datasets, delivering a much more refined\nview on the functionality of enzymes. Our evaluation leverages the largest\nenzyme-reaction dataset to date, derived from the SwissProt and Rhea databases\nwith entries up to January 8, 2024. We frame the enzyme-reaction prediction as\na retrieval problem, aiming to rank enzymes by their catalytic ability for\nspecific reactions. With our model, we can recruit proteins for novel reactions\nand predict reactions in novel proteins, facilitating enzyme discovery and\nfunction annotation (https://github.com/WillHua127/ReactZyme).\n","authors":["Chenqing Hua","Bozitao Zhong","Sitao Luan","Liang Hong","Guy Wolf","Doina Precup","Shuangjia Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.13659v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15963v3","updated":"2024-10-01T02:00:50Z","published":"2024-09-24T10:48:13Z","title":"Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning","summary":"  To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with guaranteed efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments.\n","authors":["Bo Yue","Jian Li","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15963v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03247v3","updated":"2024-10-01T01:48:58Z","published":"2024-08-06T15:07:08Z","title":"Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons","summary":"  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n","authors":["Yifei Wang","Yuheng Chen","Wanting Wen","Yu Sheng","Linjing Li","Daniel Dajun Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.03247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00760v2","updated":"2024-10-01T01:04:58Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\nhttps://github.com/SusungHong/SEG-SDXL.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.08611v2","updated":"2024-10-01T00:14:32Z","published":"2024-04-12T17:20:57Z","title":"Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin\n  Lymphoma Patients Using a Longitudinally-Aware Segmentation Network","summary":"  $\\textbf{Purpose}$: Automatic quantification of longitudinal changes in PET\nscans for lymphoma patients has proven challenging, as residual disease in\ninterim-therapy scans is often subtle and difficult to detect. Our goal was to\ndevelop a longitudinally-aware segmentation network (LAS-Net) that can quantify\nserial PET/CT images for pediatric Hodgkin lymphoma patients.\n$\\textbf{Materials and Methods}$: This retrospective study included baseline\n(PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two\nChildren's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net\nincorporates longitudinal cross-attention, allowing relevant features from PET1\nto inform the analysis of PET2. Model performance was evaluated using Dice\ncoefficients for PET1 and detection F1 scores for PET2. Additionally, we\nextracted and compared quantitative PET metrics, including metabolic tumor\nvolume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and\n$\\Delta$SUVmax in PET2, against physician measurements. We quantified their\nagreement using Spearman's $\\rho$ correlations and employed bootstrap\nresampling for statistical analysis. $\\textbf{Results}$: LAS-Net detected\nresidual lymphoma in PET2 with an F1 score of 0.606 (precision/recall:\n0.615/0.600), outperforming all comparator methods (P<0.01). For baseline\nsegmentation, LAS-Net achieved a mean Dice score of 0.772. In PET\nquantification, LAS-Net's measurements of qPET, $\\Delta$SUVmax, MTV and TLG\nwere strongly correlated with physician measurements, with Spearman's $\\rho$ of\n0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a\nslight decrease, in an external testing cohort. $\\textbf{Conclusion}$: LAS-Net\ndemonstrated significant improvements in quantifying PET metrics across serial\nscans, highlighting the value of longitudinal awareness in evaluating\nmulti-time-point imaging datasets.\n","authors":["Xin Tie","Muheon Shin","Changhee Lee","Scott B. Perlman","Zachary Huemann","Amy J. Weisman","Sharon M. Castellino","Kara M. Kelly","Kathleen M. McCarten","Adina L. Alazraki","Junjie Hu","Steve Y. Cho","Tyler J. Bradshaw"],"pdf_url":"https://arxiv.org/pdf/2404.08611v2.pdf","comment":"There are 6 figures and 4 tables in the main text. The supplementary\n  material is appended to the main text"},{"id":"http://arxiv.org/abs/2405.19420v2","updated":"2024-10-01T00:14:07Z","published":"2024-05-29T18:01:58Z","title":"Using Contrastive Learning with Generative Similarity to Learn Spaces\n  that Capture Human Inductive Biases","summary":"  Humans rely on strong inductive biases to learn from few examples and\nabstract useful information from sensory data. Instilling such biases in\nmachine learning models has been shown to improve their performance on various\nbenchmarks including few-shot learning, robustness, and alignment. However,\nfinding effective training procedures to achieve that goal can be challenging\nas psychologically-rich training data such as human similarity judgments are\nexpensive to scale, and Bayesian models of human inductive biases are often\nintractable for complex, realistic domains. Here, we address this challenge by\nintroducing a Bayesian notion of generative similarity whereby two datapoints\nare considered similar if they are likely to have been sampled from the same\ndistribution. This measure can be applied to complex generative processes,\nincluding probabilistic programs. We show that generative similarity can be\nused to define a contrastive learning objective even when its exact form is\nintractable, enabling learning of spatial embeddings that express specific\ninductive biases. We demonstrate the utility of our approach by showing that it\ncan be used to capture human inductive biases for geometric shapes, distinguish\ndifferent abstract drawing styles that are parameterized by probabilistic\nprograms, and capture abstract high-level categories that enable\ngeneralization.\n","authors":["Raja Marjieh","Sreejan Kumar","Declan Campbell","Liyi Zhang","Gianluca Bencomo","Jake Snell","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2405.19420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04978v2","updated":"2024-10-01T00:11:48Z","published":"2024-01-10T07:47:42Z","title":"Closed-Form Interpretation of Neural Network Classifiers with Symbolic\n  Gradients","summary":"  I introduce a unified framework for finding a closed-form interpretation of\nany single neuron in an artificial neural network. Using this framework I\ndemonstrate how to interpret neural network classifiers to reveal closed-form\nexpressions of the concepts encoded in their decision boundaries. In contrast\nto neural network-based regression, for classification, it is in general\nimpossible to express the neural network in the form of a symbolic equation\neven if the neural network itself bases its classification on a quantity that\ncan be written as a closed-form equation. The interpretation framework is based\non embedding trained neural networks into an equivalence class of functions\nthat encode the same concept. I interpret these neural networks by finding an\nintersection between the equivalence class and human-readable equations defined\nby a symbolic search space. The approach is not limited to classifiers or full\nneural networks and can be applied to arbitrary neurons in hidden layers or\nlatent spaces.\n","authors":["Sebastian Johann Wetzel"],"pdf_url":"https://arxiv.org/pdf/2401.04978v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.12618v2","updated":"2024-10-01T17:50:25Z","published":"2024-09-19T09:44:17Z","title":"Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning","summary":"  Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.\n","authors":["Santosh Kumar Radha","Yasamin Nouri Jelyani","Ara Ghukasyan","Oktay Goktas"],"pdf_url":"https://arxiv.org/pdf/2409.12618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19300v3","updated":"2024-10-01T17:39:02Z","published":"2024-05-29T17:27:08Z","title":"Measuring and Mitigating Bias for Tabular Datasets with Multiple\n  Protected Attributes","summary":"  Motivated by the recital (67) of the current corrigendum of the AI Act in the\nEuropean Union, we propose and present measures and mitigation strategies for\ndiscrimination in tabular datasets. We specifically focus on datasets that\ncontain multiple protected attributes, such as nationality, age, and sex. This\nmakes measuring and mitigating bias more challenging, as many existing methods\nare designed for a single protected attribute. This paper comes with a twofold\ncontribution: Firstly, new discrimination measures are introduced. These\nmeasures are categorized in our framework along with existing ones, guiding\nresearchers and practitioners in choosing the right measure to assess the\nfairness of the underlying dataset. Secondly, a novel application of an\nexisting bias mitigation method, FairDo, is presented. We show that this\nstrategy can mitigate any type of discrimination, including intersectional\ndiscrimination, by transforming the dataset. By conducting experiments on\nreal-world datasets (Adult, Bank, COMPAS), we demonstrate that de-biasing\ndatasets with multiple protected attributes is possible. All transformed\ndatasets show a reduction in discrimination, on average by 28%. Further, these\ndatasets do not compromise any of the tested machine learning models'\nperformances significantly compared to the original datasets. Conclusively,\nthis study demonstrates the effectiveness of the mitigation strategy used and\ncontributes to the ongoing discussion on the implementation of the European\nUnion's AI Act.\n","authors":["Manh Khoi Duong","Stefan Conrad"],"pdf_url":"https://arxiv.org/pdf/2405.19300v3.pdf","comment":"Submission accepted in AEQUITAS'24 (co-located with ECAI 2024)"},{"id":"http://arxiv.org/abs/2403.18079v2","updated":"2024-10-01T17:33:13Z","published":"2024-03-26T19:58:39Z","title":"Paths to Equilibrium in Games","summary":"  In multi-agent reinforcement learning (MARL) and game theory, agents\nrepeatedly interact and revise their strategies as new data arrives, producing\na sequence of strategy profiles. This paper studies sequences of strategies\nsatisfying a pairwise constraint inspired by policy updating in reinforcement\nlearning, where an agent who is best responding in one period does not switch\nits strategy in the next period. This constraint merely requires that\noptimizing agents do not switch strategies, but does not constrain the\nnon-optimizing agents in any way, and thus allows for exploration. Sequences\nwith this property are called satisficing paths, and arise naturally in many\nMARL algorithms. A fundamental question about strategic dynamics is such: for a\ngiven game and initial strategy profile, is it always possible to construct a\nsatisficing path that terminates at an equilibrium? The resolution of this\nquestion has implications about the capabilities or limitations of a class of\nMARL algorithms. We answer this question in the affirmative for normal-form\ngames. Our analysis reveals a counterintuitive insight that reward\ndeteriorating strategic updates are key to driving play to equilibrium along a\nsatisficing path.\n","authors":["Bora Yongacoglu","Gürdal Arslan","Lacra Pavel","Serdar Yüksel"],"pdf_url":"https://arxiv.org/pdf/2403.18079v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.17238v2","updated":"2024-10-01T17:12:57Z","published":"2024-06-25T02:59:02Z","title":"Generative Expansion of Small Datasets: An Expansive Graph Approach","summary":"  Limited data availability in machine learning significantly impacts\nperformance and generalization. Traditional augmentation methods enhance\nmoderately sufficient datasets. GANs struggle with convergence when generating\ndiverse samples. Diffusion models, while effective, have high computational\ncosts. We introduce an Expansive Synthesis model generating large-scale,\ninformation-rich datasets from minimal samples. It uses expander graph mappings\nand feature interpolation to preserve data distribution and feature\nrelationships. The model leverages neural networks' non-linear latent space,\ncaptured by a Koopman operator, to create a linear feature space for dataset\nexpansion. An autoencoder with self-attention layers and optimal transport\nrefines distributional consistency. We validate by comparing classifiers\ntrained on generated data to those trained on original datasets. Results show\ncomparable performance, demonstrating the model's potential to augment training\ndata effectively. This work advances data generation, addressing scarcity in\nmachine learning applications.\n","authors":["Vahid Jebraeeli","Bo Jiang","Hamid Krim","Derya Cansever"],"pdf_url":"https://arxiv.org/pdf/2406.17238v2.pdf","comment":"5 pages, 3 figures and 2 tables. Under review in ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.09111v2","updated":"2024-10-01T17:10:07Z","published":"2024-07-12T09:24:34Z","title":"Inference Optimization of Foundation Models on AI Accelerators","summary":"  Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.\n","authors":["Youngsuk Park","Kailash Budhathoki","Liangfu Chen","Jonas Kübler","Jiaji Huang","Matthäus Kleindessner","Jun Huan","Volkan Cevher","Yida Wang","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2407.09111v2.pdf","comment":"[v2] Tutorial website added [v1] Tutorial published at KDD 2024.\n  Camera-ready version"},{"id":"http://arxiv.org/abs/2407.13687v3","updated":"2024-10-01T16:33:36Z","published":"2024-07-18T17:42:37Z","title":"Dynamic Pricing in Securities Lending Market: Application in Revenue\n  Optimization for an Agent Lender Portfolio","summary":"  Securities lending is an important part of the financial market structure,\nwhere agent lenders help long term institutional investors to lend out their\nsecurities to short sellers in exchange for a lending fee. Agent lenders within\nthe market seek to optimize revenue by lending out securities at the highest\nrate possible. Typically, this rate is set by hard-coded business rules or\nstandard supervised machine learning models. These approaches are often\ndifficult to scale and are not adaptive to changing market conditions. Unlike a\ntraditional stock exchange with a centralized limit order book, the securities\nlending market is organized similarly to an e-commerce marketplace, where agent\nlenders and borrowers can transact at any agreed price in a bilateral fashion.\nThis similarity suggests that the use of typical methods for addressing dynamic\npricing problems in e-commerce could be effective in the securities lending\nmarket. We show that existing contextual bandit frameworks can be successfully\nutilized in the securities lending market. Using offline evaluation on real\nhistorical data, we show that the contextual bandit approach can consistently\noutperform typical approaches by at least 15% in terms of total revenue\ngenerated.\n","authors":["Jing Xu","Yung-Cheng Hsu","William Biscarri"],"pdf_url":"https://arxiv.org/pdf/2407.13687v3.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.09852v7","updated":"2024-10-01T16:11:27Z","published":"2023-11-16T12:28:31Z","title":"Short vs. Long-term Coordination of Drones: When Distributed\n  Optimization Meets Deep Reinforcement Learning","summary":"  Swarms of autonomous interactive drones can provide compelling sensing\ncapabilities in Smart City applications, such as traffic monitoring. This paper\nfocuses on the task assignment problem for large-scale spatio-temporal sensing\nby a drone swarm. However, existing approaches have distinct challenges:\ndistributed evolutionary optimization, such as collective learning, lacks\nlong-term adaptability in dynamic environments, while deep reinforcement\nlearning (DRL) is limited to scale effectively due to the curse of\ndimensionality. Therefore, this paper proposes a novel synergetic optimization\napproach by integrating long-term DRL and short-term collective learning.\nThrough this approach, each drone independently and proactively determines its\nflying direction and recharging location using DRL, while evolving their\nnavigation and sensing policies through collective learning based on a\nstructured tree communication model. Extensive experiments with datasets\ngenerated from realistic urban mobility demonstrate an outstanding performance\nof the proposed solution in complex scenarios. New insights show that this\napproach provides a win-win synthesis of short-term and long-term strategies\nfor drone-based traffic monitoring, with short-term methods addressing training\ncomplexity and energy management, while long-term methods preserving high\nsensing performance.\n","authors":["Chuhao Qin","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2311.09852v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05288v3","updated":"2024-10-01T16:08:52Z","published":"2023-10-08T21:27:29Z","title":"Clustering Three-Way Data with Outliers","summary":"  Matrix-variate distributions are a recent addition to the model-based\nclustering field, thereby making it possible to analyze data in matrix form\nwith complex structure such as images and time series. Due to its recent\nappearance, there is limited literature on matrix-variate data, with even less\non dealing with outliers in these models. An approach for clustering\nmatrix-variate normal data with outliers is discussed. The approach, which uses\nthe distribution of subset log-likelihoods, extends the OCLUST algorithm to\nmatrix-variate normal data and uses an iterative approach to detect and trim\noutliers.\n","authors":["Katharine M. Clark","Paul D. McNicholas"],"pdf_url":"https://arxiv.org/pdf/2310.05288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16176v4","updated":"2024-10-01T15:50:57Z","published":"2023-11-23T15:47:33Z","title":"Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse\n  Ensembles","summary":"  Spurious correlations in the data, where multiple cues are predictive of the\ntarget labels, often lead to a phenomenon known as shortcut learning, where a\nmodel relies on erroneous, easy-to-learn cues while ignoring reliable ones. In\nthis work, we propose DiffDiv an ensemble diversification framework exploiting\nDiffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show\nthat at particular training intervals, DPMs can generate images with novel\nfeature combinations, even when trained on samples displaying correlated input\nfeatures. We leverage this crucial property to generate synthetic\ncounterfactuals to increase model diversity via ensemble disagreement. We show\nthat DPM-guided diversification is sufficient to remove dependence on shortcut\ncues, without a need for additional supervised signals. We further empirically\nquantify its efficacy on several diversification objectives, and finally show\nimproved generalization and diversification on par with prior work that relies\non auxiliary data collection.\n","authors":["Luca Scimeca","Alexander Rubinstein","Damien Teney","Seong Joon Oh","Armand Mihai Nicolicioiu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2311.16176v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.02230"},{"id":"http://arxiv.org/abs/2409.17055v2","updated":"2024-10-01T15:47:14Z","published":"2024-09-25T16:13:57Z","title":"DRIM: Learning Disentangled Representations from Incomplete Multimodal\n  Healthcare Data","summary":"  Real-life medical data is often multimodal and incomplete, fueling the\ngrowing need for advanced deep learning models capable of integrating them\nefficiently. The use of diverse modalities, including histopathology slides,\nMRI, and genetic data, offers unprecedented opportunities to improve prognosis\nprediction and to unveil new treatment pathways. Contrastive learning, widely\nused for deriving representations from paired data in multimodal tasks, assumes\nthat different views contain the same task-relevant information and leverages\nonly shared information. This assumption becomes restrictive when handling\nmedical data since each modality also harbors specific knowledge relevant to\ndownstream tasks. We introduce DRIM, a new multimodal method for capturing\nthese shared and unique representations, despite data sparsity. More\nspecifically, given a set of modalities, we aim to encode a representation for\neach one that can be divided into two components: one encapsulating\npatient-related information common across modalities and the other,\nencapsulating modality-specific details. This is achieved by increasing the\nshared information among different patient modalities while minimizing the\noverlap between shared and unique components within each modality. Our method\noutperforms state-of-the-art algorithms on glioma patients survival prediction\ntasks, while being robust to missing modalities. To promote reproducibility,\nthe code is made publicly available at https://github.com/Lucas-rbnt/DRIM\n","authors":["Lucas Robinet","Ahmad Berjaoui","Ziad Kheil","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2409.17055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17510v2","updated":"2024-10-01T15:23:56Z","published":"2024-09-26T03:40:12Z","title":"NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human\n  Connectomes","summary":"  Although modern imaging technologies allow us to study connectivity between\ntwo distinct brain regions in-vivo, an in-depth understanding of how anatomical\nstructure supports brain function and how spontaneous functional fluctuations\nemerge remarkable cognition is still elusive. Meanwhile, tremendous efforts\nhave been made in the realm of machine learning to establish the nonlinear\nmapping between neuroimaging data and phenotypic traits. However, the absence\nof neuroscience insight in the current approaches poses significant challenges\nin understanding cognitive behavior from transient neural activities. To\naddress this challenge, we put the spotlight on the coupling mechanism of\nstructural connectivity (SC) and functional connectivity (FC) by formulating\nsuch network neuroscience question into an expressive graph representation\nlearning problem for high-order topology. Specifically, we introduce the\nconcept of topological detour to characterize how a ubiquitous instance of FC\n(direct link) is supported by neural pathways (detour) physically wired by SC,\nwhich forms a cyclic loop interacted by brain structure and function. In the\nclich\\'e of machine learning, the multi-hop detour pathway underlying SC-FC\ncoupling allows us to devise a novel multi-head self-attention mechanism within\nTransformer to capture multi-modal feature representation from paired graphs of\nSC and FC. Taken together, we propose a biological-inspired deep model, coined\nas NeuroPath, to find putative connectomic feature representations from the\nunprecedented amount of neuroimages, which can be plugged into various\ndownstream applications such as task recognition and disease diagnosis. We have\nevaluated NeuroPath on large-scale public datasets including HCP and UK Biobank\nunder supervised and zero-shot learning, where the state-of-the-art performance\nby our NeuroPath indicates great potential in network neuroscience.\n","authors":["Ziquan Wei","Tingting Dan","Jiaqi Ding","Guorong Wu"],"pdf_url":"https://arxiv.org/pdf/2409.17510v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.10932v2","updated":"2024-10-01T15:21:05Z","published":"2024-09-17T07:08:39Z","title":"Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine\n  Learning Approach","summary":"  Coronary heart disease (CHD) is a severe cardiac disease, and hence, its\nearly diagnosis is essential as it improves treatment results and saves money\non medical care. The prevailing development of quantum computing and machine\nlearning (ML) technologies may bring practical improvement to the performance\nof CHD diagnosis. Quantum machine learning (QML) is receiving tremendous\ninterest in various disciplines due to its higher performance and capabilities.\nA quantum leap in the healthcare industry will increase processing power and\noptimise multiple models. Techniques for QML have the potential to forecast\ncardiac disease and help in early detection. To predict the risk of coronary\nheart disease, a hybrid approach utilizing an ensemble machine learning model\nbased on QML classifiers is presented in this paper. Our approach, with its\nunique ability to address multidimensional healthcare data, reassures the\nmethod's robustness by fusing quantum and classical ML algorithms in a\nmulti-step inferential framework. The marked rise in heart disease and death\nrates impacts worldwide human health and the global economy. Reducing cardiac\nmorbidity and mortality requires early detection of heart disease. In this\nresearch, a hybrid approach utilizes techniques with quantum computing\ncapabilities to tackle complex problems that are not amenable to conventional\nmachine learning algorithms and to minimize computational expenses. The\nproposed method has been developed in the Raspberry Pi 5 Graphics Processing\nUnit (GPU) platform and tested on a broad dataset that integrates clinical and\nimaging data from patients suffering from CHD and healthy controls. Compared to\nclassical machine learning models, the accuracy, sensitivity, F1 score, and\nspecificity of the proposed hybrid QML model used with CHD are manifold higher.\n","authors":["Mehroush Banday","Sherin Zafar","Parul Agarwal","M Afshar Alam","Abubeker K M"],"pdf_url":"https://arxiv.org/pdf/2409.10932v2.pdf","comment":"I found a mistake in methodology presentation. Also I have observed\n  more precised results with new dataset. So my research guide ask me to modify\n  the current version"},{"id":"http://arxiv.org/abs/2407.00568v4","updated":"2024-10-01T15:19:42Z","published":"2024-06-30T02:50:28Z","title":"Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep\n  Penalty Neural Ordinary Differential Equations","summary":"  Forecasting high-dimensional dynamical systems is a fundamental challenge in\nvarious fields, such as geosciences and engineering. Neural Ordinary\nDifferential Equations (NODEs), which combine the power of neural networks and\nnumerical solvers, have emerged as a promising algorithm for forecasting\ncomplex nonlinear dynamical systems. However, classical techniques used for\nNODE training are ineffective for learning chaotic dynamical systems. In this\nwork, we propose a novel NODE-training approach that allows for robust learning\nof chaotic dynamical systems. Our method addresses the challenges of\nnon-convexity and exploding gradients associated with underlying chaotic\ndynamics. Training data trajectories from such systems are split into multiple,\nnon-overlapping time windows. In addition to the deviation from the training\ndata, the optimization loss term further penalizes the discontinuities of the\npredicted trajectory between the time windows. The window size is selected\nbased on the fastest Lyapunov time scale of the system. Multi-step penalty(MP)\nmethod is first demonstrated on Lorenz equation, to illustrate how it improves\nthe loss landscape and thereby accelerates the optimization convergence. MP\nmethod can optimize chaotic systems in a manner similar to least-squares\nshadowing with significantly lower computational costs. Our proposed algorithm,\ndenoted the Multistep Penalty NODE, is applied to chaotic systems such as the\nKuramoto-Sivashinsky equation, the two-dimensional Kolmogorov flow, and ERA5\nreanalysis data for the atmosphere. It is observed that MP-NODE provide viable\nperformance for such chaotic systems, not only for short-term trajectory\npredictions but also for invariant statistics that are hallmarks of the chaotic\nnature of these dynamics.\n","authors":["Dibyajyoti Chakraborty","Seung Whan Chung","Troy Arcomano","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2407.00568v4.pdf","comment":"25 pages, 17 Figures, submitted to Computer Methods in Applied\n  Mechanics and Engineering"},{"id":"http://arxiv.org/abs/2405.03869v3","updated":"2024-10-01T15:07:09Z","published":"2024-05-06T21:34:46Z","title":"Outlier Gradient Analysis: Efficiently Improving Deep Learning Model\n  Performance via Hessian-Free Influence Functions","summary":"  A core data-centric learning challenge is the identification of training\nsamples that are detrimental to model performance. Influence functions serve as\na prominent tool for this task and offer a robust framework for assessing\ntraining data influence on model predictions. Despite their widespread use,\ntheir high computational cost associated with calculating the inverse of the\nHessian matrix pose constraints, particularly when analyzing large-sized deep\nmodels. In this paper, we establish a bridge between identifying detrimental\ntraining samples via influence functions and outlier gradient detection. This\ntransformation not only presents a straightforward and Hessian-free formulation\nbut also provides insights into the role of the gradient in sample impact.\nThrough systematic empirical evaluations, we first validate the hypothesis of\nour proposed outlier gradient analysis approach on synthetic datasets. We then\ndemonstrate its effectiveness in detecting mislabeled samples in vision models\nand selecting data samples for improving performance of natural language\nprocessing transformer models. We also extend its use to influential sample\nidentification for fine-tuning Large Language Models.\n","authors":["Anshuman Chhabra","Bo Li","Jian Chen","Prasant Mohapatra","Hongfu Liu"],"pdf_url":"https://arxiv.org/pdf/2405.03869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19177v2","updated":"2024-10-01T14:44:52Z","published":"2024-09-27T23:13:17Z","title":"Evidence Is All You Need: Ordering Imaging Studies via Language Model\n  Alignment with the ACR Appropriateness Criteria","summary":"  Diagnostic imaging studies are an increasingly important component of the\nworkup and management of acutely presenting patients. However, ordering\nappropriate imaging studies according to evidence-based medical guidelines is a\nchallenging task with a high degree of variability between healthcare\nproviders. To address this issue, recent work has investigated if generative AI\nand large language models can be leveraged to help clinicians order relevant\nimaging studies for patients. However, it is challenging to ensure that these\ntools are correctly aligned with medical guidelines, such as the American\nCollege of Radiology's Appropriateness Criteria (ACR AC). In this study, we\nintroduce a framework to intelligently leverage language models by recommending\nimaging studies for patient cases that are aligned with evidence-based\nguidelines. We make available a novel dataset of patient \"one-liner\" scenarios\nto power our experiments, and optimize state-of-the-art language models to\nachieve an accuracy on par with clinicians in image ordering. Finally, we\ndemonstrate that our language model-based pipeline can be used as intelligent\nassistants by clinicians to support image ordering workflows and improve the\naccuracy of imaging study ordering according to the ACR AC. Our work\ndemonstrates and validates a strategy to leverage AI-based software to improve\ntrustworthy clinical decision making in alignment with expert evidence-based\nguidelines.\n","authors":["Michael S. Yao","Allison Chae","Charles E. Kahn Jr.","Walter R. Witschey","James C. Gee","Hersh Sagreiya","Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2409.19177v2.pdf","comment":"15 pages main text, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2210.16928v2","updated":"2024-10-01T14:39:12Z","published":"2022-10-30T19:08:38Z","title":"FELRec: Efficient Handling of Item Cold-Start With Dynamic\n  Representation Learning in Recommender Systems","summary":"  Recommender systems suffer from the cold-start problem whenever a new user\njoins the platform or a new item is added to the catalog. To address item\ncold-start, we propose to replace the embedding layer in sequential\nrecommenders with a dynamic storage that has no learnable weights and can keep\nan arbitrary number of representations. In this paper, we present FELRec, a\nlarge embedding network that refines the existing representations of users and\nitems in a recursive manner, as new information becomes available. In contrast\nto similar approaches, our model represents new users and items without side\ninformation and time-consuming finetuning, instead it runs a single forward\npass over a sequence of existing representations. During item cold-start, our\nmethod outperforms similar method by 29.50%-47.45%. Further, our proposed model\ngeneralizes well to previously unseen datasets in zero-shot settings. The\nsource code is publicly available at https://github.com/kweimann/FELRec .\n","authors":["Kuba Weimann","Tim O. F. Conrad"],"pdf_url":"https://arxiv.org/pdf/2210.16928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07932v2","updated":"2024-10-01T14:26:16Z","published":"2024-08-15T05:03:14Z","title":"MobileMEF: Fast and Efficient Method for Multi-Exposure Fusion","summary":"  Recent advances in camera design and imaging technology have enabled the\ncapture of high-quality images using smartphones. However, due to the limited\ndynamic range of digital cameras, the quality of photographs captured in\nenvironments with highly imbalanced lighting often results in poor-quality\nimages. To address this issue, most devices capture multi-exposure frames and\nthen use some multi-exposure fusion method to merge those frames into a final\nfused image. Nevertheless, most traditional and current deep learning\napproaches are unsuitable for real-time applications on mobile devices due to\ntheir heavy computational and memory requirements. We propose a new method for\nmulti-exposure fusion based on an encoder-decoder deep learning architecture\nwith efficient building blocks tailored for mobile devices. This efficient\ndesign makes our model capable of processing 4K resolution images in less than\n2 seconds on mid-range smartphones. Our method outperforms state-of-the-art\ntechniques regarding full-reference quality measures and computational\nefficiency (runtime and memory usage), making it ideal for real-time\napplications on hardware-constrained devices. Our code is available at:\nhttps://github.com/LucasKirsten/MobileMEF.\n","authors":["Lucas Nedel Kirsten","Zhicheng Fu","Nikhil Ambha Madhusudhana"],"pdf_url":"https://arxiv.org/pdf/2408.07932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20340v2","updated":"2024-10-01T14:14:32Z","published":"2024-09-30T14:39:56Z","title":"Enhancing GANs with Contrastive Learning-Based Multistage Progressive\n  Finetuning SNN and RL-Based External Optimization","summary":"  The application of deep learning in cancer research, particularly in early\ndiagnosis, case understanding, and treatment strategy design, emphasizes the\nneed for high-quality data. Generative AI, especially Generative Adversarial\nNetworks (GANs), has emerged as a leading solution to challenges like class\nimbalance, robust learning, and model training, while addressing issues\nstemming from patient privacy and the scarcity of real data. Despite their\npromise, GANs face several challenges, both inherent and specific to\nhistopathology data. Inherent issues include training imbalance, mode collapse,\nlinear learning from insufficient discriminator feedback, and hard boundary\nconvergence due to stringent feedback. Histopathology data presents a unique\nchallenge with its complex representation, high spatial resolution, and\nmultiscale features. To address these challenges, we propose a framework\nconsisting of two components. First, we introduce a contrastive learning-based\nMultistage Progressive Finetuning Siamese Neural Network (MFT-SNN) for\nassessing the similarity between histopathology patches. Second, we implement a\nReinforcement Learning-based External Optimizer (RL-EO) within the GAN training\nloop, serving as a reward signal generator. The modified discriminator loss\nfunction incorporates a weighted reward, guiding the GAN to maximize this\nreward while minimizing loss. This approach offers an external optimization\nguide to the discriminator, preventing generator overfitting and ensuring\nsmooth convergence. Our proposed solution has been benchmarked against\nstate-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,\noutperforming previous SOTA across various metrics, including FID score, KID\nscore, Perceptual Path Length, and downstream classification tasks.\n","authors":["Osama Mustafa"],"pdf_url":"https://arxiv.org/pdf/2409.20340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08040v4","updated":"2024-10-01T14:08:10Z","published":"2024-03-12T19:23:13Z","title":"Low-Energy On-Device Personalization for MCUs","summary":"  Microcontroller Units (MCUs) are ideal platforms for edge applications due to\ntheir low cost and energy consumption, and are widely used in various\napplications, including personalized machine learning tasks, where customized\nmodels can enhance the task adaptation. However, existing approaches for local\non-device personalization mostly support simple ML architectures or require\ncomplex local pre-training/training, leading to high energy consumption and\nnegating the low-energy advantage of MCUs. In this paper, we introduce\n$MicroT$, an efficient and low-energy MCU personalization approach. $MicroT$\nincludes a robust, general, but tiny feature extractor, developed through\nself-supervised knowledge distillation, which trains a task-specific head to\nenable independent on-device personalization with minimal energy and\ncomputational requirements. MicroT implements an MCU-optimized early-exit\ninference mechanism called stage-decision to further reduce energy costs. This\nmechanism allows for user-configurable exit criteria (stage-decision ratio) to\nadaptively balance energy cost with model performance. We evaluated MicroT\nusing two models, three datasets, and two MCU boards. $MicroT$ outperforms\ntraditional transfer learning (TTL) and two SOTA approaches by 2.12 - 11.60%\nacross two models and three datasets. Targeting widely used energy-aware edge\ndevices, MicroT's on-device training requires no additional complex operations,\nhalving the energy cost compared to SOTA approaches by up to 2.28X while\nkeeping SRAM usage below 1MB. During local inference, MicroT reduces energy\ncost by 14.17% compared to TTL across two boards and two datasets, highlighting\nits suitability for long-term use on energy-aware resource-constrained MCUs.\n","authors":["Yushan Huang","Ranya Aloufi","Xavier Cadet","Yuchen Zhao","Payam Barnaghi","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.08040v4.pdf","comment":"Accepted to The 9th ACM/IEEE Symposium on Edge Computing (SEC 2024)"},{"id":"http://arxiv.org/abs/2405.01453v3","updated":"2024-10-01T13:46:04Z","published":"2024-05-02T16:36:26Z","title":"Creative Problem Solving in Large Language and Vision Models -- What\n  Would it Take?","summary":"  We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs\n","authors":["Lakshmi Nair","Evana Gizzi","Jivko Sinapov"],"pdf_url":"https://arxiv.org/pdf/2405.01453v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2312.17397v2","updated":"2024-10-01T13:45:04Z","published":"2023-12-28T23:34:38Z","title":"Classifier-free graph diffusion for molecular property targeting","summary":"  This work focuses on the task of property targeting: that is, generating\nmolecules conditioned on target chemical properties to expedite candidate\nscreening for novel drug and materials development. DiGress is a recent\ndiffusion model for molecular graphs whose distinctive feature is allowing\nproperty targeting through classifier-based (CB) guidance. While CB guidance\nmay work to generate molecular-like graphs, we hint at the fact that its\nassumptions apply poorly to the chemical domain. Based on this insight we\npropose a classifier-free DiGress (FreeGress), which works by directly\ninjecting the conditioning information into the training process. CF guidance\nis convenient given its less stringent assumptions and since it does not\nrequire to train an auxiliary property regressor, thus halving the number of\ntrainable parameters in the model. We empirically show that our model yields up\nto 79% improvement in Mean Absolute Error with respect to DiGress on property\ntargeting tasks on QM9 and ZINC-250k benchmarks. As an additional contribution,\nwe propose a simple yet powerful approach to improve chemical validity of\ngenerated samples, based on the observation that certain chemical properties\nsuch as molecular weight correlate with the number of atoms in molecules.\n","authors":["Matteo Ninniri","Marco Podda","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2312.17397v2.pdf","comment":"Proceedings of ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2407.11878v2","updated":"2024-10-01T13:35:15Z","published":"2024-07-16T16:02:27Z","title":"Learning Confidence Bounds for Classification with Imbalanced Data","summary":"  Class imbalance poses a significant challenge in classification tasks, where\ntraditional approaches often lead to biased models and unreliable predictions.\nUndersampling and oversampling techniques have been commonly employed to\naddress this issue, yet they suffer from inherent limitations stemming from\ntheir simplistic approach such as loss of information and additional biases\nrespectively. In this paper, we propose a novel framework that leverages\nlearning theory and concentration inequalities to overcome the shortcomings of\ntraditional solutions. We focus on understanding the uncertainty in a\nclass-dependent manner, as captured by confidence bounds that we directly embed\ninto the learning process. By incorporating class-dependent estimates, our\nmethod can effectively adapt to the varying degrees of imbalance across\ndifferent classes, resulting in more robust and reliable classification\noutcomes. We empirically show how our framework provides a promising direction\nfor handling imbalanced data in classification tasks, offering practitioners a\nvaluable tool for building more accurate and trustworthy models.\n","authors":["Matt Clifford","Jonathan Erskine","Alexander Hepburn","Raúl Santos-Rodríguez","Dario Garcia-Garcia"],"pdf_url":"https://arxiv.org/pdf/2407.11878v2.pdf","comment":"Accepted at ECAI 2024 main track"},{"id":"http://arxiv.org/abs/2408.01571v2","updated":"2024-10-01T13:34:36Z","published":"2024-08-02T21:01:30Z","title":"Counterfactual Explanations for Medical Image Classification and\n  Regression using Diffusion Autoencoder","summary":"  Counterfactual explanations (CEs) aim to enhance the interpretability of\nmachine learning models by illustrating how alterations in input features would\naffect the resulting predictions. Common CE approaches require an additional\nmodel and are typically constrained to binary counterfactuals. In contrast, we\npropose a novel method that operates directly on the latent space of a\ngenerative model, specifically a Diffusion Autoencoder (DAE). This approach\noffers inherent interpretability by enabling the generation of CEs and the\ncontinuous visualization of the model's internal representation across decision\nboundaries.\n  Our method leverages the DAE's ability to encode images into a semantically\nrich latent space in an unsupervised manner, eliminating the need for labeled\ndata or separate feature extraction models. We show that these latent\nrepresentations are helpful for medical condition classification and the\nordinal regression of severity pathologies, such as vertebral compression\nfractures (VCF) and diabetic retinopathy (DR). Beyond binary CEs, our method\nsupports the visualization of ordinal CEs using a linear model, providing\ndeeper insights into the model's decision-making process and enhancing\ninterpretability.\n  Experiments across various medical imaging datasets demonstrate the method's\nadvantages in interpretability and versatility. The linear manifold of the\nDAE's latent space allows for meaningful interpolation and manipulation, making\nit a powerful tool for exploring medical image properties. Our code is\navailable at https://doi.org/10.5281/zenodo.13859266.\n","authors":["Matan Atad","David Schinz","Hendrik Moeller","Robert Graf","Benedikt Wiestler","Daniel Rueckert","Nassir Navab","Jan S. Kirschke","Matthias Keicher"],"pdf_url":"https://arxiv.org/pdf/2408.01571v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:024. arXiv admin\n  note: text overlap with arXiv:2303.12031"},{"id":"http://arxiv.org/abs/2405.08967v3","updated":"2024-10-01T13:33:09Z","published":"2024-05-14T21:15:29Z","title":"Gradient-Free Training of Recurrent Neural Networks using Random\n  Perturbations","summary":"  Recurrent neural networks (RNNs) hold immense potential for computations due\nto their Turing completeness and sequential processing capabilities, yet\nexisting methods for their training encounter efficiency challenges.\nBackpropagation through time (BPTT), the prevailing method, extends the\nbackpropagation (BP) algorithm by unrolling the RNN over time. However, this\napproach suffers from significant drawbacks, including the need to interleave\nforward and backward phases and store exact gradient information. Furthermore,\nBPTT has been shown to struggle to propagate gradient information for long\nsequences, leading to vanishing gradients. An alternative strategy to using\ngradient-based methods like BPTT involves stochastically approximating\ngradients through perturbation-based methods. This learning approach is\nexceptionally simple, necessitating only forward passes in the network and a\nglobal reinforcement signal as feedback. Despite its simplicity, the random\nnature of its updates typically leads to inefficient optimization, limiting its\neffectiveness in training neural networks. In this study, we present a new\napproach to perturbation-based learning in RNNs whose performance is\ncompetitive with BPTT, while maintaining the inherent advantages over\ngradient-based learning. To this end, we extend the recently introduced\nactivity-based node perturbation (ANP) method to operate in the time domain,\nleading to more efficient learning and generalization. We subsequently conduct\na range of experiments to validate our approach. Our results show similar\nperformance, convergence time and scalability compared to BPTT, strongly\noutperforming standard node and weight perturbation methods. These findings\nsuggest that perturbation-based learning methods offer a versatile alternative\nto gradient-based methods for training RNNs which can be ideally suited for\nneuromorphic computing applications\n","authors":["Jesus Garcia Fernandez","Sander Keemink","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2405.08967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.07718v4","updated":"2024-10-01T13:22:32Z","published":"2021-06-14T19:27:54Z","title":"HUMAP: Hierarchical Uniform Manifold Approximation and Projection","summary":"  Dimensionality reduction (DR) techniques help analysts to understand patterns\nin high-dimensional spaces. These techniques, often represented by scatter\nplots, are employed in diverse science domains and facilitate similarity\nanalysis among clusters and data samples. For datasets containing many\ngranularities or when analysis follows the information visualization mantra,\nhierarchical DR techniques are the most suitable approach since they present\nmajor structures beforehand and details on demand. This work presents HUMAP, a\nnovel hierarchical dimensionality reduction technique designed to be flexible\non preserving local and global structures and preserve the mental map\nthroughout hierarchical exploration. We provide empirical evidence of our\ntechnique's superiority compared with current hierarchical approaches and show\na case study applying HUMAP for dataset labelling.\n","authors":["Wilson E. Marcílio-Jr","Danilo M. Eler","Fernando V. Paulovich","Rafael M. Martins"],"pdf_url":"https://arxiv.org/pdf/2106.07718v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12957v3","updated":"2024-10-01T13:18:54Z","published":"2024-05-17T07:46:05Z","title":"Enhancing the analysis of murine neonatal ultrasonic vocalizations:\n  Development, evaluation, and application of different mathematical models","summary":"  Rodents employ a broad spectrum of ultrasonic vocalizations (USVs) for social\ncommunication. As these vocalizations offer valuable insights into affective\nstates, social interactions, and developmental stages of animals, various deep\nlearning approaches have aimed to automate both the quantitative (detection)\nand qualitative (classification) analysis of USVs. Here, we present the first\nsystematic evaluation of different types of neural networks for USV\nclassification. We assessed various feedforward networks, including a\ncustom-built, fully-connected network and convolutional neural network,\ndifferent residual neural networks (ResNets), an EfficientNet, and a Vision\nTransformer (ViT). Paired with a refined, entropy-based detection algorithm\n(achieving recall of 94.9% and precision of 99.3%), the best architecture\n(achieving 86.79% accuracy) was integrated into a fully automated pipeline\ncapable of analyzing extensive USV datasets with high reliability.\nAdditionally, users can specify an individual minimum accuracy threshold based\non their research needs. In this semi-automated setup, the pipeline selectively\nclassifies calls with high pseudo-probability, leaving the rest for manual\ninspection. Our study focuses exclusively on neonatal USVs. As part of an\nongoing phenotyping study, our pipeline has proven to be a valuable tool for\nidentifying key differences in USVs produced by mice with autism-like\nbehaviors.\n","authors":["Rudolf Herdt","Louisa Kinzel","Johann Georg Maaß","Marvin Walther","Henning Fröhlich","Tim Schubert","Peter Maass","Christian Patrick Schaaf"],"pdf_url":"https://arxiv.org/pdf/2405.12957v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04223v3","updated":"2024-10-01T13:18:41Z","published":"2022-12-08T12:05:50Z","title":"Vicious Classifiers: Assessing Inference-time Data Reconstruction Risk\n  in Edge Computing","summary":"  Privacy-preserving inference in edge computing paradigms encourages the users\nof machine-learning services to locally run a model on their private input and\nonly share the models outputs for a target task with the server. We study how a\nvicious server can reconstruct the input data by observing only the models\noutputs while keeping the target accuracy very close to that of a honest server\nby jointly training a target model (to run at users' side) and an attack model\nfor data reconstruction (to secretly use at servers' side). We present a new\nmeasure to assess the inference-time reconstruction risk. Evaluations on six\nbenchmark datasets show the model's input can be approximately reconstructed\nfrom the outputs of a single inference. We propose a primary defense mechanism\nto distinguish vicious versus honest classifiers at inference time. By studying\nsuch a risk associated with emerging ML services our work has implications for\nenhancing privacy in edge computing. We discuss open challenges and directions\nfor future studies and release our code as a benchmark for the community at\nhttps://github.com/mmalekzadeh/vicious-classifiers .\n","authors":["Mohammad Malekzadeh","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2212.04223v3.pdf","comment":"Published at BMVC 2024 workshop on Privacy, Fairness, Accountability\n  and Transparency in Computer Vision"},{"id":"http://arxiv.org/abs/2408.14126v2","updated":"2024-10-01T13:18:35Z","published":"2024-08-26T09:19:58Z","title":"Enhancing Fairness through Reweighting: A Path to Attain the Sufficiency\n  Rule","summary":"  We introduce an innovative approach to enhancing the empirical risk\nminimization (ERM) process in model training through a refined reweighting\nscheme of the training data to enhance fairness. This scheme aims to uphold the\nsufficiency rule in fairness by ensuring that optimal predictors maintain\nconsistency across diverse sub-groups. We employ a bilevel formulation to\naddress this challenge, wherein we explore sample reweighting strategies.\nUnlike conventional methods that hinge on model size, our formulation bases\ngeneralization complexity on the space of sample weights. We discretize the\nweights to improve training speed. Empirical validation of our method showcases\nits effectiveness and robustness, revealing a consistent improvement in the\nbalance between prediction performance and fairness metrics across various\nexperiments.\n","authors":["Xuan Zhao","Klaus Broelemann","Salvatore Ruggieri","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2408.14126v2.pdf","comment":"accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2409.16167v2","updated":"2024-10-01T13:16:45Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06890v2","updated":"2024-10-01T13:10:40Z","published":"2024-08-13T13:36:48Z","title":"BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning","summary":"  Developing models with robust group fairness properties is paramount,\nparticularly in ethically sensitive domains such as medical diagnosis. Recent\napproaches to achieving fairness in machine learning require a substantial\namount of training data and depend on model retraining, which may not be\npractical in real-world scenarios. To mitigate these challenges, we propose\nBias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method\nthat enhances the fairness of a trained model in significantly fewer epochs\nwithout requiring access to the original training data. BMFT produces a mask\nover model parameters, which efficiently identifies the weights contributing\nthe most towards biased predictions. Furthermore, we propose a two-step\ndebiasing strategy, wherein the feature extractor undergoes initial fine-tuning\non the identified bias-influenced weights, succeeded by a fine-tuning phase on\na reinitialised classification layer to uphold discriminative performance.\nExtensive experiments across four dermatological datasets and two sensitive\nattributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA)\ntechniques in both diagnostic accuracy and fairness metrics. Our findings\nunderscore the efficacy and robustness of BMFT in advancing fairness across\nvarious out-of-distribution (OOD) settings. Our code is available at:\nhttps://github.com/vios-s/BMFT\n","authors":["Yuyang Xue","Junyu Yan","Raman Dutt","Fasih Haider","Jingshuai Liu","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2408.06890v2.pdf","comment":"Accepted by MICCAI 2024 FAIMI Workshop Oral"},{"id":"http://arxiv.org/abs/2406.09864v2","updated":"2024-10-01T13:07:02Z","published":"2024-06-14T09:22:07Z","title":"LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data","summary":"  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique benchmark dataset,\nfeaturing audio, image, and textual data from 50 classes, for learning from\nuncertain and multimodal data. It extends the well-known CIFAR 10/100 dataset\nwith audio samples extracted from three audio corpora, and text data generated\nusing the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the\ncontrolled injection of varying types and degrees of uncertainty to achieve and\ntailor specific experiments and benchmarking initiatives. LUMA is also\navailable as a Python package including the functions for generating multiple\nvariants of the dataset with controlling the diversity of the data, the amount\nof noise for each modality, and adding out-of-distribution samples. A baseline\npre-trained model is also provided alongside three uncertainty quantification\nmethods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive\nMulti-View Learning. This comprehensive dataset and its benchmarking tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the ICLR community to design more trustworthy\nand robust machine learning approaches for safety critical applications.\n","authors":["Grigor Bezirganyan","Sana Sellami","Laure Berti-Équille","Sébastien Fournier"],"pdf_url":"https://arxiv.org/pdf/2406.09864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00677v3","updated":"2024-10-01T12:45:01Z","published":"2023-07-02T22:30:08Z","title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary\n  Directed Differential with Normalized Density and Self-Adaption","summary":"  Density-based clustering could be the most popular clustering algorithm since\nit can identify clusters of arbitrary shape as long as they are separated by\nlow-density regions. However, a high-density region that is not separated by\nlow-density ones might also have different structures belonging to multiple\nclusters. As far as we know, all previous density-based clustering algorithms\nfail to detect such structures. In this paper, we provide a novel density-based\nclustering scheme that can not only detect clusters separated by low-density\nregions but also detect structures in high-density regions not separated by\nlow-density ones. The algorithm employs secondary directed differential,\nhierarchy, normalized density, as well as the self-adaption coefficient, and\nthus is called Structure Detecting Cluster by Hierarchical Secondary Directed\nDifferential with Normalized Density and Self-Adaption, dubbed by\nSDC-HSDD-NDSA. The algorithm is run on several datasets to verify its\neffectiveness, robustness, as well as granularity independence, and results\ndemonstrate that it has the ability that previous ones do not have. The Python\ncode is on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2307.00677v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2409.19924v2","updated":"2024-10-01T12:43:09Z","published":"2024-09-30T03:58:43Z","title":"On The Planning Abilities of OpenAI's o1 Models: Feasibility,\n  Optimality, and Generalizability","summary":"  Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.\n","authors":["Kevin Wang","Junbo Li","Neel P. Bhatt","Yihan Xi","Qiang Liu","Ufuk Topcu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2409.19924v2.pdf","comment":"Updated link to code repository"},{"id":"http://arxiv.org/abs/2409.09003v3","updated":"2024-10-01T12:42:24Z","published":"2024-09-13T17:32:05Z","title":"Model-independent variable selection via the rule-based variable\n  priority","summary":"  While achieving high prediction accuracy is a fundamental goal in machine\nlearning, an equally important task is finding a small number of features with\nhigh explanatory power. One popular selection technique is permutation\nimportance, which assesses a variable's impact by measuring the change in\nprediction error after permuting the variable. However, this can be problematic\ndue to the need to create artificial data, a problem shared by other methods as\nwell. Another problem is that variable selection methods can be limited by\nbeing model-specific. We introduce a new model-independent approach, Variable\nPriority (VarPro), which works by utilizing rules without the need to generate\nartificial data or evaluate prediction error. The method is relatively easy to\nuse, requiring only the calculation of sample averages of simple statistics,\nand can be applied to many data settings, including regression, classification,\nand survival. We investigate the asymptotic properties of VarPro and show,\namong other things, that VarPro has a consistent filtering property for noise\nvariables. Empirical studies using synthetic and real-world data show the\nmethod achieves a balanced performance and compares favorably to many\nstate-of-the-art procedures currently used for variable selection.\n","authors":["Min Lu","Hemant Ishwaran"],"pdf_url":"https://arxiv.org/pdf/2409.09003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01656v2","updated":"2024-10-01T12:39:15Z","published":"2024-07-01T14:13:11Z","title":"Statistical signatures of abstraction in deep neural networks","summary":"  We study how abstract representations emerge in a Deep Belief Network (DBN)\ntrained on benchmark datasets. Our analysis targets the principles of learning\nin the early stages of information processing, starting from the \"primordial\nsoup\" of the under-sampling regime. As the data is processed by deeper and\ndeeper layers, features are detected and removed, transferring more and more\n\"context-invariant\" information to deeper layers. We show that the\nrepresentation approaches an universal model -- the Hierarchical Feature Model\n(HFM) -- determined by the principle of maximal relevance. Relevance quantifies\nthe uncertainty on the model of the data, thus suggesting that \"meaning\" --\ni.e. syntactic information -- is that part of the data which is not yet\ncaptured by a model. Our analysis shows that shallow layers are well described\nby pairwise Ising models, which provide a representation of the data in terms\nof generic, low order features. We also show that plasticity increases with\ndepth, in a similar way as it does in the brain. These findings suggest that\nDBNs are capable of extracting a hierarchy of features from the data which is\nconsistent with the principle of maximal relevance.\n","authors":["Carlo Orientale Caputo","Matteo Marsili"],"pdf_url":"https://arxiv.org/pdf/2407.01656v2.pdf","comment":"The estimate of the Kullback-Leibler distance used in the paper is\n  affected by strong sampling errors. Additional statistical analysis is needed"},{"id":"http://arxiv.org/abs/2407.03728v2","updated":"2024-10-01T12:26:24Z","published":"2024-07-04T08:21:54Z","title":"Measuring Orthogonality in Representations of Generative Models","summary":"  In unsupervised representation learning, models aim to distill essential\nfeatures from high-dimensional data into lower-dimensional learned\nrepresentations, guided by inductive biases. Understanding the characteristics\nthat make a good representation remains a topic of ongoing research.\nDisentanglement of independent generative processes has long been credited with\nproducing high-quality representations. However, focusing solely on\nrepresentations that adhere to the stringent requirements of most\ndisentanglement metrics, may result in overlooking many high-quality\nrepresentations, well suited for various downstream tasks. These metrics often\ndemand that generative factors be encoded in distinct, single dimensions\naligned with the canonical basis of the representation space.\n  Motivated by these observations, we propose two novel metrics:\nImportance-Weighted Orthogonality (IWO) and Importance-Weighted Rank (IWR).\nThese metrics evaluate the mutual orthogonality and rank of generative factor\nsubspaces. Throughout extensive experiments on common downstream tasks, over\nseveral benchmark datasets and models, IWO and IWR consistently show stronger\ncorrelations with downstream task performance than traditional disentanglement\nmetrics. Our findings suggest that representation quality is closer related to\nthe orthogonality of independent generative processes rather than their\ndisentanglement, offering a new direction for evaluating and improving\nunsupervised learning models.\n","authors":["Robin C. Geyer","Alessandro Torcinovich","João B. Carvalho","Alexander Meyer","Joachim M. Buhmann"],"pdf_url":"https://arxiv.org/pdf/2407.03728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12589v2","updated":"2024-10-01T11:40:11Z","published":"2024-09-19T09:12:29Z","title":"Is Tokenization Needed for Masked Particle Modelling?","summary":"  In this work, we significantly enhance masked particle modeling (MPM), a\nself-supervised learning scheme for constructing highly expressive\nrepresentations of unordered sets relevant to developing foundation models for\nhigh-energy physics. In MPM, a model is trained to recover the missing elements\nof a set, a learning objective that requires no labels and can be applied\ndirectly to experimental data. We achieve significant performance improvements\nover previous work on MPM by addressing inefficiencies in the implementation\nand incorporating a more powerful decoder. We compare several pre-training\ntasks and introduce new reconstruction methods that utilize conditional\ngenerative models without data tokenization or discretization. We show that\nthese new methods outperform the tokenized learning objective from the original\nMPM on a new test bed for foundation models for jets, which includes using a\nwide variety of downstream tasks relevant to jet physics, such as\nclassification, secondary vertex finding, and track identification.\n","authors":["Matthew Leigh","Samuel Klein","François Charton","Tobias Golling","Lukas Heinrich","Michael Kagan","Inês Ochoa","Margarita Osadchy"],"pdf_url":"https://arxiv.org/pdf/2409.12589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17198v2","updated":"2024-10-01T11:35:49Z","published":"2024-08-30T10:52:18Z","title":"Towards Symbolic XAI -- Explanation Through Human Understandable Logical\n  Relationships Between Features","summary":"  Explainable Artificial Intelligence (XAI) plays a crucial role in fostering\ntransparency and trust in AI systems, where traditional XAI approaches\ntypically offer one level of abstraction for explanations, often in the form of\nheatmaps highlighting single or multiple input features. However, we ask\nwhether abstract reasoning or problem-solving strategies of a model may also be\nrelevant, as these align more closely with how humans approach solutions to\nproblems. We propose a framework, called Symbolic XAI, that attributes\nrelevance to symbolic queries expressing logical relationships between input\nfeatures, thereby capturing the abstract reasoning behind a model's\npredictions. The methodology is built upon a simple yet general multi-order\ndecomposition of model predictions. This decomposition can be specified using\nhigher-order propagation-based relevance methods, such as GNN-LRP, or\nperturbation-based explanation methods commonly used in XAI. The effectiveness\nof our framework is demonstrated in the domains of natural language processing\n(NLP), vision, and quantum chemistry (QC), where abstract symbolic domain\nknowledge is abundant and of significant interest to users. The Symbolic XAI\nframework provides an understanding of the model's decision-making process that\nis both flexible for customization by the user and human-readable through\nlogical formulas.\n","authors":["Thomas Schnake","Farnoush Rezaei Jafari","Jonas Lederer","Ping Xiong","Shinichi Nakajima","Stefan Gugler","Grégoire Montavon","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2408.17198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15216v2","updated":"2024-10-01T11:20:53Z","published":"2024-09-23T17:00:35Z","title":"FLeNS: Federated Learning with Enhanced Nesterov-Newton Sketch","summary":"  Federated learning faces a critical challenge in balancing communication\nefficiency with rapid convergence, especially for second-order methods. While\nNewton-type algorithms achieve linear convergence in communication rounds,\ntransmitting full Hessian matrices is often impractical due to quadratic\ncomplexity. We introduce Federated Learning with Enhanced Nesterov-Newton\nSketch (FLeNS), a novel method that harnesses both the acceleration\ncapabilities of Nesterov's method and the dimensionality reduction benefits of\nHessian sketching. FLeNS approximates the centralized Newton's method without\nrelying on the exact Hessian, significantly reducing communication overhead. By\ncombining Nesterov's acceleration with adaptive Hessian sketching, FLeNS\npreserves crucial second-order information while preserving the rapid\nconvergence characteristics. Our theoretical analysis, grounded in statistical\nlearning, demonstrates that FLeNS achieves super-linear convergence rates in\ncommunication rounds - a notable advancement in federated optimization. We\nprovide rigorous convergence guarantees and characterize tradeoffs between\nacceleration, sketch size, and convergence speed. Extensive empirical\nevaluation validates our theoretical findings, showcasing FLeNS's\nstate-of-the-art performance with reduced communication requirements,\nparticularly in privacy-sensitive and edge-computing scenarios. The code is\navailable at https://github.com/sunnyinAI/FLeNS\n","authors":["Sunny Gupta","Mohit Jindal","Pankhi Kashyap","Pranav Jeevan","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2409.15216v2.pdf","comment":"10 pages, 3 figures, 2 Tables"},{"id":"http://arxiv.org/abs/2406.06841v2","updated":"2024-10-01T11:14:40Z","published":"2024-06-10T23:23:36Z","title":"CompassDock: Comprehensive Accurate Assessment Approach for Deep\n  Learning-Based Molecular Docking in Inference and Fine-Tuning","summary":"  Datasets used for molecular docking, such as PDBBind, contain technical\nvariability - they are noisy. Although the origins of the noise have been\ndiscussed, a comprehensive analysis of the physical, chemical, and bioactivity\ncharacteristics of the datasets is still lacking. To address this gap, we\nintroduce the Comprehensive Accurate Assessment (Compass). Compass integrates\ntwo key components: PoseCheck, which examines ligand strain energy,\nprotein-ligand steric clashes, and interactions, and AA-Score, a new empirical\nscoring function for calculating binding affinity energy. Together, these form\na unified workflow that assesses both the physical/chemical properties and\nbioactivity favorability of ligands and protein-ligand interactions. Our\nanalysis of the PDBBind dataset using Compass reveals substantial noise in the\nground truth data. Additionally, we propose CompassDock, which incorporates the\nCompass module with DiffDock, the state-of-the-art deep learning-based\nmolecular docking method, to enable accurate assessment of docked ligands\nduring inference. Finally, we present a new paradigm for enhancing molecular\ndocking model performance by fine-tuning with Compass Scores, which encompass\nbinding affinity energy, strain energy, and the number of steric clashes\nidentified by Compass. Our results show that, while fine-tuning without Compass\nimproves the percentage of docked poses with RMSD < 2{\\AA}, it leads to a\ndecrease in physical/chemical and bioactivity favorability. In contrast,\nfine-tuning with Compass shows a limited improvement in RMSD < 2{\\AA} but\nenhances the physical/chemical and bioactivity favorability of the ligand\nconformation. The source code is available publicly at\nhttps://github.com/BIMSBbioinfo/CompassDock.\n","authors":["Ahmet Sarigun","Vedran Franke","Bora Uyar","Altuna Akalin"],"pdf_url":"https://arxiv.org/pdf/2406.06841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06494v2","updated":"2024-10-01T11:10:59Z","published":"2024-07-09T01:56:23Z","title":"A Generative Approach to Control Complex Physical Systems","summary":"  Controlling the evolution of complex physical systems is a fundamental task\nacross science and engineering. Classical techniques suffer from limited\napplicability or huge computational costs. On the other hand, recent deep\nlearning and reinforcement learning-based approaches often struggle to optimize\nlong-term control sequences under the constraints of system dynamics. In this\nwork, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class\nof method to address the physical systems control problem. DiffPhyCon excels by\nsimultaneously minimizing both the learned generative energy function and the\npredefined control objectives across the entire trajectory and control\nsequence. Thus, it can explore globally and plan near-optimal control\nsequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the\ndiscovery of control sequences that significantly deviate from the training\ndistribution. We test our method on three tasks: 1D Burgers' equation, 2D\njellyfish movement control, and 2D high-dimensional smoke control, where our\ngenerated jellyfish dataset is released as a benchmark for complex physical\nsystem control research. Our method outperforms widely applied classical\napproaches and state-of-the-art deep learning and reinforcement learning\nmethods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern\nobserved in the jellyfish, aligning with established findings in the field of\nfluid dynamics. The project website, jellyfish dataset, and code can be found\nat https://github.com/AI4Science-WestlakeU/diffphycon.\n","authors":["Long Wei","Peiyan Hu","Ruiqi Feng","Haodong Feng","Yixuan Du","Tao Zhang","Rui Wang","Yue Wang","Zhi-Ming Ma","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2407.06494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10286v2","updated":"2024-10-01T11:08:24Z","published":"2024-09-16T13:47:52Z","title":"Enhancing Image Classification in Small and Unbalanced Datasets through\n  Synthetic Data Augmentation","summary":"  Accurate and robust medical image classification is a challenging task,\nespecially in application domains where available annotated datasets are small\nand present high imbalance between target classes. Considering that data\nacquisition is not always feasible, especially for underrepresented classes,\nour approach introduces a novel synthetic augmentation strategy using\nclass-specific Variational Autoencoders (VAEs) and latent space interpolation\nto improve discrimination capabilities.\n  By generating realistic, varied synthetic data that fills feature space gaps,\nwe address issues of data scarcity and class imbalance. The method presented in\nthis paper relies on the interpolation of latent representations within each\nclass, thus enriching the training set and improving the model's\ngeneralizability and diagnostic accuracy. The proposed strategy was tested in a\nsmall dataset of 321 images created to train and validate an automatic method\nfor assessing the quality of cleanliness of esophagogastroduodenoscopy images.\nBy combining real and synthetic data, an increase of over 18\\% in the accuracy\nof the most challenging underrepresented class was observed. The proposed\nstrategy not only benefited the underrepresented class but also led to a\ngeneral improvement in other metrics, including a 6\\% increase in global\naccuracy and precision.\n","authors":["Neil De La Fuente","Mireia Majó","Irina Luzko","Henry Córdova","Gloria Fernández-Esparrach","Jorge Bernal"],"pdf_url":"https://arxiv.org/pdf/2409.10286v2.pdf","comment":"MICCAI 2024 (CLIP Workshop)"},{"id":"http://arxiv.org/abs/2404.12968v2","updated":"2024-10-01T11:01:37Z","published":"2024-04-19T15:54:15Z","title":"Scalable Data Assimilation with Message Passing","summary":"  Data assimilation is a core component of numerical weather prediction\nsystems. The large quantity of data processed during assimilation requires the\ncomputation to be distributed across increasingly many compute nodes, yet\nexisting approaches suffer from synchronisation overhead in this setting. In\nthis paper, we exploit the formulation of data assimilation as a Bayesian\ninference problem and apply a message-passing algorithm to solve the spatial\ninference problem. Since message passing is inherently based on local\ncomputations, this approach lends itself to parallel and distributed\ncomputation. In combination with a GPU-accelerated implementation, we can scale\nthe algorithm to very large grid sizes while retaining good accuracy and\ncompute and memory requirements.\n","authors":["Oscar Key","So Takao","Daniel Giles","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2404.12968v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17663v2","updated":"2024-10-01T09:43:43Z","published":"2024-05-27T21:28:26Z","title":"Finding Shared Decodable Concepts and their Negations in the Brain","summary":"  Prior work has offered evidence for functional localization in the brain;\ndifferent anatomical regions preferentially activate for certain types of\nvisual input. For example, the fusiform face area preferentially activates for\nvisual stimuli that include a face. However, the spectrum of visual semantics\nis extensive, and only a few semantically-tuned patches of cortex have so far\nbeen identified in the human brain. Using a multimodal (natural language and\nimage) neural network architecture (CLIP) we train a highly accurate\ncontrastive model that maps brain responses during naturalistic image viewing\nto CLIP embeddings. We then use a novel adaptation of the DBSCAN clustering\nalgorithm to cluster the parameters of these participant-specific contrastive\nmodels. This reveals what we call Shared Decodable Concepts (SDCs): clusters in\nCLIP space that are decodable from common sets of voxels across multiple\nparticipants.\n  Examining the images most and least associated with each SDC cluster gives us\nadditional insight into the semantic properties of each SDC. We note SDCs for\npreviously reported visual features (e.g. orientation tuning in early visual\ncortex) as well as visual semantic concepts such as faces, places and bodies.\nIn cases where our method finds multiple clusters for a visuo-semantic concept,\nthe least associated images allow us to dissociate between confounding factors.\nFor example, we discovered two clusters of food images, one driven by color,\nthe other by shape. We also uncover previously unreported areas such as regions\nof extrastriate body area (EBA) tuned for legs/hands and sensitivity to\nnumerosity in right intraparietal sulcus, and more. Thus, our\ncontrastive-learning methodology better characterizes new and existing\nvisuo-semantic representations in the brain by leveraging multimodal neural\nnetwork representations and a novel adaptation of clustering algorithms.\n","authors":["Cory Efird","Alex Murphy","Joel Zylberberg","Alona Fyshe"],"pdf_url":"https://arxiv.org/pdf/2405.17663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15301v2","updated":"2024-10-01T09:05:45Z","published":"2024-08-27T15:03:01Z","title":"The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization","summary":"  We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts.\n","authors":["Minghai Qin"],"pdf_url":"https://arxiv.org/pdf/2408.15301v2.pdf","comment":"27 pages, 41 figures"},{"id":"http://arxiv.org/abs/2312.08782v3","updated":"2024-10-01T08:54:53Z","published":"2023-12-14T10:02:55Z","title":"Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis","summary":"  Building general-purpose robots that operate seamlessly in any environment,\nwith any object, and utilizing various skills to complete diverse tasks has\nbeen a long-standing goal in Artificial Intelligence. However, as a community,\nwe have been constraining most robotic systems by designing them for specific\ntasks, training them on specific datasets, and deploying them within specific\nenvironments. These systems require extensively-labeled data and task-specific\nmodels. When deployed in real-world scenarios, such systems face several\ngeneralization issues and struggle to remain robust to distribution shifts.\nMotivated by the impressive open-set performance and content generation\ncapabilities of web-scale, large-capacity pre-trained models (i.e., foundation\nmodels) in research fields such as Natural Language Processing (NLP) and\nComputer Vision (CV), we devote this survey to exploring (i) how these existing\nfoundation models from NLP and CV can be applied to the field of\ngeneral-purpose robotics, and also exploring (ii) what a robotics-specific\nfoundation model would look like. We begin by providing a generalized\nformulation of how foundation models are used in robotics, and the fundamental\nbarriers to making generalist robots universally applicable. Next, we establish\na taxonomy to discuss current work exploring ways to leverage existing\nfoundation models for robotics and develop ones catered to robotics. Finally,\nwe discuss key challenges and promising future directions in using foundation\nmodels for enabling general-purpose robotic systems. We encourage readers to\nview our living GitHub repository 2 of resources, including papers reviewed in\nthis survey, as well as related projects and repositories for developing\nfoundation models for robotics.\n","authors":["Yafei Hu","Quanting Xie","Vidhi Jain","Jonathan Francis","Jay Patrikar","Nikhil Keetha","Seungchan Kim","Yaqi Xie","Tianyi Zhang","Hao-Shu Fang","Shibo Zhao","Shayegan Omidshafiei","Dong-Ki Kim","Ali-akbar Agha-mohammadi","Katia Sycara","Matthew Johnson-Roberson","Dhruv Batra","Xiaolong Wang","Sebastian Scherer","Chen Wang","Zsolt Kira","Fei Xia","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2312.08782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13693v2","updated":"2024-10-01T08:40:17Z","published":"2024-05-22T14:39:07Z","title":"Mutatis Mutandis: Revisiting the Comparator in Discrimination Testing","summary":"  Testing for discrimination consists of deriving a profile, known as the\ncomparator, similar to the profile making the discrimination claim, known as\nthe complainant, and comparing the outcomes of these two profiles. An important\naspect for establishing discrimination is evidence, often obtained via\ndiscrimination testing tools that implement the complainant-comparator pair. In\nthis work, we revisit the role of the comparator in discrimination testing. We\nargue for the causal modeling nature of deriving the comparator, and introduce\na two-kinds classification for the comparator: the ceteris paribus (CP), and\nmutatis mutandis (MM) comparators. The CP comparator is the standard one among\ndiscrimination testing, representing an idealized comparison as it aims for\nhaving a complainant-comparator pair that only differs on membership to the\nprotected attribute. As an alternative to it, we define the MM comparator,\nwhich requires that the comparator represents what would have been of the\ncomplainant without the effects of the protected attribute on the non-protected\nattributes. The complainant-comparator pair, in that case, may also be\ndissimilar in terms of all attributes. We illustrate these two comparators and\ntheir impact on discrimination testing using a real illustrative example.\nImportantly, we position generative models and, overall, machine learning\nmethods as useful tools for constructing the MM comparator and, in turn,\nachieving more complex and realistic comparisons when testing for\ndiscrimination.\n","authors":["Jose M. Alvarez","Salvatore Ruggieri"],"pdf_url":"https://arxiv.org/pdf/2405.13693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08887v4","updated":"2024-10-01T08:30:05Z","published":"2023-12-13T09:42:04Z","title":"SpeedUpNet: A Plug-and-Play Adapter Network for Accelerating\n  Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models (SD) exhibit significant advancements while\nrequiring extensive computational resources. Existing acceleration methods\nusually require extensive training and are not universally applicable.\nLCM-LoRA, trainable once for diverse models, offers universality but rarely\nconsiders ensuring the consistency of generated content before and after\nacceleration. This paper proposes SpeedUpNet (SUN), an innovative acceleration\nmodule, to address the challenges of universality and consistency. Exploiting\nthe role of cross-attention layers in U-Net for SD models, we introduce an\nadapter specifically designed for these layers, quantifying the offset in image\ngeneration caused by negative prompts relative to positive prompts. This\nlearned offset demonstrates stability across a range of models, enhancing SUN's\nuniversality. To improve output consistency, we propose a Multi-Step\nConsistency (MSC) loss, which stabilizes the offset and ensures fidelity in\naccelerated content. Experiments on SD v1.5 show that SUN leads to an overall\nspeedup of more than 10 times compared to the baseline 25-step DPM-solver++,\nand offers two extra advantages: (1) training-free integration into various\nfine-tuned Stable-Diffusion models and (2) state-of-the-art FIDs of the\ngenerated data set before and after acceleration guided by random combinations\nof positive and negative prompts. Code is available:\nhttps://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io.\n","authors":["Weilong Chai","DanDan Zheng","Jiajiong Cao","Zhiquan Chen","Changbao Wang","Chenguang Ma"],"pdf_url":"https://arxiv.org/pdf/2312.08887v4.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2409.05395v2","updated":"2024-10-01T08:29:53Z","published":"2024-09-09T07:49:09Z","title":"Shaking Up VLMs: Comparing Transformers and Structured State Space\n  Models for Vision & Language Modeling","summary":"  This study explores replacing Transformers in Visual Language Models (VLMs)\nwith Mamba, a recent structured state space model (SSM) that demonstrates\npromising performance in sequence modeling. We test models up to 3B parameters\nunder controlled conditions, showing that Mamba-based VLMs outperforms\nTransformers-based VLMs in captioning, question answering, and reading\ncomprehension. However, we find that Transformers achieve greater performance\nin visual grounding and the performance gap widens with scale. We explore two\nhypotheses to explain this phenomenon: 1) the effect of task-agnostic visual\nencoding on the updates of the hidden states, and 2) the difficulty in\nperforming visual grounding from the perspective of in-context multimodal\nretrieval. Our results indicate that a task-aware encoding yields minimal\nperformance gains on grounding, however, Transformers significantly outperform\nMamba at in-context multimodal retrieval. Overall, Mamba shows promising\nperformance on tasks where the correct output relies on a summary of the image\nbut struggles when retrieval of explicit information from the context is\nrequired.\n","authors":["Georgios Pantazopoulos","Malvina Nikandrou","Alessandro Suglia","Oliver Lemon","Arash Eshghi"],"pdf_url":"https://arxiv.org/pdf/2409.05395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19262v2","updated":"2024-10-01T08:05:23Z","published":"2024-03-28T09:36:55Z","title":"Removing the need for ground truth UWB data collection: self-supervised\n  ranging error correction using deep reinforcement learning","summary":"  Indoor positioning using UWB technology has gained interest due to its\ncentimeter-level accuracy potential. However, multipath effects and\nnon-line-of-sight conditions cause ranging errors between anchors and tags.\nExisting approaches for mitigating these ranging errors rely on collecting\nlarge labeled datasets, making them impractical for real-world deployments.\nThis paper proposes a novel self-supervised deep reinforcement learning\napproach that does not require labeled ground truth data. A reinforcement\nlearning agent uses the channel impulse response as a state and predicts\ncorrections to minimize the error between corrected and estimated ranges. The\nagent learns, self-supervised, by iteratively improving corrections that are\ngenerated by combining the predictability of trajectories with filtering and\nsmoothening. Experiments on real-world UWB measurements demonstrate comparable\nperformance to state-of-the-art supervised methods, overcoming data dependency\nand lack of generalizability limitations. This makes self-supervised deep\nreinforcement learning a promising solution for practical and scalable\nUWB-ranging error correction.\n","authors":["Dieter Coppens","Ben Van Herbruggen","Adnan Shahid","Eli De Poorter"],"pdf_url":"https://arxiv.org/pdf/2403.19262v2.pdf","comment":"13 pages, 9 figures and 5 tables"},{"id":"http://arxiv.org/abs/2408.07165v3","updated":"2024-10-01T07:52:54Z","published":"2024-08-13T19:08:56Z","title":"A POD-TANN approach for the multiscale modeling of materials and\n  macroelement derivation in geomechanics","summary":"  This paper introduces a novel approach that combines Proper Orthogonal\nDecomposition (POD) with Thermodynamics-based Artificial Neural Networks (TANN)\nto capture the macroscopic behavior of complex inelastic systems and derive\nmacroelements in geomechanics.\n  The methodology leverages POD to extract macroscopic Internal State Variables\nfrom microscopic state information, thereby enriching the macroscopic state\ndescription used to train an energy potential network within the TANN\nframework. The thermodynamic consistency provided by TANN, combined with the\nhierarchical nature of POD, allows to reproduce complex, non-linear inelastic\nmaterial behaviors as well as macroscopic geomechanical systems responses.\n  The approach is validated through applications of increasing complexity,\ndemonstrating its capability to reproduce high-fidelity simulation data. The\napplications proposed include the homogenization of continuous inelastic\nrepresentative unit cells and the derivation of a macroelement for a\ngeotechnical system involving a monopile in a clay layer subjected to\nhorizontal loading. Eventually, the projection operators directly obtained via\nPOD, are exploit to easily reconstruct the microscopic fields.\n  The results indicate that the POD-TANN approach not only offers accuracy in\nreproducing the studied constitutive responses, but also reduces computational\ncosts, making it a practical tool for the multiscale modeling of heterogeneous\ninelastic geomechanical systems.\n","authors":["Giovanni Piunno","Ioannis Stefanou","Cristina Jommi"],"pdf_url":"https://arxiv.org/pdf/2408.07165v3.pdf","comment":"46 pages, Submitted to International Journal for Numerical and\n  Analytical Methods in Geomechanics"},{"id":"http://arxiv.org/abs/2409.18467v2","updated":"2024-10-01T07:46:04Z","published":"2024-09-27T06:12:31Z","title":"A TextGCN-Based Decoding Approach for Improving Remote Sensing Image\n  Captioning","summary":"  Remote sensing images are highly valued for their ability to address complex\nreal-world issues such as risk management, security, and meteorology. However,\nmanually captioning these images is challenging and requires specialized\nknowledge across various domains. This letter presents an approach for\nautomatically describing (captioning) remote sensing images. We propose a novel\nencoder-decoder setup that deploys a Text Graph Convolutional Network (TextGCN)\nand multi-layer LSTMs. The embeddings generated by TextGCN enhance the\ndecoder's understanding by capturing the semantic relationships among words at\nboth the sentence and corpus levels. Furthermore, we advance our approach with\na comparison-based beam search method to ensure fairness in the search strategy\nfor generating the final caption. We present an extensive evaluation of our\napproach against various other state-of-the-art encoder-decoder frameworks. We\nevaluated our method across three datasets using seven metrics: BLEU-1 to\nBLEU-4, METEOR, ROUGE-L, and CIDEr. The results demonstrate that our approach\nsignificantly outperforms other state-of-the-art encoder-decoder methods.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2409.18467v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2402.15898v4","updated":"2024-10-01T07:45:38Z","published":"2024-02-13T09:22:45Z","title":"Transductive Active Learning: Theory and Applications","summary":"  We generalize active learning to address real-world settings with concrete\nprediction targets where sampling is restricted to an accessible region of the\ndomain, while prediction targets may lie outside this region. We analyze a\nfamily of decision rules that sample adaptively to minimize uncertainty about\nprediction targets. We are the first to show, under general regularity\nassumptions, that such decision rules converge uniformly to the smallest\npossible uncertainty obtainable from the accessible data. We demonstrate their\nstrong sample efficiency in two key applications: Active few-shot fine-tuning\nof large neural networks and safe Bayesian optimization, where they improve\nsignificantly upon the state-of-the-art.\n","authors":["Jonas Hübotter","Bhavya Sukhija","Lenart Treven","Yarden As","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2402.15898v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.15441"},{"id":"http://arxiv.org/abs/2409.04813v2","updated":"2024-10-01T07:28:39Z","published":"2024-09-07T12:53:44Z","title":"Generalized Learning of Coefficients in Spectral Graph Convolutional\n  Networks","summary":"  Spectral Graph Convolutional Networks (GCNs) have gained popularity in graph\nmachine learning applications due, in part, to their flexibility in\nspecification of network propagation rules. These propagation rules are often\nconstructed as polynomial filters whose coefficients are learned using label\ninformation during training. In contrast to learned polynomial filters,\nexplicit filter functions are useful in capturing relationships between network\ntopology and distribution of labels across the network. A number of algorithms\nincorporating either approach have been proposed; however the relationship\nbetween filter functions and polynomial approximations is not fully resolved.\nThis is largely due to the ill-conditioned nature of the linear systems that\nmust be solved to derive polynomial approximations of filter functions. To\naddress this challenge, we propose a novel Arnoldi orthonormalization-based\nalgorithm, along with a unifying approach, called G-Arnoldi-GCN that can\nefficiently and effectively approximate a given filter function with a\npolynomial. We evaluate G-Arnoldi-GCN in the context of multi-class node\nclassification across ten datasets with diverse topological characteristics.\nOur experiments show that G-Arnoldi-GCN consistently outperforms\nstate-of-the-art methods when suitable filter functions are employed. Overall,\nG-Arnoldi-GCN opens important new directions in graph machine learning by\nenabling the explicit design and application of diverse filter functions. Code\nlink: https://github.com/mustafaCoskunAgu/GArnoldi-GCN\n","authors":["Mustafa Coşkun","Ananth Grama","Mehmet Koyutürk"],"pdf_url":"https://arxiv.org/pdf/2409.04813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10874v2","updated":"2024-10-01T07:10:39Z","published":"2023-01-25T23:47:34Z","title":"Recursive deep learning framework for forecasting the decadal world\n  economic outlook","summary":"  The gross domestic product (GDP) is the most widely used indicator in\nmacroeconomics and the main tool for measuring a country's economic output. Due\nto the diversity and complexity of the world economy, a wide range of models\nhave been used, but there are challenges in making decadal GDP forecasts given\nunexpected changes such as emergence of catastrophic world events including\npandemics and wars. Deep learning models are well suited for modelling temporal\nsequences and time series forecasting. In this paper, we develop a deep\nlearning framework to forecast the GDP growth rate of the world economy over a\ndecade. We use the Penn World Table as the data source featuring 13 countries\nprior to the COVID-19 pandemic, such as Australia, China, India, and the United\nStates. We present a recursive deep learning framework to predict the GDP\ngrowth rate in the next ten years. We test prominent deep learning models and\ncompare their results with traditional econometric models for selected\ndeveloped and developing countries. Our decadal forecasts reveal that that most\nof the developed countries would experience economic growth slowdown,\nstagnation and even recession within five years (2020-2024). Furthermore, our\nmodel forecasts show that only China, France, and India would experience stable\nGDP growth.\n","authors":["Tianyi Wang","Rodney Beard","John Hawkins","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2301.10874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14119v2","updated":"2024-10-01T07:10:02Z","published":"2024-09-21T12:20:18Z","title":"Obliviate: Neutralizing Task-agnostic Backdoors within the\n  Parameter-efficient Fine-tuning Paradigm","summary":"  Parameter-efficient fine-tuning (PEFT) has become a key training strategy for\nlarge language models. However, its reliance on fewer trainable parameters\nposes security risks, such as task-agnostic backdoors. Despite their severe\nimpact on a wide range of tasks, there is no practical defense solution\navailable that effectively counters task-agnostic backdoors within the context\nof PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor\ndefense. We develop two techniques aimed at amplifying benign neurons within\nPEFT layers and penalizing the influence of trigger tokens. Our evaluations\nacross three major PEFT architectures show that our method can significantly\nreduce the attack success rate of the state-of-the-art task-agnostic backdoors\n(83.6%$\\downarrow$). Furthermore, our method exhibits robust defense\ncapabilities against both task-specific backdoors and adaptive attacks. Source\ncode will be obtained at https://github.com/obliviateARR/Obliviate.\n","authors":["Jaehan Kim","Minkyoo Song","Seung Ho Na","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2409.14119v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.07776v2","updated":"2024-10-01T07:04:55Z","published":"2024-04-11T14:13:53Z","title":"Unsupervised Concept Drift Detection based on Parallel Activations of\n  Neural Network","summary":"  Practical applications of artificial intelligence increasingly often have to\ndeal with the streaming properties of real data, which, considering the time\nfactor, are subject to phenomena such as periodicity and more or less chaotic\ndegeneration - resulting directly in the concept drifts. The modern concept\ndrift detectors almost always assume immediate access to labels, which due to\ntheir cost, limited availability and possible delay has been shown to be\nunrealistic. This work proposes an unsupervised Parallel Activations Drift\nDetector, utilizing the outputs of an untrained neural network, presenting its\nkey design elements, intuitions about processing properties, and a pool of\ncomputer experiments demonstrating its competitiveness with state-of-the-art\nmethods.\n","authors":["Joanna Komorniczak","Paweł Ksieniewicz"],"pdf_url":"https://arxiv.org/pdf/2404.07776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19422v2","updated":"2024-10-01T07:04:04Z","published":"2024-09-28T17:43:17Z","title":"Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures","summary":"  A core task in multi-modal learning is to integrate information from multiple\nfeature spaces (e.g., text and audio), offering modality-invariant essential\nrepresentations of data. Recent research showed that, classical tools such as\n{\\it canonical correlation analysis} (CCA) provably identify the shared\ncomponents up to minor ambiguities, when samples in each modality are generated\nfrom a linear mixture of shared and private components. Such identifiability\nresults were obtained under the condition that the cross-modality samples are\naligned/paired according to their shared information. This work takes a step\nfurther, investigating shared component identifiability from multi-modal linear\nmixtures where cross-modality samples are unaligned. A distribution divergence\nminimization-based loss is proposed, under which a suite of sufficient\nconditions ensuring identifiability of the shared components are derived. Our\nconditions are based on cross-modality distribution discrepancy\ncharacterization and density-preserving transform removal, which are much\nmilder than existing studies relying on independent component analysis. More\nrelaxed conditions are also provided via adding reasonable structural\nconstraints, motivated by available side information in various applications.\nThe identifiability claims are thoroughly validated using synthetic and\nreal-world data.\n","authors":["Subash Timilsina","Sagar Shrestha","Xiao Fu"],"pdf_url":"https://arxiv.org/pdf/2409.19422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13208v3","updated":"2024-10-01T06:42:29Z","published":"2024-09-20T04:32:54Z","title":"Redefining Data Pairing for Motion Retargeting Leveraging a Human Body\n  Prior","summary":"  We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a\ncost-effective and convenient method to collect high-quality upper body paired\n<robot, human> pose data, which is essential for data-driven motion retargeting\nmethods. Unlike existing approaches which collect <robot, human> pose data by\nconverting human MoCap poses into robot poses, our method goes in reverse. We\nfirst sample diverse random robot poses, and then convert them into human\nposes. However, since random robot poses can result in extreme and infeasible\nhuman poses, we propose an additional technique to sort out extreme poses by\nexploiting a human body prior trained from a large amount of human pose data.\nOur data collection method can be used for any humanoid robots, if one designs\nor optimizes the system's hyperparameters which include a size scale factor and\nthe joint angle ranges for sampling. In addition to this data collection\nmethod, we also present a two-stage motion retargeting neural network that can\nbe trained via supervised learning on a large amount of paired data. Compared\nto other learning-based methods trained via unsupervised learning, we found\nthat our deep neural network trained with ample high-quality paired data\nachieved notable performance. Our experiments also show that our data filtering\nmethod yields better retargeting results than training the model with raw and\nnoisy data. Our code and video results are available on\nhttps://sites.google.com/view/mr-hubo/\n","authors":["Xiyana Figuera","Soogeun Park","Hyemin Ahn"],"pdf_url":"https://arxiv.org/pdf/2409.13208v3.pdf","comment":"8 pages, 5 Figures, Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2405.16124v2","updated":"2024-10-01T06:29:08Z","published":"2024-05-25T08:29:46Z","title":"Unsupervised Meta-Learning via In-Context Learning","summary":"  Unsupervised meta-learning aims to learn feature representations from\nunsupervised datasets that can transfer to downstream tasks with limited\nlabeled data. In this paper, we propose a novel approach to unsupervised\nmeta-learning that leverages the generalization abilities of in-context\nlearning observed in transformer architectures. Our method reframes\nmeta-learning as a sequence modeling problem, enabling the transformer encoder\nto learn task context from support images and utilize it to predict query\nimages. At the core of our approach lies the creation of diverse tasks\ngenerated using a combination of data augmentations and a mixing strategy that\nchallenges the model during training while fostering generalization to unseen\ntasks at test time. Experimental results on benchmark datasets showcase the\nsuperiority of our approach over existing unsupervised meta-learning baselines,\nestablishing it as the new state-of-the-art in the field. Remarkably, our\nmethod achieves competitive results with supervised and self-supervised\napproaches, underscoring the efficacy of the model in leveraging generalization\nover memorization.\n","authors":["Anna Vettoruzzo","Lorenzo Braccaioli","Joaquin Vanschoren","Marlena Nowaczyk"],"pdf_url":"https://arxiv.org/pdf/2405.16124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10569v2","updated":"2024-10-01T06:08:00Z","published":"2024-06-15T09:08:58Z","title":"MDA: An Interpretable Multi-Modal Fusion with Missing Modalities and\n  Intrinsic Noise","summary":"  Multi-modal fusion is crucial in medical data research, enabling a\ncomprehensive understanding of diseases and improving diagnostic performance by\ncombining diverse modalities. However, multi-modal fusion faces challenges,\nincluding capturing interactions between modalities, addressing missing\nmodalities, handling erroneous modal information, and ensuring\ninterpretability. Many existing researchers tend to design different solutions\nfor these problems, often overlooking the commonalities among them. This paper\nproposes a novel multi-modal fusion framework that achieves adaptive adjustment\nover the weights of each modality by introducing the Modal-Domain Attention\n(MDA). It aims to facilitate the fusion of multi-modal information while\nallowing for the inclusion of missing modalities or intrinsic noise, thereby\nenhancing the representation of multi-modal data. We provide visualizations of\naccuracy changes and MDA weights by observing the process of modal fusion,\noffering a comprehensive analysis of its interpretability. Extensive\nexperiments on various gastrointestinal disease benchmarks, the proposed MDA\nmaintains high accuracy even in the presence of missing modalities and\nintrinsic noise. One thing worth mentioning is that the visualization of MDA is\nhighly consistent with the conclusions of existing clinical studies on the\ndependence of different diseases on various modalities. Code and dataset will\nbe made available.\n","authors":["Lin Fan","Yafei Ou","Cenyang Zheng","Pengyu Dai","Tamotsu Kamishima","Masayuki Ikebe","Kenji Suzuki","Xun Gong"],"pdf_url":"https://arxiv.org/pdf/2406.10569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v2","updated":"2024-10-01T05:55:33Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL technology has become crucial for translating natural language\ninto SQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, we found that the\nExecution Accuracy (EX), the most promising evaluation metric, still shows a\nsubstantial portion of false positives and negatives compared to human\nevaluation. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our method shows\nsignificantly higher agreement with human expert judgments, improving Cohen's\nkappa from 61 to 78.17. Re-evaluating top-performing models on the Spider and\nBIRD benchmarks using FLEX reveals substantial shifts in performance rankings,\nwith an average performance decrease of 3.15 due to false positive corrections\nand an increase of 6.07 from addressing false negatives. This work contributes\nto a more accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v2.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2404.07198v2","updated":"2024-10-01T05:52:11Z","published":"2024-04-10T17:56:07Z","title":"A Foundation Model for Zero-shot Logical Query Reasoning","summary":"  Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond\nsimple KG completion and aims at answering compositional queries comprised of\nmultiple projections and logical operations. Existing CLQA methods that learn\nparameters bound to certain entity or relation vocabularies can only be applied\nto the graph they are trained on which requires substantial training time\nbefore being deployed on a new graph. Here we present UltraQuery, the first\nfoundation model for inductive reasoning that can zero-shot answer logical\nqueries on any KG. The core idea of UltraQuery is to derive both projections\nand logical operations as vocabulary-independent functions which generalize to\nnew entities and relations in any KG. With the projection operation initialized\nfrom a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on\nany KG after finetuning on a single dataset. Experimenting on 23 datasets,\nUltraQuery in the zero-shot inference mode shows competitive or better query\nanswering performance than best available baselines and sets a new state of the\nart on 15 of them.\n","authors":["Mikhail Galkin","Jincheng Zhou","Bruno Ribeiro","Jian Tang","Zhaocheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.07198v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.08642v2","updated":"2024-10-01T05:42:12Z","published":"2024-09-13T08:59:31Z","title":"CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning\n  Tasks","summary":"  Post-training, particularly reinforcement learning (RL) using\nself-play-generated data, has become a new learning paradigm for large language\nmodels (LLMs). However, scaling RL to develop a general reasoner remains a\nresearch challenge, as existing methods focus on task-specific reasoning\nwithout adequately addressing generalization across a broader range of tasks.\nMoreover, unlike traditional RL with limited action space, LLMs operate in an\ninfinite space, making it crucial to search for valuable and diverse strategies\nto solve problems effectively. To address this, we propose searching within the\naction space on high-level abstract plans to enhance model generalization and\nintroduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan,\nusing Monte Carlo Tree Search (MCTS) to explore diverse plan steps in\nmulti-step reasoning tasks, and 2) learning critical plan steps through\nStep-level Advantage Preference Optimization (Step-APO), which integrates\nadvantage estimates for step preference obtained via MCTS into Direct\nPreference Optimization (DPO). This combination helps the model effectively\nlearn critical plan steps, enhancing both reasoning capabilities and\ngeneralization. Experimental results demonstrate that our method, trained\nexclusively on GSM8K and MATH, not only significantly improves performance on\nGSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning\nbenchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM\n(+2.2%), and BBH (+1.8%).\n","authors":["Tianlong Wang","Junzhe Chen","Xueting Han","Jing Bai"],"pdf_url":"https://arxiv.org/pdf/2409.08642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20135v2","updated":"2024-10-01T05:37:07Z","published":"2024-09-30T09:34:31Z","title":"Federated Instruction Tuning of LLMs with Domain Coverage Augmentation","summary":"  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data alongside server-side public data for instruction\naugmentation, ultimately enhancing model performance within specific domains.\nWhile the factors affecting FedDIT remain unclear and existing instruction\naugmentation methods mainly focus on the centralized setting without\nconsidering the distributed environment. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. To alleviate client-side computational burdens, FedDCA$^*$ uses\nheterogeneous encoders with server-side feature alignment. Extensive\nexperiments across four distinct domains (code, medical, financial, and\nmathematical) substantiate the effectiveness of both methods. Additionally, we\ninvestigate privacy preservation against memory extraction attacks utilizing\nvarying amounts of public data. Results show no significant correlation between\nthe volume of public data and the privacy-preserving capability. However, as\nthe fine-tuning round increases, the risk of privacy leakage reduces or\nconverges.\n","authors":["Zezhou Wang","Yaxin Du","Zhuzhong Qian","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04272v5","updated":"2024-10-01T05:20:59Z","published":"2024-07-05T05:55:18Z","title":"Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression","summary":"  DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.\n","authors":["Hao Feng","Boyuan Zhang","Fanjiang Ye","Min Si","Ching-Hsiang Chu","Jiannan Tian","Chunxing Yin","Summer Deng","Yuchen Hao","Pavan Balaji","Tong Geng","Dingwen Tao"],"pdf_url":"https://arxiv.org/pdf/2407.04272v5.pdf","comment":"camera-ready version for SC '24"},{"id":"http://arxiv.org/abs/2405.16206v3","updated":"2024-10-01T05:14:15Z","published":"2024-05-25T12:35:31Z","title":"GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine\n  Learning","summary":"  Glycans are basic biomolecules and perform essential functions within living\norganisms. The rapid increase of functional glycan data provides a good\nopportunity for machine learning solutions to glycan understanding. However,\nthere still lacks a standard machine learning benchmark for glycan property and\nfunction prediction. In this work, we fill this blank by building a\ncomprehensive benchmark for Glycan Machine Learning (GlycanML). The GlycanML\nbenchmark consists of diverse types of tasks including glycan taxonomy\nprediction, glycan immunogenicity prediction, glycosylation type prediction,\nand protein-glycan interaction prediction. Glycans can be represented by both\nsequences and graphs in GlycanML, which enables us to extensively evaluate\nsequence-based models and graph neural networks (GNNs) on benchmark tasks.\nFurthermore, by concurrently performing eight glycan taxonomy prediction tasks,\nwe introduce the GlycanML-MTL testbed for multi-task learning (MTL) algorithms.\nAlso, we evaluate how taxonomy prediction can boost other three function\nprediction tasks by MTL. Experimental results show the superiority of modeling\nglycans with multi-relational GNNs, and suitable MTL methods can further boost\nmodel performance. We provide all datasets and source codes at\nhttps://github.com/GlycanML/GlycanML and maintain a leaderboard at\nhttps://GlycanML.github.io/project\n","authors":["Minghao Xu","Yunteng Geng","Yihang Zhang","Ling Yang","Jian Tang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16206v3.pdf","comment":"Research project paper. All code and data are released"},{"id":"http://arxiv.org/abs/2310.06341v2","updated":"2024-10-01T04:44:29Z","published":"2023-10-10T06:22:06Z","title":"Federated Learning with Reduced Information Leakage and Computation","summary":"  Federated learning (FL) is a distributed learning paradigm that allows\nmultiple decentralized clients to collaboratively learn a common model without\nsharing local data. Although local data is not exposed directly, privacy\nconcerns nonetheless exist as clients' sensitive information can be inferred\nfrom intermediate computations. Moreover, such information leakage accumulates\nsubstantially over time as the same data is repeatedly used during the\niterative learning process. As a result, it can be particularly difficult to\nbalance the privacy-accuracy trade-off when designing privacy-preserving FL\nalgorithms. This paper introduces Upcycled-FL, a simple yet effective strategy\nthat applies first-order approximation at every even round of model update.\nUnder this strategy, half of the FL updates incur no information leakage and\nrequire much less computational and transmission costs. We first conduct the\ntheoretical analysis on the convergence (rate) of Upcycled-FL and then apply\ntwo perturbation mechanisms to preserve privacy. Extensive experiments on both\nsynthetic and real-world data show that the Upcycled-FL strategy can be adapted\nto many existing FL frameworks and consistently improve the privacy-accuracy\ntrade-off.\n","authors":["Tongxin Yin","Xuwei Tan","Xueru Zhang","Mohammad Mahdi Khalili","Mingyan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.06341v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2312.02186v2","updated":"2024-10-01T04:39:14Z","published":"2023-12-01T20:16:02Z","title":"Identifying Spurious Correlations using Counterfactual Alignment","summary":"  Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual (CF) alignment method to detect and\nquantify spurious correlations of black box classifiers. Our methodology is\nbased on counterfactual images generated with respect to one classifier being\ninput into other classifiers to see if they also induce changes in the outputs\nof these classifiers. The relationship between these responses can be\nquantified and used to identify specific instances where a spurious correlation\nexists. This is validated by observing intuitive trends in a face-attribute\nface-attribute and waterbird classifiers, as well as by fabricating spurious\ncorrelations and detecting their presence, both visually and quantitatively.\nFurthermore, utilizing the CF alignment method, we demonstrate that we can\nevaluate robust optimization methods (GroupDRO, JTT, and FLAC) by detecting a\nreduction in spurious correlations.\n","authors":["Joseph Paul Cohen","Louis Blankemeier","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2312.02186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.03169v4","updated":"2024-10-01T04:34:34Z","published":"2022-01-10T05:26:33Z","title":"FedDTG:Federated Data-Free Knowledge Distillation via Three-Player\n  Generative Adversarial Networks","summary":"  While existing federated learning approaches primarily focus on aggregating\nlocal models to construct a global model, in realistic settings, some clients\nmay be reluctant to share their private models due to the inclusion of\nprivacy-sensitive information. Knowledge distillation, which can extract model\nknowledge without accessing model parameters, is well-suited for this federated\nscenario. However, most distillation methods in federated learning (federated\ndistillation) require a proxy dataset, which is difficult to obtain in the real\nworld. Therefore, in this paper, we introduce a distributed three-player\nGenerative Adversarial Network (GAN) to implement data-free mutual distillation\nand propose an effective method called FedDTG. We confirmed that the fake\nsamples generated by GAN can make federated distillation more efficient and\nrobust. Additionally, the distillation process between clients can deliver good\nindividual client performance while simultaneously acquiring global knowledge\nand protecting data privacy. Our extensive experiments on benchmark vision\ndatasets demonstrate that our method outperforms other federated distillation\nalgorithms in terms of generalization.\n","authors":["Lingzhi Gao","Zhenyuan Zhang","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2201.03169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02262v2","updated":"2024-10-01T04:10:34Z","published":"2023-11-03T22:56:43Z","title":"Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs","summary":"  In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need --\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .\n","authors":["Qingru Zhang","Chandan Singh","Liyuan Liu","Xiaodong Liu","Bin Yu","Jianfeng Gao","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.02262v2.pdf","comment":"The 12th International Conference on Learning Representations (ICLR\n  2024)"},{"id":"http://arxiv.org/abs/2409.15355v3","updated":"2024-10-01T03:40:08Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20310v2","updated":"2024-10-01T03:32:24Z","published":"2024-09-30T14:10:02Z","title":"A SSM is Polymerized from Multivariate Time Series","summary":"  For multivariate time series (MTS) tasks, previous state space models (SSMs)\nfollowed the modeling paradigm of Transformer-based methods. However, none of\nthem explicitly model the complex dependencies of MTS: the Channel Dependency\nvariations with Time (CDT). In view of this, we delve into the derivation of\nSSM, which involves approximating continuously updated functions by orthogonal\nfunction basis. We then develop Poly-Mamba, a novel method for MTS forecasting.\nIts core concept is to expand the original orthogonal function basis space into\na multivariate orthogonal function space containing variable mixing terms, and\nmake a projection on this space so as to explicitly describe the CDT by\nweighted coefficients. In Poly-Mamba, we propose the Multivariate Orthogonal\nPolynomial Approximation (MOPA) as a simplified implementation of this concept.\nFor the simple linear relationship between channels, we propose Linear Channel\nMixing (LCM) and generate CDT patterns adaptively for different channels\nthrough a proposed Order Combining method. Experiments on six real-world\ndatasets demonstrate that Poly-Mamba outperforms the SOTA methods, especially\nwhen dealing with datasets having a large number of channels and complex\ncorrelations. The codes and log files will be released at:\nhttps://github.com/Joeland4/Poly-Mamba.\n","authors":["Haixiang Wu"],"pdf_url":"https://arxiv.org/pdf/2409.20310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15868v3","updated":"2024-10-01T03:12:35Z","published":"2024-09-24T08:41:26Z","title":"Privacy Evaluation Benchmarks for NLP Models","summary":"  By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.\n","authors":["Wei Huang","Yinggui Wang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.15868v3.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.15004v2","updated":"2024-10-01T02:48:20Z","published":"2024-03-22T07:32:21Z","title":"ParFormer: A Vision Transformer with Parallel Mixer and Sparse Channel\n  Attention Patch Embedding","summary":"  Convolutional Neural Networks (CNNs) and Transformers have achieved\nremarkable success in computer vision tasks. However, their deep architectures\noften lead to high computational redundancy, making them less suitable for\nresource-constrained environments, such as edge devices. This paper introduces\nParFormer, a novel vision transformer that addresses this challenge by\nincorporating a Parallel Mixer and a Sparse Channel Attention Patch Embedding\n(SCAPE). By combining convolutional and attention mechanisms, ParFormer\nimproves feature extraction. This makes spatial feature extraction more\nefficient and cuts down on unnecessary computation. The SCAPE module further\nreduces computational redundancy while preserving essential feature information\nduring down-sampling. Experimental results on the ImageNet-1K dataset show that\nParFormer-T achieves 78.9\\% Top-1 accuracy with a high throughput on a GPU that\noutperforms other small models with 2.56$\\times$ higher throughput than\nMobileViT-S, 0.24\\% faster than FasterNet-T2, and 1.79$\\times$ higher than\nEdgeNeXt-S. For edge device deployment, ParFormer-T excels with a throughput of\n278.1 images/sec, which is 1.38 $\\times$ higher than EdgeNeXt-S and\n2.36$\\times$ higher than MobileViT-S, making it highly suitable for real-time\napplications in resource-constrained settings. The larger variant, ParFormer-L,\nreaches 83.5\\% Top-1 accuracy, offering a balanced trade-off between accuracy\nand efficiency, surpassing many state-of-the-art models. In COCO object\ndetection, ParFormer-M achieves 40.7 AP for object detection and 37.6 AP for\ninstance segmentation, surpassing models like ResNet-50, PVT-S and\nPoolFormer-S24 with significantly higher efficiency. These results validate\nParFormer as a highly efficient and scalable model for both high-performance\nand resource-constrained scenarios, making it an ideal solution for edge-based\nAI applications.\n","authors":["Novendra Setyawan","Ghufron Wahyu Kurniawan","Chi-Chia Sun","Jun-Wei Hsieh","Jing-Ming Guo","Wen-Kai Kuo"],"pdf_url":"https://arxiv.org/pdf/2403.15004v2.pdf","comment":"Under Review in IEEE Transactions on Cognitive and Developmental\n  System"},{"id":"http://arxiv.org/abs/2409.19546v2","updated":"2024-10-01T02:33:09Z","published":"2024-09-29T04:16:24Z","title":"Almost Sure Convergence of Average Reward Temporal Difference Learning","summary":"  Tabular average reward Temporal Difference (TD) learning is perhaps the\nsimplest and the most fundamental policy evaluation algorithm in average reward\nreinforcement learning. After at least 25 years since its discovery, we are\nfinally able to provide a long-awaited almost sure convergence analysis.\nNamely, we are the first to prove that, under very mild conditions, tabular\naverage reward TD converges almost surely to a sample path dependent fixed\npoint. Key to this success is a new general stochastic approximation result\nconcerning nonexpansive mappings with Markovian and additive noise, built on\nrecent advances in stochastic Krasnoselskii-Mann iterations.\n","authors":["Ethan Blaser","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13659v3","updated":"2024-10-01T02:12:41Z","published":"2024-08-24T19:19:33Z","title":"ReactZyme: A Benchmark for Enzyme-Reaction Prediction","summary":"  Enzymes, with their specific catalyzed reactions, are necessary for all\naspects of life, enabling diverse biological processes and adaptations.\nPredicting enzyme functions is essential for understanding biological pathways,\nguiding drug development, enhancing bioproduct yields, and facilitating\nevolutionary studies. Addressing the inherent complexities, we introduce a new\napproach to annotating enzymes based on their catalyzed reactions. This method\nprovides detailed insights into specific reactions and is adaptable to newly\ndiscovered reactions, diverging from traditional classifications by protein\nfamily or expert-derived reaction classes. We employ machine learning\nalgorithms to analyze enzyme reaction datasets, delivering a much more refined\nview on the functionality of enzymes. Our evaluation leverages the largest\nenzyme-reaction dataset to date, derived from the SwissProt and Rhea databases\nwith entries up to January 8, 2024. We frame the enzyme-reaction prediction as\na retrieval problem, aiming to rank enzymes by their catalytic ability for\nspecific reactions. With our model, we can recruit proteins for novel reactions\nand predict reactions in novel proteins, facilitating enzyme discovery and\nfunction annotation (https://github.com/WillHua127/ReactZyme).\n","authors":["Chenqing Hua","Bozitao Zhong","Sitao Luan","Liang Hong","Guy Wolf","Doina Precup","Shuangjia Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.13659v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15963v3","updated":"2024-10-01T02:00:50Z","published":"2024-09-24T10:48:13Z","title":"Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning","summary":"  To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with guaranteed efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments.\n","authors":["Bo Yue","Jian Li","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15963v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00760v2","updated":"2024-10-01T01:04:58Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\nhttps://github.com/SusungHong/SEG-SDXL.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.19420v2","updated":"2024-10-01T00:14:07Z","published":"2024-05-29T18:01:58Z","title":"Using Contrastive Learning with Generative Similarity to Learn Spaces\n  that Capture Human Inductive Biases","summary":"  Humans rely on strong inductive biases to learn from few examples and\nabstract useful information from sensory data. Instilling such biases in\nmachine learning models has been shown to improve their performance on various\nbenchmarks including few-shot learning, robustness, and alignment. However,\nfinding effective training procedures to achieve that goal can be challenging\nas psychologically-rich training data such as human similarity judgments are\nexpensive to scale, and Bayesian models of human inductive biases are often\nintractable for complex, realistic domains. Here, we address this challenge by\nintroducing a Bayesian notion of generative similarity whereby two datapoints\nare considered similar if they are likely to have been sampled from the same\ndistribution. This measure can be applied to complex generative processes,\nincluding probabilistic programs. We show that generative similarity can be\nused to define a contrastive learning objective even when its exact form is\nintractable, enabling learning of spatial embeddings that express specific\ninductive biases. We demonstrate the utility of our approach by showing that it\ncan be used to capture human inductive biases for geometric shapes, distinguish\ndifferent abstract drawing styles that are parameterized by probabilistic\nprograms, and capture abstract high-level categories that enable\ngeneralization.\n","authors":["Raja Marjieh","Sreejan Kumar","Declan Campbell","Liyi Zhang","Gianluca Bencomo","Jake Snell","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2405.19420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04978v2","updated":"2024-10-01T00:11:48Z","published":"2024-01-10T07:47:42Z","title":"Closed-Form Interpretation of Neural Network Classifiers with Symbolic\n  Gradients","summary":"  I introduce a unified framework for finding a closed-form interpretation of\nany single neuron in an artificial neural network. Using this framework I\ndemonstrate how to interpret neural network classifiers to reveal closed-form\nexpressions of the concepts encoded in their decision boundaries. In contrast\nto neural network-based regression, for classification, it is in general\nimpossible to express the neural network in the form of a symbolic equation\neven if the neural network itself bases its classification on a quantity that\ncan be written as a closed-form equation. The interpretation framework is based\non embedding trained neural networks into an equivalence class of functions\nthat encode the same concept. I interpret these neural networks by finding an\nintersection between the equivalence class and human-readable equations defined\nby a symbolic search space. The approach is not limited to classifiers or full\nneural networks and can be applied to arbitrary neurons in hidden layers or\nlatent spaces.\n","authors":["Sebastian Johann Wetzel"],"pdf_url":"https://arxiv.org/pdf/2401.04978v2.pdf","comment":null}]},"2024-10-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.01805v1","updated":"2024-10-02T17:59:52Z","published":"2024-10-02T17:59:52Z","title":"Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads","summary":"  Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.\n","authors":["Yuxiang Huang","Binhang Yuan","Xu Han","Chaojun Xiao","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01805v1.pdf","comment":"Preprints"},{"id":"http://arxiv.org/abs/2410.01795v1","updated":"2024-10-02T17:53:08Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01794v1","updated":"2024-10-02T17:52:41Z","published":"2024-10-02T17:52:41Z","title":"Loki: An Open-Source Tool for Fact Verification","summary":"  We introduce Loki, an open-source tool designed to address the growing\nproblem of misinformation. Loki adopts a human-centered approach, striking a\nbalance between the quality of fact-checking and the cost of human involvement.\nIt decomposes the fact-checking task into a five-step pipeline: breaking down\nlong texts into individual claims, assessing their check-worthiness, generating\nqueries, retrieving evidence, and verifying the claims. Instead of fully\nautomating the claim verification process, Loki provides essential information\nat each step to assist human judgment, especially for general users such as\njournalists and content moderators. Moreover, it has been optimized for\nlatency, robustness, and cost efficiency at a commercially usable level. Loki\nis released under an MIT license and is available on GitHub. We also provide a\nvideo presenting the system and its capabilities.\n","authors":["Haonan Li","Xudong Han","Hao Wang","Yuxia Wang","Minghan Wang","Rui Xing","Yilin Geng","Zenan Zhai","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2410.01794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01792v1","updated":"2024-10-02T17:50:19Z","published":"2024-10-02T17:50:19Z","title":"When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1","summary":"  In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.\n","authors":["R. Thomas McCoy","Shunyu Yao","Dan Friedman","Mathew D. Hardy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2410.01792v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.01791v1","updated":"2024-10-02T17:49:07Z","published":"2024-10-02T17:49:07Z","title":"DreamGarden: A Designer Assistant for Growing Games from a Single Prompt","summary":"  Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.\n","authors":["Sam Earle","Samyak Parajuli","Andrzej Banburski-Fahey"],"pdf_url":"https://arxiv.org/pdf/2410.01791v1.pdf","comment":"21 pages + appendix, 11 figures"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01784v1","updated":"2024-10-02T17:40:44Z","published":"2024-10-02T17:40:44Z","title":"OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic\n  Foundation Models","summary":"  The advancements in artificial intelligence in recent years, such as Large\nLanguage Models (LLMs), have fueled expectations for breakthroughs in genomic\nfoundation models (GFMs). The code of nature, hidden in diverse genomes since\nthe very beginning of life's evolution, holds immense potential for impacting\nhumans and ecosystems through genome modeling. Recent breakthroughs in GFMs,\nsuch as Evo, have attracted significant investment and attention to genomic\nmodeling, as they address long-standing challenges and transform in-silico\ngenomic studies into automated, reliable, and efficient paradigms. In the\ncontext of this flourishing era of consecutive technological revolutions in\ngenomics, GFM studies face two major challenges: the lack of GFM benchmarking\ntools and the absence of open-source software for diverse genomics. These\nchallenges hinder the rapid evolution of GFMs and their wide application in\ntasks such as understanding and synthesizing genomes, problems that have\npersisted for decades. To address these challenges, we introduce GFMBench, a\nframework dedicated to GFM-oriented benchmarking. GFMBench standardizes\nbenchmark suites and automates benchmarking for a wide range of open-source\nGFMs. It integrates millions of genomic sequences across hundreds of genomic\ntasks from four large-scale benchmarks, democratizing GFMs for a wide range of\nin-silico genomic applications. Additionally, GFMBench is released as\nopen-source software, offering user-friendly interfaces and diverse tutorials,\napplicable for AutoBench and complex tasks like RNA design and structure\nprediction. To facilitate further advancements in genome modeling, we have\nlaunched a public leaderboard showcasing the benchmark performance derived from\nAutoBench. GFMBench represents a step toward standardizing GFM benchmarking and\ndemocratizing GFM applications.\n","authors":["Heng Yang","Jack Cole","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2410.01784v1.pdf","comment":"https://github.com/yangheng95/OmniGenomeBench"},{"id":"http://arxiv.org/abs/2409.02449v2","updated":"2024-10-02T17:40:25Z","published":"2024-09-04T05:08:23Z","title":"What is lost in Normalization? Exploring Pitfalls in Multilingual ASR\n  Model Evaluations","summary":"  This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.\n","authors":["Kavya Manohar","Leena G Pillai"],"pdf_url":"https://arxiv.org/pdf/2409.02449v2.pdf","comment":"Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.00274v2","updated":"2024-10-02T17:34:41Z","published":"2024-09-30T23:02:51Z","title":"Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds","summary":"  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n","authors":["Amina Kobenova","Cyan DeVeaux","Samyak Parajuli","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2410.00274v2.pdf","comment":"27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01772v1","updated":"2024-10-02T17:29:34Z","published":"2024-10-02T17:29:34Z","title":"DeFine: Enhancing LLM Decision-Making with Factor Profiles and\n  Analogical Reasoning","summary":"  LLMs are ideal for decision-making due to their ability to reason over long\ncontexts and identify critical factors. However, challenges arise when\nprocessing transcripts of spoken speech describing complex scenarios. These\ntranscripts often contain ungrammatical or incomplete sentences, repetitions,\nhedging, and vagueness. For example, during a company's earnings call, an\nexecutive might project a positive revenue outlook to reassure investors,\ndespite significant uncertainty regarding future earnings. It is crucial for\nLLMs to incorporate this uncertainty systematically when making decisions. In\nthis paper, we introduce DeFine, a new framework that constructs probabilistic\nfactor profiles from complex scenarios. DeFine then integrates these profiles\nwith analogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in novel situations. Our framework\nseparates the tasks of quantifying uncertainty in complex scenarios and\nincorporating it into LLM decision-making. This approach is particularly useful\nin fields such as medical consultations, negotiations, and political debates,\nwhere making decisions under uncertainty is vital.\n","authors":["Yebowen Hu","Xiaoyang Wang","Wenlin Yao","Yiming Lu","Daoan Zhang","Hassan Foroosh","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01769v1","updated":"2024-10-02T17:25:37Z","published":"2024-10-02T17:25:37Z","title":"Quantifying Generalization Complexity for Large Language Models","summary":"  While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.\n","authors":["Zhenting Qi","Hongyin Luo","Xuliang Huang","Zhuokai Zhao","Yibo Jiang","Xiangjun Fan","Himabindu Lakkaraju","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.01769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01100v2","updated":"2024-10-02T17:09:53Z","published":"2024-07-01T09:06:57Z","title":"Eliminating Position Bias of Language Models: A Mechanistic Approach","summary":"  Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.\n","authors":["Ziqi Wang","Hanlin Zhang","Xiner Li","Kuan-Hao Huang","Chi Han","Shuiwang Ji","Sham M. Kakade","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.01100v2.pdf","comment":"26 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01744v1","updated":"2024-10-02T16:55:01Z","published":"2024-10-02T16:55:01Z","title":"LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose \\OurMethod, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v1.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2402.19085v2","updated":"2024-10-02T16:54:33Z","published":"2024-02-29T12:12:30Z","title":"Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment","summary":"  Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nPareto improvements in multi-objective alignment.\n","authors":["Yiju Guo","Ganqu Cui","Lifan Yuan","Ning Ding","Zexu Sun","Bowen Sun","Huimin Chen","Ruobing Xie","Jie Zhou","Yankai Lin","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19085v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2312.15561v4","updated":"2024-10-02T16:52:30Z","published":"2023-12-24T23:01:00Z","title":"README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP","summary":"  The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.\n","authors":["Zonghai Yao","Nandyala Siddharth Kantu","Guanghao Wei","Hieu Tran","Zhangqi Duan","Sunjae Kwon","Zhichao Yang","README annotation team","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.15561v4.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01736v1","updated":"2024-10-02T16:47:35Z","published":"2024-10-02T16:47:35Z","title":"Recursive Abstractive Processing for Retrieval in Dynamic Datasets","summary":"  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n","authors":["Charbel Chucri","Rami Azouz","Joachim Ott"],"pdf_url":"https://arxiv.org/pdf/2410.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10882v5","updated":"2024-10-02T16:46:54Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking","summary":"  Recent studies have shown that maintaining a consistent response style by\nhuman experts and enhancing data quality in training sets can significantly\nimprove the performance of fine-tuned Large Language Models (LLMs) while\nreducing the number of training examples needed. However, the precise\ndefinition of style and the relationship between style, data quality, and LLM\nperformance remains unclear. This research identifies two key stylistic\nelements in responses: linguistic form and semantic surprisal. We find that,\namong training data of comparable quality, higher consistency in these response\nelements leads to better LLM performance. Inspired by this, we introduce Style\nConsistency-Aware Response Ranking (SCAR), which automatically prioritizes\ninstruction-response pairs in the training set based on their response\nstylistic consistency. By selecting the most style-consistent examples,\nsometimes as few as 0.7% of the full dataset, the fine-tuned LLMs can match or\neven surpass the performance of models trained on the entire dataset in coding\nand open-ended question-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v5.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2410.01735v1","updated":"2024-10-02T16:46:38Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v1.pdf","comment":"20 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2410.01733v1","updated":"2024-10-02T16:46:01Z","published":"2024-10-02T16:46:01Z","title":"Visual Perception in Text Strings","summary":"  Understanding visual semantics embedded in consecutive characters is a\ncrucial capability for both large language models (LLMs) and multi-modal large\nlanguage models (MLLMs). This type of artifact possesses the unique\ncharacteristic that identical information can be readily formulated in both\ntexts and images, making them a significant proxy for analyzing modern LLMs'\nand MLLMs' capabilities in modality-agnostic vision understanding. In this\nwork, we select ASCII art as a representative artifact, where the lines and\nbrightness used to depict each concept are rendered by characters, and we frame\nthe problem as an ASCII art recognition task. We benchmark model performance on\nthis task by constructing an evaluation dataset with an elaborate\ncategorization tree and also collect a training set to elicit the models'\nvisual perception ability. Through a comprehensive analysis of dozens of\nmodels, results reveal that although humans can achieve nearly 100% accuracy,\nthe state-of-the-art LLMs and MLLMs lag far behind. Models are capable of\nrecognizing concepts depicted in the ASCII arts given only text inputs\nindicated by over 60% accuracy for some concepts, but most of them achieves\nmerely around 30% accuracy when averaged across all categories. When provided\nwith images as inputs, GPT-4o gets 82.68%, outperforming the strongest\nopen-source MLLM by 21.95%. Although models favor different kinds of ASCII art\ndepending on the modality provided, none of the MLLMs successfully benefit when\nboth modalities are supplied simultaneously. Moreover, supervised fine-tuning\nhelps improve models' accuracy especially when provided with the image\nmodality, but also highlights the need for better training techniques to\nenhance the information fusion among modalities.\n","authors":["Qi Jia","Xiang Yue","Shanshan Huang","Ziheng Qin","Yizhu Liu","Bill Yuchen Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.01733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01731v1","updated":"2024-10-02T16:43:24Z","published":"2024-10-02T16:43:24Z","title":"ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation","summary":"  The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.\n","authors":["Rinon Gal","Adi Haviv","Yuval Alaluf","Amit H. Bermano","Daniel Cohen-Or","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2410.01731v1.pdf","comment":"Project website: https://comfygen-paper.github.io/"},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01727v1","updated":"2024-10-02T16:37:19Z","published":"2024-10-02T16:37:19Z","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.01727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01724v1","updated":"2024-10-02T16:34:40Z","published":"2024-10-02T16:34:40Z","title":"Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for\n  Enhanced Batch Prompting","summary":"  Batch prompting is a common technique in large language models (LLMs) used to\nprocess multiple inputs simultaneously, aiming to improve computational\nefficiency. However, as batch sizes increase, performance degradation often\noccurs due to the model's difficulty in handling lengthy context inputs.\nExisting methods that attempt to mitigate these issues rely solely on batch\ndata arrangement and majority voting rather than improving the design of the\nbatch prompt itself. In this paper, we address these limitations by proposing\n\"Auto-Demo Prompting,\" a novel approach that leverages the question-output\npairs from earlier questions within a batch as demonstrations for subsequent\nanswer inference. We provide a formal theoretical analysis of how Auto-Demo\nPrompting functions within the autoregressive generation process of LLMs,\nillustrating how it utilizes prior outputs to optimize the model's internal\nrepresentations. Our method effectively bridges the gap between batch prompting\nand few-shot prompting, enhancing performance with only a slight compromise in\ntoken usage. Experimental results across five NLP tasks demonstrate its\neffectiveness in mitigating performance degradation and occasionally\noutperforming single prompts. Furthermore, it opens new avenues for applying\nfew-shot learning techniques, such as demonstration selection, within batch\nprompting, making it a robust solution for real-world applications.\n","authors":["Longyu Feng","Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v2","updated":"2024-10-02T16:30:34Z","published":"2024-01-01T13:53:53Z","title":"LogicAsker: Evaluating and Improving the Logical Reasoning Ability of\n  Large Language Models","summary":"  We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.19653v2","updated":"2024-10-02T16:23:12Z","published":"2024-05-30T03:12:04Z","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","summary":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call \"system captions\" or\nSysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","authors":["Patrick Emami","Zhaonan Li","Saumya Sinha","Truc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.19653v2.pdf","comment":"21 pages. Under review"},{"id":"http://arxiv.org/abs/2410.01708v1","updated":"2024-10-02T16:16:02Z","published":"2024-10-02T16:16:02Z","title":"Examining the Role of Relationship Alignment in Large Language Models","summary":"  The rapid development and deployment of Generative AI in social settings\nraise important questions about how to optimally personalize them for users\nwhile maintaining accuracy and realism. Based on a Facebook public post-comment\ndataset, this study evaluates the ability of Llama 3.0 (70B) to predict the\nsemantic tones across different combinations of a commenter's and poster's\ngender, age, and friendship closeness and to replicate these differences in\nLLM-generated comments.\n  The study consists of two parts: Part I assesses differences in semantic\ntones across social relationship categories, and Part II examines the\nsimilarity between comments generated by Llama 3.0 (70B) and human comments\nfrom Part I given public Facebook posts as input. Part I results show that\nincluding social relationship information improves the ability of a model to\npredict the semantic tone of human comments. However, Part II results show that\neven without including social context information in the prompt, LLM-generated\ncomments and human comments are equally sensitive to social context, suggesting\nthat LLMs can comprehend semantics from the original post alone. When we\ninclude all social relationship information in the prompt, the similarity\nbetween human comments and LLM-generated comments decreases. This inconsistency\nmay occur because LLMs did not include social context information as part of\ntheir training data. Together these results demonstrate the ability of LLMs to\ncomprehend semantics from the original post and respond similarly to human\ncomments, but also highlights their limitations in generalizing personalized\ncomments through prompting alone.\n","authors":["Kristen M. Altenburger","Hongda Jiang","Robert E. Kraut","Yi-Chia Wang","Jane Dwivedi-Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01707v1","updated":"2024-10-02T16:15:31Z","published":"2024-10-02T16:15:31Z","title":"Interpretable Contrastive Monte Carlo Tree Search Reasoning","summary":"  We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*.\n","authors":["Zitian Gao","Boye Niu","Xuzheng He","Haotian Xu","Hongzhang Liu","Aiwei Liu","Xuming Hu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2410.01707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01704v1","updated":"2024-10-02T16:15:04Z","published":"2024-10-02T16:15:04Z","title":"An Exploration of Self-Supervised Mutual Information Alignment for\n  Multi-Task Settings","summary":"  There is a growing need for pluralistic alignment methods that can steer\nlanguage models towards individual attributes and preferences. One such method,\nSelf-Supervised Alignment with Mutual Information (SAMI), uses conditional\nmutual information to encourage the connection between behavioral preferences\nand model responses. We conduct two experiments exploring SAMI in multi-task\nsettings. First, we compare SAMI to Direct Preference Optimization (DPO) on a\nmulti-task benchmark (MT-Bench), using a stronger model to generate training\ndata for a weaker one across diverse categories (humanities, STEM, extraction,\ncoding, math, reasoning, and roleplay). Our results indicate that one iteration\nof SAMI has a 57% win rate against DPO, with significant variation in\nperformance between task categories. Second, we examine SAMI's impact on\nmathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While\nSAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%\nboost. However, SAMI shows interesting scaling trends. When given 10 attempts,\nSAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining\nSAMI with SFT yields an additional improvement of 1.3% in multi-attempt\nsettings, though single-attempt accuracy remains unchanged.\n","authors":["Soham Govande"],"pdf_url":"https://arxiv.org/pdf/2410.01704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09722v2","updated":"2024-10-02T16:14:09Z","published":"2024-07-12T23:29:54Z","title":"Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.\n","authors":["Zongyue Qin","Ziniu Hu","Zifan He","Neha Prakriya","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01696v1","updated":"2024-10-02T16:05:01Z","published":"2024-10-02T16:05:01Z","title":"CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving\n  Long-Range Reasoning Problems using LLMs","summary":"  Large language models (LLMs) have demonstrated limitations in handling\ncombinatorial optimization problems involving long-range reasoning, partially\ndue to causal hallucinations and huge search space. As for causal\nhallucinations, i.e., the inconsistency between reasoning and corresponding\nstate transition, this paper introduces the Causal Relationship Enhancement\n(CRE) mechanism combining cause-effect interventions and the Individual\nTreatment Effect (ITE) to guarantee the solid causal rightness between each\nstep of reasoning and state transition. As for the long causal range and huge\nsearch space limiting the performances of existing models featuring\nsingle-direction search, a Dual-End Searching (DES) approach is proposed to\nseek solutions by simultaneously starting from both the initial and goal states\non the causal probability tree. By integrating CRE and DES (CreDes), our model\nhas realized simultaneous multi-step reasoning, circumventing the\ninefficiencies from cascading multiple one-step reasoning like the\nChain-of-Thought (CoT). Experiments demonstrate that CreDes significantly\noutperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning\ntasks in terms of both accuracy and time efficiency.\n","authors":["Kangsheng Wang","Xiao Zhang","Hao Liu","Songde Han","Huimin Ma","Tianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v1","updated":"2024-10-02T16:03:49Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where performance seems to stagnate at first and then\nimprove sharply and unpredictably with scale beyond a threshold. By dividing\nquestions in the datasets according to difficulty level by average performance,\nwe observe U-shaped scaling for hard questions, and inverted-U scaling followed\nby steady improvement for easy questions. Moreover, the emergence threshold\nroughly coincides with the point at which performance on easy questions reverts\nfrom inverse scaling to standard scaling. Capitalizing on the observable though\nopposing scaling trend on easy and hard questions, we propose a simple yet\neffective pipeline, called Slice-and-Sandwich, to predict both the emergence\nthreshold and model performance beyond the threshold.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.01691v1","updated":"2024-10-02T16:03:13Z","published":"2024-10-02T16:03:13Z","title":"FactAlign: Long-form Factuality Alignment of Large Language Models","summary":"  Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01691v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.03807v2","updated":"2024-10-02T16:00:39Z","published":"2024-06-06T07:30:14Z","title":"Tool-Planner: Task Planning with Clusters across Multiple Tools","summary":"  Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\n\\url{https://github.com/OceannTwT/Tool-Planner}\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yuwei Zhang","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2406.03807v2.pdf","comment":"48pages second version"},{"id":"http://arxiv.org/abs/2410.01679v1","updated":"2024-10-02T15:49:30Z","published":"2024-10-02T15:49:30Z","title":"VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment","summary":"  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n","authors":["Amirhossein Kazemnejad","Milad Aghajohari","Eva Portelance","Alessandro Sordoni","Siva Reddy","Aaron Courville","Nicolas Le Roux"],"pdf_url":"https://arxiv.org/pdf/2410.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19597v2","updated":"2024-10-02T15:47:40Z","published":"2024-04-30T14:43:57Z","title":"TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning","summary":"  The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.\n","authors":["Xuanli He","Jun Wang","Qiongkai Xu","Pasquale Minervini","Pontus Stenetorp","Benjamin I. P. Rubinstein","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2404.19597v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.01675v1","updated":"2024-10-02T15:46:40Z","published":"2024-10-02T15:46:40Z","title":"Trying to be human: Linguistic traces of stochastic empathy in language\n  models","summary":"  Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.\n","authors":["Bennett Kleinberg","Jari Zegers","Jonas Festor","Stefana Vida","Julian Präsent","Riccardo Loconte","Sanne Peereboom"],"pdf_url":"https://arxiv.org/pdf/2410.01675v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.01671v1","updated":"2024-10-02T15:39:55Z","published":"2024-10-02T15:39:55Z","title":"Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yanxin Shen","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2410.01671v1.pdf","comment":"Underreview version of LQCA, Bridge context gap for long context"},{"id":"http://arxiv.org/abs/2410.00907v2","updated":"2024-10-02T15:34:12Z","published":"2024-10-01T17:53:28Z","title":"Addition is All You Need for Energy-efficient Language Models","summary":"  Large neural networks spend most computation on floating point tensor\nmultiplications. In this work, we find that a floating point multiplier can be\napproximated by one integer adder with high precision. We propose the\nlinear-complexity multiplication L-Mul algorithm that approximates floating\npoint number multiplication with integer addition operations. The new algorithm\ncosts significantly less computation resource than 8-bit floating point\nmultiplication but achieves higher precision. Compared to 8-bit floating point\nmultiplications, the proposed method achieves higher precision but consumes\nsignificantly less bit-level computation. Since multiplying floating point\nnumbers requires substantially higher energy compared to integer addition\noperations, applying the L-Mul operation in tensor processing hardware can\npotentially reduce 95% energy cost by element-wise floating point tensor\nmultiplications and 80% energy cost of dot products. We calculated the\ntheoretical error expectation of L-Mul, and evaluated the algorithm on a wide\nrange of textual, visual, and symbolic tasks, including natural language\nunderstanding, structural reasoning, mathematics, and commonsense question\nanswering. Our numerical analysis experiments agree with the theoretical error\nestimation, which indicates that L-Mul with 4-bit mantissa achieves comparable\nprecision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa\noutperforms float8_e5m2. Evaluation results on popular benchmarks show that\ndirectly applying L-Mul to the attention mechanism is almost lossless. We\nfurther show that replacing all floating point multiplications with 3-bit\nmantissa L-Mul in a transformer model achieves equivalent precision as using\nfloat8_e4m3 as accumulation precision in both fine-tuning and inference.\n","authors":["Hongyin Luo","Wei Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04222v4","updated":"2024-10-02T15:27:36Z","published":"2024-02-06T18:29:39Z","title":"What is \"Typological Diversity\" in NLP?","summary":"  The NLP research community has devoted increased attention to languages\nbeyond English, resulting in considerable improvements for multilingual NLP.\nHowever, these improvements only apply to a small subset of the world's\nlanguages. Aiming to extend this, an increasing number of papers aspires to\nenhance generalizable multilingual performance across languages. To this end,\nlinguistic typology is commonly used to motivate language selection, on the\nbasis that a broad typological sample ought to imply generalization across a\nbroad range of languages. These selections are often described as being\n'typologically diverse'. In this work, we systematically investigate NLP\nresearch that includes claims regarding 'typological diversity'. We find there\nare no set definitions or criteria for such claims. We introduce metrics to\napproximate the diversity of language selection along several axes and find\nthat the results vary considerably across papers. Crucially, we show that\nskewed language selection can lead to overestimated multilingual performance.\nWe recommend future work to include an operationalization of 'typological\ndiversity' that empirically justifies the diversity of language samples.\n","authors":["Esther Ploeger","Wessel Poelman","Miryam de Lhoneux","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2402.04222v4.pdf","comment":"EMNLP 2024: Main Conference"},{"id":"http://arxiv.org/abs/2408.00118v3","updated":"2024-10-02T15:22:49Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Léonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ramé","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogozińska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Plucińska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Görner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Cogan","Sarah Perrin","Sébastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01651v1","updated":"2024-10-02T15:18:34Z","published":"2024-10-02T15:18:34Z","title":"Efficient Long-range Language Modeling with Self-supervised Causal\n  Retrieval","summary":"  Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.\n","authors":["Xiang Hu","Zhihao Teng","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.01651v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.14302v2","updated":"2024-10-02T15:17:35Z","published":"2024-09-22T03:13:38Z","title":"Reliable and diverse evaluation of LLM medical knowledge mastery","summary":"  Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.\n","authors":["Yuxuan Zhou","Xien Liu","Chen Ning","Xiao Zhang","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.14302v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.01648v1","updated":"2024-10-02T15:16:02Z","published":"2024-10-02T15:16:02Z","title":"DeIDClinic: A Multi-Layered Framework for De-identification of Clinical\n  Free-text Data","summary":"  De-identification is important in protecting patients' privacy for healthcare\ntext analytics. The MASK framework is one of the best on the de-identification\nshared task organised by n2c2/i2b2 challenges. This work enhances the MASK\nframework by integrating ClinicalBERT, a deep learning model specifically\nfine-tuned on clinical texts, alongside traditional de-identification methods\nlike dictionary lookup and rule-based approaches. The system effectively\nidentifies and either redacts or replaces sensitive identifiable entities\nwithin clinical documents, while also allowing users to customise the masked\ndocuments according to their specific needs. The integration of ClinicalBERT\nsignificantly improves the performance of entity recognition, achieving 0.9732\nF1-score, especially for common entities such as names, dates, and locations.\n  A risk assessment feature has also been developed, which analyses the\nuniqueness of context within documents to classify them into risk levels,\nguiding further de-identification efforts. While the system demonstrates strong\noverall performance, this work highlights areas for future improvement,\nincluding handling more complex entity occurrences and enhancing the system's\nadaptability to different clinical settings.\n","authors":["Angel Paul","Dhivin Shaji","Lifeng Han","Warren Del-Pinto","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2410.01648v1.pdf","comment":"ongoing work"},{"id":"http://arxiv.org/abs/2410.01637v1","updated":"2024-10-02T15:08:12Z","published":"2024-10-02T15:08:12Z","title":"On The Adaptation of Unlimiformer for Decoder-Only Transformers","summary":"  One of the prominent issues stifling the current generation of large language\nmodels is their limited context length. Recent proprietary models such as GPT-4\nand Claude 2 have introduced longer context lengths, 8k/32k and 100k,\nrespectively; however, despite the efforts in the community, most common\nmodels, such as LLama-2, have a context length of 4k or less. Unlimiformer\n(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation\nmethod that offloads cross-attention computations to a kNN index. However, its\nmain limitation is incompatibility with decoder-only transformers out of the\nbox. In this work, we explore practical considerations of adapting Unlimiformer\nto decoder-only transformers and introduce a series of modifications to\novercome this limitation. Moreover, we expand the original experimental setup\non summarization to include a new task (i.e., free-form Q&A) and an\ninstruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase\nthe effectiveness of these modifications on summarization, performing on par\nwith a model with 2x the context length. Moreover, we discuss limitations and\nfuture directions for free-form Q&A and instruction-tuned models.\n","authors":["Kian Ahrabian","Alon Benhaim","Barun Patra","Jay Pujara","Saksham Singhal","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2410.01637v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.04701v2","updated":"2024-10-02T15:07:09Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.\n","authors":["Michael Günther","Isabelle Mohr","Daniel James Williams","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v2.pdf","comment":"11 pages, 3rd draft"},{"id":"http://arxiv.org/abs/2309.12934v3","updated":"2024-10-02T15:04:59Z","published":"2023-09-22T15:32:49Z","title":"TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with\n  Diverse Writing Styles","summary":"  Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as deepfake texts.\nThere are currently over 72K text generation models in the huggingface model\nrepo. As such, users with malicious intent can easily use these open-sourced\nLLMs to generate harmful texts and dis/misinformation at scale. To mitigate\nthis problem, a computational method to determine if a given text is a deepfake\ntext or not is desired--i.e., Turing Test (TT). In particular, in this work, we\ninvestigate the more general version of the problem, known as Authorship\nAttribution (AA), in a multi-class setting--i.e., not only determining if a\ngiven text is a deepfake text or not but also being able to pinpoint which LLM\nis the author. We propose TopFormer to improve existing AA solutions by\ncapturing more linguistic patterns in deepfake texts by including a Topological\nData Analysis (TDA) layer in the Transformer-based model. We show the benefits\nof having a TDA layer when dealing with imbalanced, and multi-style datasets,\nby extracting TDA features from the reshaped $pooled\\_output$ of our backbone\nas input. This Transformer-based model captures contextual representations\n(i.e., semantic and syntactic linguistic features), while TDA captures the\nshape and structure of data (i.e., linguistic structures). Finally, TopFormer,\noutperforms all baselines in all 3 datasets, achieving up to 7\\% increase in\nMacro F1 score. Our code and datasets are available at:\nhttps://github.com/AdaUchendu/topformer\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2309.12934v3.pdf","comment":"Accepted at The 27th European Conference on Artificial Intelligence\n  (ECAI 2024)"},{"id":"http://arxiv.org/abs/2410.01633v1","updated":"2024-10-02T15:04:21Z","published":"2024-10-02T15:04:21Z","title":"A Thematic Framework for Analyzing Large-scale Self-reported Social\n  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product","summary":"  Background: One of the key FDA-approved medications for Opioid Use Disorder\n(OUD) is buprenorphine. Despite its popularity, individuals often report\nvarious information needs regarding buprenorphine treatment on social media\nplatforms like Reddit. However, the key challenge is to characterize these\nneeds. In this study, we propose a theme-based framework to curate and analyze\nlarge-scale data from social media to characterize self-reported treatment\ninformation needs (TINs).\n  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit\nsub-community for buprenorphine products. Following the standard protocol, we\nfirst identified and defined five main themes from the data and then coded\n6,000 posts based on these themes, where one post can be labeled with\napplicable one to three themes. Finally, we determined the most frequently\nappearing sub-themes (topics) for each theme by analyzing samples from each\ngroup.\n  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two\nthemes, and 13.9% three themes. The most frequent topics for each theme or\ntheme combination came with several key findings - prevalent reporting of\npsychological and physical effects during recovery, complexities in accessing\nbuprenorphine, and significant information gaps regarding medication\nadministration, tapering, and usage of substances during different stages of\nrecovery. Moreover, self-treatment strategies and peer-driven advice reveal\nvaluable insights and potential misconceptions.\n  Conclusions: The findings obtained using our proposed framework can inform\nbetter patient education and patient-provider communication, design systematic\ninterventions to address treatment-related misconceptions and rumors, and\nstreamline the generation of hypotheses for future research.\n","authors":["Madhusudan Basak","Omar Sharif","Sarah E. Lord","Jacob T. Borodovsky","Lisa A. Marsch","Sandra A. Springer","Edward Nunes","Charlie D. Brackett","Luke J. ArchiBald","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2410.01633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01627v1","updated":"2024-10-02T15:01:55Z","published":"2024-10-02T15:01:55Z","title":"Intent Detection in the Age of LLMs","summary":"  Intent detection is a critical component of task-oriented dialogue systems\n(TODS) which enables the identification of suitable actions to address user\nutterances at each dialog turn. Traditional approaches relied on\ncomputationally efficient supervised sentence transformer encoder models, which\nrequire substantial training data and struggle with out-of-scope (OOS)\ndetection. The emergence of generative large language models (LLMs) with\nintrinsic world knowledge presents new opportunities to address these\nchallenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context\nlearning and chain-of-thought prompting for intent detection, and compare their\nperformance with contrastively fine-tuned sentence transformer (SetFit) models\nto highlight prediction quality and latency tradeoff. We propose a hybrid\nsystem using uncertainty based routing strategy to combine the two approaches\nthat along with negative data augmentation results in achieving the best of\nboth worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To\nbetter understand LLM OOS detection capabilities, we perform controlled\nexperiments revealing that this capability is significantly influenced by the\nscope of intent labels and the size of the label space. We also introduce a\ntwo-step approach utilizing internal LLM representations, demonstrating\nempirical gains in OOS detection accuracy and F1-score by >5% for the\nMistral-7B model.\n","authors":["Gaurav Arora","Shreya Jain","Srujana Merugu"],"pdf_url":"https://arxiv.org/pdf/2410.01627v1.pdf","comment":"Accepted at EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01610v1","updated":"2024-10-02T14:48:22Z","published":"2024-10-02T14:48:22Z","title":"Upcycling Instruction Tuning from Dense to Mixture-of-Experts via\n  Parameter Merging","summary":"  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.\n","authors":["Tingfeng Hui","Zhenyu Zhang","Shuohuan Wang","Yu Sun","Hua Wu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2410.01610v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2405.16869v2","updated":"2024-10-02T14:42:10Z","published":"2024-05-27T06:36:17Z","title":"Multiple Heads are Better than One: Mixture of Modality Knowledge\n  Experts for Entity Representation Learning","summary":"  Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Binbin Hu","Ziqi Liu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.16869v2.pdf","comment":"Work in progress. Code and data will be released at\n  https://github.com/zjukg/MoMoK"},{"id":"http://arxiv.org/abs/2410.01600v1","updated":"2024-10-02T14:39:13Z","published":"2024-10-02T14:39:13Z","title":"ENTP: Encoder-only Next Token Prediction","summary":"  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n","authors":["Ethan Ewer","Daewon Chae","Thomas Zeng","Jinkyu Kim","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15452v2","updated":"2024-10-02T14:35:40Z","published":"2024-09-23T18:27:03Z","title":"CUTE: Measuring LLMs' Understanding of Their Tokens","summary":"  Large Language Models (LLMs) show remarkable performance on a wide variety of\ntasks. Most LLMs split text into multi-character tokens and process them as\natomic units without direct access to individual characters. This raises the\nquestion: To what extent can LLMs learn orthographic information? To answer\nthis, we propose a new benchmark, CUTE, which features a collection of tasks\ndesigned to test the orthographic knowledge of LLMs. We evaluate popular LLMs\non CUTE, finding that most of them seem to know the spelling of their tokens,\nyet fail to use this information effectively to manipulate text, calling into\nquestion how much of this knowledge is generalizable.\n","authors":["Lukas Edman","Helmut Schmid","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2409.15452v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2409.13385v2","updated":"2024-10-02T14:30:28Z","published":"2024-09-20T10:36:49Z","title":"Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.\n","authors":["Sourav Verma"],"pdf_url":"https://arxiv.org/pdf/2409.13385v2.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2312.11062v2","updated":"2024-10-02T14:26:30Z","published":"2023-12-18T09:58:19Z","title":"Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction","summary":"  Relation extraction is essentially a text classification problem, which can\nbe tackled by fine-tuning a pre-trained language model (LM). However, a key\nchallenge arises from the fact that relation extraction cannot\nstraightforwardly be reduced to sequence or token classification. Existing\napproaches therefore solve the problem in an indirect way: they fine-tune an LM\nto learn embeddings of the head and tail entities, and then predict the\nrelationship from these entity embeddings. Our hypothesis in this paper is that\nrelation extraction models can be improved by capturing relationships in a more\ndirect way. In particular, we experiment with appending a prompt with a [MASK]\ntoken, whose contextualised representation is treated as a relation embedding.\nWhile, on its own, this strategy significantly underperforms the aforementioned\napproach, we find that the resulting relation embeddings are highly\ncomplementary to what is captured by embeddings of the head and tail entity. By\njointly considering both types of representations, we end up with a simple\nmodel that outperforms the state-of-the-art across several relation extraction\nbenchmarks.\n","authors":["Frank Mtumbuka","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2312.11062v2.pdf","comment":"Accepted in the Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.10421v3","updated":"2024-10-02T14:20:50Z","published":"2024-06-14T21:52:21Z","title":"SciEx: Benchmarking Large Language Models on Scientific Exams with Human\n  Expert Grading and Automatic Grading","summary":"  With the rapid development of Large Language Models (LLMs), it is crucial to\nhave benchmarks which can evaluate the ability of LLMs on different domains.\nOne common use of LLMs is performing tasks on scientific topics, such as\nwriting algorithms, querying databases or giving mathematical proofs. Inspired\nby the way university students are evaluated on such tasks, in this paper, we\npropose SciEx - a benchmark consisting of university computer science exam\nquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)\nmultilingual, containing both English and German exams, and (2) multi-modal,\ncontaining questions that involve images, and (3) contains various types of\nfreeform questions with different difficulty levels, due to the nature of\nuniversity exams. We evaluate the performance of various state-of-the-art LLMs\non our new benchmark. Since SciEx questions are freeform, it is not\nstraightforward to evaluate LLM performance. Therefore, we provide human expert\ngrading of the LLM outputs on SciEx. We show that the free-form exams in SciEx\nremain challenging for the current LLMs, where the best LLM only achieves\n59.4\\% exam grade on average. We also provide detailed comparisons between LLM\nperformance and student performance on SciEx. To enable future evaluation of\nnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.\nOur experiments show that, although they do not perform perfectly on solving\nthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with\nexpert grading.\n","authors":["Tu Anh Dinh","Carlos Mullov","Leonard Bärmann","Zhaolin Li","Danni Liu","Simon Reiß","Jueun Lee","Nathan Lerzer","Fabian Ternava","Jianfeng Gao","Tobias Röddiger","Alexander Waibel","Tamim Asfour","Michael Beigl","Rainer Stiefelhagen","Carsten Dachsbacher","Klemens Böhm","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2406.10421v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.11176v3","updated":"2024-10-02T14:20:29Z","published":"2024-02-17T02:54:32Z","title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models","summary":"  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n","authors":["Yougang Lyu","Lingyong Yan","Shuaiqiang Wang","Haibo Shi","Dawei Yin","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2402.11176v3.pdf","comment":"EMNLP 2024 main paper"},{"id":"http://arxiv.org/abs/2410.01579v1","updated":"2024-10-02T14:15:13Z","published":"2024-10-02T14:15:13Z","title":"Spoken Grammar Assessment Using LLM","summary":"  Spoken language assessment (SLA) systems restrict themselves to evaluating\nthe pronunciation and oral fluency of a speaker by analysing the read and\nspontaneous spoken utterances respectively. The assessment of language grammar\nor vocabulary is relegated to written language assessment (WLA) systems. Most\nWLA systems present a set of sentences from a curated finite-size database of\nsentences thereby making it possible to anticipate the test questions and train\noneself. In this paper, we propose a novel end-to-end SLA system to assess\nlanguage grammar from spoken utterances thus making WLA systems redundant;\nadditionally, we make the assessment largely unteachable by employing a large\nlanguage model (LLM) to bring in variations in the test. We further demonstrate\nthat a hybrid automatic speech recognition (ASR) with a custom-built language\nmodel outperforms the state-of-the-art ASR engine for spoken grammar\nassessment.\n","authors":["Sunil Kumar Kopparapu","Chitralekha Bhat","Ashish Panda"],"pdf_url":"https://arxiv.org/pdf/2410.01579v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v2","updated":"2024-10-02T13:59:40Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01556v1","updated":"2024-10-02T13:52:55Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01555v1","updated":"2024-10-02T13:52:09Z","published":"2024-10-02T13:52:09Z","title":"ACE: A LLM-based Negotiation Coaching System","summary":"  The growing prominence of LLMs has led to an increase in the development of\nAI tutoring systems. These systems are crucial in providing underrepresented\npopulations with improved access to valuable education. One important area of\neducation that is unavailable to many learners is strategic bargaining related\nto negotiation. To address this, we develop a LLM-based Assistant for Coaching\nnEgotiation (ACE). ACE not only serves as a negotiation partner for users but\nalso provides them with targeted feedback for improvement. To build our system,\nwe collect a dataset of negotiation transcripts between MBA students. These\ntranscripts come from trained negotiators and emulate realistic bargaining\nscenarios. We use the dataset, along with expert consultations, to design an\nannotation scheme for detecting negotiation mistakes. ACE employs this scheme\nto identify mistakes and provide targeted feedback to users. To test the\neffectiveness of ACE-generated feedback, we conducted a user experiment with\ntwo consecutive trials of negotiation and found that it improves negotiation\nperformances significantly compared to a system that doesn't provide feedback\nand one which uses an alternative method of providing feedback.\n","authors":["Ryan Shea","Aymen Kallala","Xin Lucy Liu","Michael W. Morris","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01555v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01553v1","updated":"2024-10-02T13:47:17Z","published":"2024-10-02T13:47:17Z","title":"MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an\n  AI-SCE Framework","summary":"  Artificial intelligence (AI) and large language models (LLMs) in healthcare\nrequire advanced clinical skills (CS), yet current benchmarks fail to evaluate\nthese comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by\nmedical education's Objective Structured Clinical Examinations (OSCEs), to\naddress this gap. MedQA-CS evaluates LLMs through two instruction-following\ntasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real\nclinical scenarios. Our contributions include developing MedQA-CS, a\ncomprehensive evaluation framework with publicly available data and expert\nannotations, and providing the quantitative and qualitative assessment of LLMs\nas reliable judges in CS evaluation. Our experiments show that MedQA-CS is a\nmore challenging benchmark for evaluating clinical skills than traditional\nmultiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,\nMedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities\nfor both open- and closed-source LLMs.\n","authors":["Zonghai Yao","Zihao Zhang","Chaolong Tang","Xingyu Bian","Youxia Zhao","Zhichao Yang","Junda Wang","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2406.04886v2","updated":"2024-10-02T13:40:10Z","published":"2024-06-07T12:32:44Z","title":"Unveiling the Invisible: Captioning Videos with Metaphors","summary":"  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Sumit Shekhar"],"pdf_url":"https://arxiv.org/pdf/2406.04886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01548v1","updated":"2024-10-02T13:37:54Z","published":"2024-10-02T13:37:54Z","title":"In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks","summary":"  In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.\n","authors":["Dingzirui Wang","Xuangliang Zhang","Qiguang Chen","Longxu Dou","Xiao Xu","Rongyu Cao","Yingwei Ma","Qingfu Zhu","Wanxiang Che","Binhua Li","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01532v1","updated":"2024-10-02T13:24:56Z","published":"2024-10-02T13:24:56Z","title":"Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for\n  Large Language Models","summary":"  Advancements in Natural Language Processing (NLP), have led to the emergence\nof Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which\nexcel across a range of tasks but require extensive fine-tuning to align their\noutputs with human expectations. A widely used method for achieving this\nalignment is Reinforcement Learning from Human Feedback (RLHF), which, despite\nits success, faces challenges in accurately modelling human preferences. In\nthis paper, we introduce GazeReward, a novel framework that integrates implicit\nfeedback -- and specifically eye-tracking (ET) data -- into the Reward Model\n(RM). In addition, we explore how ET-based features can provide insights into\nuser preferences. Through ablation studies we test our framework with different\nintegration methods, LLMs, and ET generator models, demonstrating that our\napproach significantly improves the accuracy of the RM on established human\npreference datasets. This work advances the ongoing discussion on optimizing AI\nalignment with human values, exploring the potential of cognitive data for\nshaping future NLP research.\n","authors":["Angela Lopez-Cardona","Carlos Segura","Alexandros Karatzoglou","Sergi Abadal","Ioannis Arapakis"],"pdf_url":"https://arxiv.org/pdf/2410.01532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01524v1","updated":"2024-10-02T13:12:13Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01518v1","updated":"2024-10-02T13:09:41Z","published":"2024-10-02T13:09:41Z","title":"InfiniPot: Infinite Context Processing on Memory-Constrained LLMs","summary":"  Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.\n","authors":["Minsoo Kim","Kyuhong Shim","Jungwook Choi","Simyung Chang"],"pdf_url":"https://arxiv.org/pdf/2410.01518v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.07791v5","updated":"2024-10-02T13:08:43Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs","summary":"  LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.\n","authors":["Lin Shi","Chiyu Ma","Wenhua Liang","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01512v1","updated":"2024-10-02T13:02:23Z","published":"2024-10-02T13:02:23Z","title":"InstaTrans: An Instruction-Aware Translation Framework for Non-English\n  Instruction Datasets","summary":"  It is challenging to generate high-quality instruction datasets for\nnon-English languages due to tail phenomena, which limit performance on less\nfrequently observed data. To mitigate this issue, we propose translating\nexisting high-quality English instruction datasets as a solution, emphasizing\nthe need for complete and instruction-aware translations to maintain the\ninherent attributes of these datasets. We claim that fine-tuning LLMs with\ndatasets translated in this way can improve their performance in the target\nlanguage. To this end, we introduces a new translation framework tailored for\ninstruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through\nextensive experiments, we demonstrate the superiority of InstaTrans over other\ncompetitors in terms of completeness and instruction-awareness of translation,\nhighlighting its potential to broaden the accessibility of LLMs across diverse\nlanguages at a relatively low cost. Furthermore, we have validated that\nfine-tuning LLMs with datasets translated by InstaTrans can effectively improve\ntheir performance in the target language.\n","authors":["Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.01512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01508v1","updated":"2024-10-02T13:00:21Z","published":"2024-10-02T13:00:21Z","title":"Disentangling Latent Shifts of In-Context Learning Through Self-Training","summary":"  In-context learning (ICL) has become essential in natural language\nprocessing, particularly with autoregressive large language models capable of\nlearning from demonstrations provided within the prompt. However, ICL faces\nchallenges with stability and long contexts, especially as the number of\ndemonstrations grows, leading to poor generalization and inefficient inference.\nTo address these issues, we introduce STICL (Self-Training ICL), an approach\nthat disentangles the latent shifts of demonstrations from the latent shift of\nthe query through self-training. STICL employs a teacher model to generate\npseudo-labels and trains a student model using these labels, encoded in an\nadapter module. The student model exhibits weak-to-strong generalization,\nprogressively refining its predictions over time. Our empirical results show\nthat STICL improves generalization and stability, consistently outperforming\ntraditional ICL methods and other disentangling strategies across both\nin-domain and out-of-domain data.\n","authors":["Josip Jukić","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2410.01508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01504v1","updated":"2024-10-02T12:57:12Z","published":"2024-10-02T12:57:12Z","title":"PersonaMath: Enhancing Math Reasoning through Persona-Driven Data\n  Augmentation","summary":"  While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models continue to struggle\nwith such tasks. To bridge this gap, we propose a data augmentation approach\nand introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we\ntrain the PersonaMath models. Our approach consists of two stages: the first\nstage is learning from Persona Diversification, and the second stage is\nlearning from Reflection. In the first stage, we regenerate detailed\nchain-of-thought (CoT) solutions as instructions using a closed-source LLM and\nintroduce a novel persona-driven data augmentation technique to enhance the\ndataset's quantity and diversity. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on\nMATH and 68.7% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 70.3K data\npoints-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.\n","authors":["Jing Luo","Run Luo","Longze Chen","Liang Zhu","Chang Ao","Jiaming Li","Yukun Chen","Xin Cheng","Wen Yang","Jiayuan Su","Chengming Li","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2410.01504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v3","updated":"2024-10-02T12:47:50Z","published":"2024-07-25T00:27:07Z","title":"S2-Attention: Hardware-Aware Context Sharding Among Attention Heads","summary":"  Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Liliang Ren","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.01490v1","updated":"2024-10-02T12:40:11Z","published":"2024-10-02T12:40:11Z","title":"Extending Context Window of Large Language Models from a Distributional\n  Perspective","summary":"  Scaling the rotary position embedding (RoPE) has become a common method for\nextending the context window of RoPE-based large language models (LLMs).\nHowever, existing scaling methods often rely on empirical approaches and lack a\nprofound understanding of the internal distribution within RoPE, resulting in\nsuboptimal performance in extending the context window length. In this paper,\nwe propose to optimize the context window extending task from the view of\nrotary angle distribution. Specifically, we first estimate the distribution of\nthe rotary angles within the model and analyze the extent to which length\nextension perturbs this distribution. Then, we present a novel extension\nstrategy that minimizes the disturbance between rotary angle distributions to\nmaintain consistency with the pre-training phase, enhancing the model's\ncapability to generalize to longer sequences. Experimental results compared to\nthe strong baseline methods demonstrate that our approach reduces by up to 72%\nof the distributional disturbance when extending LLaMA2's context window to 8k,\nand reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,\nour method achieves an average improvement of up to 4.33% over existing\nstate-of-the-art methods. Furthermore, Our method maintains the model's\nperformance on the Hugging Face Open LLM benchmark after context window\nextension, with only an average performance fluctuation ranging from -0.12 to\n+0.22.\n","authors":["Yingsheng Wu. Yuxuan Gu","Xiaocheng Feng","Weihong Zhong","Dongliang Xu","Qing Yang","Hongtao Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.01490v1.pdf","comment":"14 pages, 8 figures, Accepted to EMNLP2024"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01487v1","updated":"2024-10-02T12:36:08Z","published":"2024-10-02T12:36:08Z","title":"Small Language Models Like Small Vocabularies: Probing the Linguistic\n  Abilities of Grapheme- and Phoneme-Based Baby Llamas","summary":"  Current language models use subword-based tokenization algorithms like Byte\nPair Encoding, which put their validity as models of linguistic representations\ninto question. In this paper, we explore the potential of tokenization-free,\nphoneme- and grapheme-based language models. We demonstrate that small models\nbased on the Llama architecture can achieve strong linguistic performance on\nstandard syntactic and novel lexical/phonetic benchmarks when trained with\ncharacter-level vocabularies. We further show that phoneme-based models without\nany graphemic biases almost match grapheme-based models in standard tasks and\nnovel evaluations. Our findings suggest a promising direction for creating more\nlinguistically plausible language models that are better suited for\ncomputational studies of language acquisition and processing.\n","authors":["Bastian Bunzeck","Daniel Duran","Leonie Schade","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2410.01487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01485v1","updated":"2024-10-02T12:35:53Z","published":"2024-10-02T12:35:53Z","title":"A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts","summary":"  Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.\n","authors":["Suyu Ge","Xihui Lin","Yunan Zhang","Jiawei Han","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18120v3","updated":"2024-10-02T12:34:25Z","published":"2024-02-28T07:18:39Z","title":"Exploring Multilingual Concepts of Human Value in Large Language Models:\n  Is Value Alignment Consistent, Transferable and Controllable across\n  Languages?","summary":"  Prior research has revealed that certain abstract concepts are linearly\nrepresented as directions in the representation space of LLMs, predominantly\ncentered around English. In this paper, we extend this investigation to a\nmultilingual context, with a specific focus on human values-related concepts\n(i.e., value concepts) due to their significance for AI safety. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality (e.g., monolingual, bilingual and\nmultilingual), we first empirically confirm the presence of value concepts\nwithin LLMs in a multilingual format. Further analysis on the cross-lingual\ncharacteristics of these concepts reveals 3 traits arising from language\nresource disparities: cross-lingual inconsistency, distorted linguistic\nrelationships, and unidirectional cross-lingual transfer between high- and\nlow-resource languages, all in terms of value concepts. Moreover, we validate\nthe feasibility of cross-lingual control over value alignment capabilities of\nLLMs, leveraging the dominant language as a source language. Ultimately,\nrecognizing the significant impact of LLMs' multilinguality on our results, we\nconsolidate our findings and provide prudent suggestions on the composition of\nmultilingual data for LLMs pre-training.\n","authors":["Shaoyang Xu","Weilong Dong","Zishan Guo","Xinwei Wu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.18120v3.pdf","comment":"EMNLP 2024 findings, code&dataset:\n  https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts"},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06670v2","updated":"2024-10-02T12:26:54Z","published":"2024-04-10T01:14:12Z","title":"What's Mine becomes Yours: Defining, Annotating and Detecting\n  Context-Dependent Paraphrases in News Interview Dialogs","summary":"  Best practices for high conflict conversations like counseling or customer\nsupport almost always include recommendations to paraphrase the previous\nspeaker. Although paraphrase classification has received widespread attention\nin NLP, paraphrases are usually considered independent from context, and common\nmodels and datasets are not applicable to dialog settings. In this work, we\ninvestigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\"\nbecomes Speaker 2: \"That book is yours.\"). We provide an operationalization of\ncontext-dependent paraphrases, and develop a training for crowd-workers to\nclassify paraphrases in dialog. We introduce a dataset with utterance pairs\nfrom NPR and CNN news interviews annotated for context-dependent paraphrases.\nTo enable analyses on label variation, the dataset contains 5,581 annotations\non 600 utterance pairs. We present promising results with in-context learning\nand with token classification models for automatic paraphrase detection in\ndialog.\n","authors":["Anna Wegmann","Tijs van den Broek","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.06670v2.pdf","comment":"Accepted as main conference paper to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01450v1","updated":"2024-10-02T12:01:32Z","published":"2024-10-02T12:01:32Z","title":"Agent-Driven Large Language Models for Mandarin Lyric Generation","summary":"  Generative Large Language Models have shown impressive in-context learning\nabilities, performing well across various tasks with just a prompt. Previous\nmelody-to-lyric research has been limited by scarce high-quality aligned data\nand unclear standard for creativeness. Most efforts focused on general themes\nor emotions, which are less valuable given current language model capabilities.\nIn tonal contour languages like Mandarin, pitch contours are influenced by both\nmelody and tone, leading to variations in lyric-melody fit. Our study,\nvalidated by the Mpop600 dataset, confirms that lyricists and melody writers\nconsider this fit during their composition process. In this research, we\ndeveloped a multi-agent system that decomposes the melody-to-lyric task into\nsub-tasks, with each agent controlling rhyme, syllable count, lyric-melody\nalignment, and consistency. Listening tests were conducted via a\ndiffusion-based singing voice synthesizer to evaluate the quality of lyrics\ngenerated by different agent groups.\n","authors":["Hong-Hsiang Liu","Yi-Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01450v1.pdf","comment":"6 pages, figures, Accepted at O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2404.03887v4","updated":"2024-10-02T11:56:35Z","published":"2024-04-05T04:25:47Z","title":"SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models","summary":"  This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.\n","authors":["Hyeonwoo Kim","Gyoungjin Gim","Yungi Kim","Jihoo Kim","Byungju Kim","Wonseok Lee","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.03887v4.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.13443v2","updated":"2024-10-02T11:46:10Z","published":"2024-06-19T11:08:56Z","title":"Dual-Phase Accelerated Prompt Optimization","summary":"  Gradient-free prompt optimization methods have made significant strides in\nenhancing the performance of closed-source Large Language Models (LLMs) across\na wide range of tasks. However, existing approaches make light of the\nimportance of high-quality prompt initialization and the identification of\neffective optimization directions, thus resulting in substantial optimization\nsteps to obtain satisfactory performance. In this light, we aim to accelerate\nprompt optimization process to tackle the challenge of low convergence rate. We\npropose a dual-phase approach which starts with generating high-quality initial\nprompts by adopting a well-designed meta-instruction to delve into\ntask-specific information, and iteratively optimize the prompts at the sentence\nlevel, leveraging previous tuning experience to expand prompt candidates and\naccept effective ones. Extensive experiments on eight datasets demonstrate the\neffectiveness of our proposed method, achieving a consistent accuracy gain over\nbaselines with less than five optimization steps.\n","authors":["Muchen Yang","Moxin Li","Yongle Li","Zijun Chen","Chongming Gao","Junqi Zhang","Yangyang Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.13443v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.09549v2","updated":"2024-10-02T11:44:26Z","published":"2024-06-13T19:30:32Z","title":"Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective","summary":"  Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10341v2","updated":"2024-10-02T11:38:51Z","published":"2024-09-16T14:56:59Z","title":"Detecting Sexism in German Online Newspaper Comments with Open-Source\n  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks\n  1 and 2, Closed Track)","summary":"  Sexism in online media comments is a pervasive challenge that often manifests\nsubtly, complicating moderation efforts as interpretations of what constitutes\nsexism can vary among individuals. We study monolingual and multilingual\nopen-source text embeddings to reliably detect sexism and misogyny in\nGerman-language online comments from an Austrian newspaper. We observed\nclassifiers trained on text embeddings to mimic closely the individual\njudgements of human annotators. Our method showed robust performance in the\nGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1\nscore of 0.597 (4th place, as reported on Codabench). It also accurately\npredicted the distribution of human annotations in GerMS-Detect Subtask 2, with\nan average Jensen-Shannon distance of 0.301 (2nd place). The computational\nefficiency of our approach suggests potential for scalable applications across\nvarious languages and linguistic contexts.\n","authors":["Florian Bremm","Patrick Gustav Blaneck","Tobias Bornheim","Niklas Grieger","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2409.10341v2.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.01434v1","updated":"2024-10-02T11:36:45Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions via\nsubnetworks that can be composed to perform more complex tasks. Recent\ndevelopments in mechanistic interpretability have made progress in identifying\nsubnetworks, often referred to as circuits, which represent the minimal\ncomputational subgraph responsible for a model's behavior on specific tasks.\nHowever, most studies focus on identifying circuits for individual tasks\nwithout investigating how functionally similar circuits relate to each other.\nTo address this gap, we examine the modularity of neural networks by analyzing\ncircuits for highly compositional subtasks within a transformer-based language\nmodel. Specifically, given a probabilistic context-free grammar, we identify\nand compare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through subnetwork set operations to\nrepresent more complex functional capabilities of the model.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v1.pdf","comment":"24 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02076v5","updated":"2024-10-02T11:29:18Z","published":"2024-09-03T17:25:54Z","title":"LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs","summary":"  In evaluating the long-context capabilities of large language models (LLMs),\nbenchmarks such as \"Needle-in-a-Haystack\" (NIAH), Ruler, and Needlebench are\ncommonly used. While these benchmarks measure how well models understand\nlong-context input sequences, they do not effectively gauge the quality of\nlong-form text generation--a critical aspect for applications such as design\nproposals and creative writing. To address this gap, we have introduced a new\nlong-form text evaluation benchmark, LongGenBench, which tests models' ability\nto identify specific events within generated long text sequences. In this\nbenchmark, we prompt long-context LMs to create long-form text that must\ninclude particular events or constraints and evaluate their ability to\nincorporate these elements. We evaluated ten long-context LMs across four\ndistinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenBench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.\n","authors":["Yuhao Wu","Ming Shan Hee","Zhiqing Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2409.02076v5.pdf","comment":"work in progress; Github: https://github.com/mozhu621/LongGenBench/"},{"id":"http://arxiv.org/abs/2410.01428v1","updated":"2024-10-02T11:26:02Z","published":"2024-10-02T11:26:02Z","title":"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with\n  Retrieval-Augmentation for Solving Challenging Tasks","summary":"  State-of-the-art large language models (LLMs) exhibit impressive\nproblem-solving capabilities but may struggle with complex reasoning and\nfactual correctness. Existing methods harness the strengths of chain-of-thought\nand retrieval-augmented generation (RAG) to decompose a complex problem into\nsimpler steps and apply retrieval to improve factual correctness. These methods\nwork well on straightforward reasoning tasks but often falter on challenging\ntasks such as competitive programming and mathematics, due to frequent\nreasoning errors and irrelevant knowledge retrieval. To address this, we\nintroduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a\nnovel framework that leverages fine-tuned critic models to guide both reasoning\nand retrieval processes through planning. CR-Planner solves a problem by\niteratively selecting and executing sub-goals. Initially, it identifies the\nmost promising sub-goal from reasoning, query generation, and retrieval, guided\nby rewards given by a critic model named sub-goal critic. It then executes this\nsub-goal through sampling and selecting the optimal output based on evaluations\nfrom another critic model named execution critic. This iterative process,\ninformed by retrieved information and critic models, enables CR-Planner to\neffectively navigate the solution space towards the final answer. We employ\nMonte Carlo Tree Search to collect the data for training the critic models,\nallowing for a systematic exploration of action sequences and their long-term\nimpacts. We validate CR-Planner on challenging domain-knowledge-intensive and\nreasoning-heavy tasks, including competitive programming, theorem-driven math\nreasoning, and complex domain retrieval problems. Our experiments demonstrate\nthat CR-Planner significantly outperforms baselines, highlighting its\neffectiveness in addressing challenging problems by improving both reasoning\nand retrieval.\n","authors":["Xingxuan Li","Weiwen Xu","Ruochen Zhao","Fangkai Jiao","Shafiq Joty","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.01428v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.18618v3","updated":"2024-10-02T11:08:29Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.11239v2","updated":"2024-10-02T11:07:14Z","published":"2024-09-17T14:40:02Z","title":"LLM-as-a-Judge & Reward Model: What They Can and Cannot Do","summary":"  LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\nin diverse contexts, such as non-English prompts, factual verification, or\nchallenging questions, remains unexplored. In this paper, we conduct a\ncomprehensive analysis of automated evaluators, reporting several key findings\non their behavior. First, we discover that English evaluation capabilities\nsignificantly influence language-specific evaluation capabilities, often more\nthan the language proficiency itself, enabling evaluators trained in English to\neasily transfer their skills to other languages. Second, we identify critical\nshortcomings, where LLMs fail to detect and penalize errors, such as factual\ninaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we find that state-of-the-art evaluators struggle with\nchallenging prompts, in either English or Korean, underscoring their\nlimitations in assessing or generating complex reasoning questions. We release\nthe dataset and codes used.\n","authors":["Guijin Son","Hyunwoo Ko","Hoyoung Lee","Yewon Kim","Seunghyeok Hong"],"pdf_url":"https://arxiv.org/pdf/2409.11239v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2406.14760v2","updated":"2024-10-02T11:03:16Z","published":"2024-06-20T22:10:52Z","title":"An LLM Feature-based Framework for Dialogue Constructiveness Assessment","summary":"  Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.\n","authors":["Lexin Zhou","Youmna Farag","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.14760v2.pdf","comment":"Paper accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01417v1","updated":"2024-10-02T10:58:54Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09177v2","updated":"2024-10-02T10:43:07Z","published":"2024-02-14T13:45:19Z","title":"Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks","summary":"  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.\n","authors":["Yixin Cheng","Markos Georgopoulos","Volkan Cevher","Grigorios G. Chrysos"],"pdf_url":"https://arxiv.org/pdf/2402.09177v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2409.17171v2","updated":"2024-10-02T10:28:02Z","published":"2024-09-19T21:45:13Z","title":"Cross-Domain Content Generation with Domain-Specific Small Language\n  Models","summary":"  Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.\n","authors":["Ankit Maloo","Abhinav Garg"],"pdf_url":"https://arxiv.org/pdf/2409.17171v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.01401v1","updated":"2024-10-02T10:27:07Z","published":"2024-10-02T10:27:07Z","title":"Question-guided Knowledge Graph Re-scoring and Injection for Knowledge\n  Graph Question Answering","summary":"  Knowledge graph question answering (KGQA) involves answering natural language\nquestions by leveraging structured information stored in a knowledge graph.\nTypically, KGQA initially retrieve a targeted subgraph from a large-scale\nknowledge graph, which serves as the basis for reasoning models to address\nqueries. However, the retrieved subgraph inevitably brings distraction\ninformation for knowledge utilization, impeding the model's ability to perform\naccurate reasoning. To address this issue, we propose a Question-guided\nKnowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the\ninput question, thereby focusing specifically on pertinent factual knowledge.\nMoreover, we introduce Knowformer, a parameter-efficient method for injecting\nthe re-scored knowledge graph into large language models to enhance their\nability to perform factual reasoning. Extensive experiments on multiple KGQA\nbenchmarks demonstrate the superiority of our method over existing systems.\n","authors":["Yu Zhang","Kehai Chen","Xuefeng Bai","zhao kang","Quanjiang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01401v1.pdf","comment":"findings of EMNLP2024"},{"id":"http://arxiv.org/abs/2410.01400v1","updated":"2024-10-02T10:24:51Z","published":"2024-10-02T10:24:51Z","title":"CrowdCounter: A benchmark type-specific multi-target counterspeech\n  dataset","summary":"  Counterspeech presents a viable alternative to banning or suspending users\nfor hate speech while upholding freedom of expression. However, writing\neffective counterspeech is challenging for moderators/users. Hence, developing\nsuggestion tools for writing counterspeech is the need of the hour. One\ncritical challenge in developing such a tool is the lack of quality and\ndiversity of the responses in the existing datasets. Hence, we introduce a new\ndataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs\nspanning six different counterspeech types (empathy, humor, questioning,\nwarning, shaming, contradiction), which is the first of its kind. The design of\nour annotation platform itself encourages annotators to write type-specific,\nnon-redundant and high-quality counterspeech. We evaluate two frameworks for\ngenerating counterspeech responses - vanilla and type-controlled prompts -\nacross four large language models. In terms of metrics, we evaluate the\nresponses using relevance, diversity and quality. We observe that Flan-T5 is\nthe best model in the vanilla framework across different models. Type-specific\nprompts enhance the relevance of the responses, although they might reduce the\nlanguage quality. DialoGPT proves to be the best at following the instructions\nand generating the type-specific counterspeech accurately.\n","authors":["Punyajoy Saha","Abhilash Datta","Abhik Jana","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.01400v1.pdf","comment":"19 pages, 1 figure, 14 tables, Code available\n  https://github.com/hate-alert/CrowdCounter"},{"id":"http://arxiv.org/abs/2410.01383v1","updated":"2024-10-02T09:51:42Z","published":"2024-10-02T09:51:42Z","title":"PairDistill: Pairwise Relevance Distillation for Dense Retrieval","summary":"  Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01383v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.01380v1","updated":"2024-10-02T09:49:45Z","published":"2024-10-02T09:49:45Z","title":"Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition","summary":"  In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.\n","authors":["Jiyeon Kim","Hyunji Lee","Hyowon Cho","Joel Jang","Hyeonbin Hwang","Seungpil Won","Youbin Ahn","Dohaeng Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.01380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18511v2","updated":"2024-10-02T09:42:17Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v2.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2410.01363v1","updated":"2024-10-02T09:23:07Z","published":"2024-10-02T09:23:07Z","title":"PCQPR: Proactive Conversational Question Planning with Reflection","summary":"  Conversational Question Generation (CQG) enhances the interactivity of\nconversational question-answering systems in fields such as education, customer\nservice, and entertainment. However, traditional CQG, focusing primarily on the\nimmediate context, lacks the conversational foresight necessary to guide\nconversations toward specified conclusions. This limitation significantly\nrestricts their ability to achieve conclusion-oriented conversational outcomes.\nIn this work, we redefine the CQG task as Conclusion-driven Conversational\nQuestion Generation (CCQG) by focusing on proactivity, not merely reacting to\nthe unfolding conversation but actively steering it towards a\nconclusion-oriented question-answer pair. To address this, we propose a novel\napproach, called Proactive Conversational Question Planning with self-Refining\n(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte\nCarlo Tree Search (MCTS) with the analytical capabilities of large language\nmodels (LLMs), PCQPR predicts future conversation turns and continuously\nrefines its questioning strategies. This iterative self-refining mechanism\nensures the generation of contextually relevant questions strategically devised\nto reach a specified outcome. Our extensive evaluations demonstrate that PCQPR\nsignificantly surpasses existing CQG methods, marking a paradigm shift towards\nconclusion-oriented conversational question-answering systems.\n","authors":["Shasha Guo","Lizi Liao","Jing Zhang","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01363v1.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2409.00997v2","updated":"2024-10-02T09:18:55Z","published":"2024-09-02T07:23:13Z","title":"DataSculpt: Crafting Data Landscapes for Long-Context LLMs through\n  Multi-Objective Partitioning","summary":"  In recent years, Large Language Models (LLMs) have demonstrated significant\nimprovements across a variety of tasks, one of which is the long-context\ncapability. The key to improving long-context performance lies in effective\ndata organization and management strategies that integrate data from multiple\ndomains and optimize the context window during training. Through extensive\nexperimental analysis, we identified three key challenges in designing\neffective data management strategies that enable the model to achieve\nlong-context capability without sacrificing performance in other tasks: (1) a\nshortage of long documents across multiple domains, (2) effective construction\nof context windows, and (3) efficient organization of large-scale datasets. To\naddress these challenges, we introduce DataSculpt, a novel data management\nframework designed for long-context training. We first formulate the\norganization of training data as a multi-objective combinatorial optimization\nproblem, focusing on attributes including relevance, homogeneity, integrity,\nand efficiency. Specifically, our approach utilizes a coarse-to-fine\nmethodology to optimize training data organization both efficiently and\neffectively. We begin by clustering the data based on semantic similarity\n(coarse), followed by a multi-objective greedy search within each cluster to\nscore and concatenate documents into various context windows (fine). Our\ncomprehensive evaluations demonstrate that DataSculpt significantly enhances\nlong-context training performance, resulting in improvements of 18.09% in\nretrieval augmentation, 21.23% in summarization, 21.27% in reading\ncomprehension, and a 3.81% increase in code completion, while also maintaining\noverall model proficiency with a 4.88% improvement.\n","authors":["Keer Lu","Xiaonan Nie","Zheng Liang","Da Pan","Shusen Zhang","Keshi Zhao","Weipeng Chen","Zenan Zhou","Guosheng Dong","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.00997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10770v4","updated":"2024-10-02T09:18:47Z","published":"2024-02-16T15:48:33Z","title":"How Reliable Are Automatic Evaluation Methods for Instruction-Tuned\n  LLMs?","summary":"  Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.\n","authors":["Ehsan Doostmohammadi","Oskar Holmström","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2402.10770v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01356v1","updated":"2024-10-02T09:14:39Z","published":"2024-10-02T09:14:39Z","title":"Assisted Data Annotation for Business Process Information Extraction\n  from Textual Documents","summary":"  Machine-learning based generation of process models from natural language\ntext process descriptions provides a solution for the time-intensive and\nexpensive process discovery phase. Many organizations have to carry out this\nphase, before they can utilize business process management and its benefits.\nYet, research towards this is severely restrained by an apparent lack of large\nand high-quality datasets. This lack of data can be attributed to, among other\nthings, an absence of proper tool assistance for dataset creation, resulting in\nhigh workloads and inferior data quality. We explore two assistance features to\nsupport dataset creation, a recommendation system for identifying process\ninformation in the text and visualization of the current state of already\nidentified process information as a graphical business process model. A\ncontrolled user study with 31 participants shows that assisting dataset\ncreators with recommendations lowers all aspects of workload, up to $-51.0\\%$,\nand significantly improves annotation quality, up to $+38.9\\%$. We make all\ndata and code available to encourage further research on additional novel\nassistance strategies.\n","authors":["Julian Neuberger","Han van der Aa","Lars Ackermann","Daniel Buschek","Jannic Herrmann","Stefan Jablonski"],"pdf_url":"https://arxiv.org/pdf/2410.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Défossez","Laurent Mazaré","Manu Orsini","Amélie Royer","Patrick Pérez","Hervé Jégou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11184v3","updated":"2024-10-02T09:07:15Z","published":"2024-04-17T09:01:02Z","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document","summary":"  Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems.\n","authors":["Joonho Yang","Seunghyun Yoon","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11184v3.pdf","comment":"Published as a main conference paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01335v1","updated":"2024-10-02T08:53:07Z","published":"2024-10-02T08:53:07Z","title":"Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models","summary":"  Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.\n","authors":["Lucas Bandarkar","Benjamin Muller","Pritish Yuvraj","Rui Hou","Nayan Singhal","Hongjiang Lv","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01335v1.pdf","comment":"11 main pages, 23 pages total, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.01334v1","updated":"2024-10-02T08:52:58Z","published":"2024-10-02T08:52:58Z","title":"Unveiling Language Skills under Circuits","summary":"  The exploration of language skills in language models (LMs) has always been\none of the central goals in mechanistic interpretability. However, existing\ncircuit analyses often fall short in representing the full functional scope of\nthese models, primarily due to the exclusion of Feed-Forward layers.\nAdditionally, isolating the effect of a single language skill from a text,\nwhich inherently involves multiple entangled skills, poses a significant\nchallenge. To address these gaps, we introduce a novel concept, Memory Circuit,\na minimum unit that fully and independently manipulates the memory-reading\nfunctionality of a language model, and disentangle the transformer model\nprecisely into a circuit graph which is an ensemble of paths connecting\ndifferent memory circuits. Based on this disentanglement, we identify salient\ncircuit paths, named as skill paths, responsible for three crucial language\nskills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning\n(ICL) Skill, leveraging causal effect estimation through interventions and\ncounterfactuals. Our experiments on various datasets confirm the correspondence\nbetween our identified skill paths and language skills, and validate three\nlongstanding hypotheses: 1) Language skills are identifiable through circuit\ndissection; 2) Simple language skills reside in shallow layers, whereas complex\nlanguage skills are found in deeper layers; 3) Complex language skills are\nformed on top of simpler language skills. Our codes are available at:\nhttps://github.com/Zodiark-ch/Language-Skill-of-LLMs.\n","authors":["Hang Chen","Jiaying Zhu","Xinyu Yang","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13979v3","updated":"2024-10-02T08:51:45Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03199v2","updated":"2024-10-02T08:45:32Z","published":"2024-05-24T13:33:11Z","title":"Bayesian WeakS-to-Strong from Text Classification to Generation","summary":"  Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.\n","authors":["Ziyun Cui","Ziyang Zhang","Wen Wu","Guangzhi Sun","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18164v2","updated":"2024-10-02T08:40:36Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.13550v2","updated":"2024-10-02T08:32:31Z","published":"2024-02-21T06:11:03Z","title":"Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues","summary":"  A successful negotiation requires a range of capabilities, including\ncomprehension of the conversation context, Theory-of-Mind (ToM) skills to infer\nthe partner's motives, strategic reasoning, and effective communication, making\nit challenging for automated systems. Despite the remarkable performance of\nLLMs in various NLP tasks, there is no systematic evaluation of their\ncapabilities in negotiation. Such an evaluation is critical for advancing AI\nnegotiation agents and negotiation research, ranging from designing dialogue\nsystems to providing pedagogical feedback and scaling up data collection\npractices. This work aims to systematically analyze the multifaceted\ncapabilities of LLMs across diverse dialogue scenarios throughout the stages of\na typical negotiation interaction. Our analysis highlights GPT-4's superior\nperformance in many tasks while identifying specific challenges, such as making\nsubjective assessments and generating contextually appropriate, strategically\nadvantageous responses.\n","authors":["Deuksin Kwon","Emily Weiss","Tara Kulshrestha","Kushal Chawla","Gale M. Lucas","Jonathan Gratch"],"pdf_url":"https://arxiv.org/pdf/2402.13550v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.20135v3","updated":"2024-10-02T08:32:02Z","published":"2024-09-30T09:34:31Z","title":"Federated Instruction Tuning of LLMs with Domain Coverage Augmentation","summary":"  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges.\n","authors":["Zezhou Wang","Yaxin Du","Zhuzhong Qian","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20135v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18256v2","updated":"2024-10-02T08:28:36Z","published":"2024-06-26T11:08:17Z","title":"Llamipa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.00727v2","updated":"2024-10-02T08:08:45Z","published":"2024-10-01T14:16:10Z","title":"Show Me What's Wrong!: Combining Charts and Text to Guide Data Analysis","summary":"  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome\nbut vital task across different domains. In the context of financial fraud\ndetection, analysts must quickly identify suspicious activity among\ntransactional data. This is an iterative process made of complex exploratory\ntasks such as recognizing patterns, grouping, and comparing. To mitigate the\ninformation overload inherent to these steps, we present a tool combining\nautomated information highlights, Large Language Model generated textual\ninsights, and visual analytics, facilitating exploration at different levels of\ndetail. We perform a segmentation of the data per analysis area and visually\nrepresent each one, making use of automated visual cues to signal which require\nmore attention. Upon user selection of an area, our system provides textual and\ngraphical summaries. The text, acting as a link between the high-level and\ndetailed views of the chosen segment, allows for a quick understanding of\nrelevant details. A thorough exploration of the data comprising the selection\ncan be done through graphical representations. The feedback gathered in a study\nperformed with seven domain experts suggests our tool effectively supports and\nguides exploratory analysis, easing the identification of suspicious\ninformation.\n","authors":["Beatriz Feliciano","Rita Costa","Jean Alves","Javier Liébana","Diogo Duarte","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2410.00727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01306v1","updated":"2024-10-02T08:01:05Z","published":"2024-10-02T08:01:05Z","title":"Emotion-Aware Response Generation Using Affect-Enriched Embeddings with\n  LLMs","summary":"  There is a need for empathetic and coherent responses in automated\nchatbot-facilitated psychotherapy sessions. This study addresses the challenge\nof enhancing the emotional and contextual understanding of large language\nmodels (LLMs) in psychiatric applications. We introduce a novel framework that\nintegrates multiple emotion lexicons, including NRC Emotion Lexicon, VADER,\nWordNet, and SentiWordNet, with state-of-the-art LLMs such as LLAMA 2, Flan-T5,\nChatGPT 3.0, and ChatGPT 4.0. The primary dataset comprises over 2,000 therapy\nsession transcripts from the Counseling and Psychotherapy database, covering\ndiscussions on anxiety, depression, trauma, and addiction. We segment the\ntranscripts into smaller chunks, enhancing them with lexical features and\ncomputing embeddings using BERT, GPT-3, and RoBERTa to capture semantic and\nemotional nuances. These embeddings are stored in a FAISS vector database,\nenabling efficient similarity search and clustering based on cosine similarity.\nUpon user query, the most relevant segments are retrieved and provided as\ncontext to the LLMs, significantly improving the models' ability to generate\nempathetic and contextually appropriate responses. Experimental evaluations\ndemonstrate that in-corporating emotion lexicons enhances empathy, coherence,\ninformativeness, and fluency scores. Our findings highlight the critical role\nof emotional embeddings in improving LLM performance for psychotherapy.\n","authors":["Abdur Rasool","Muhammad Irfan Shahzad","Hafsa Aslam","Vincent Chan"],"pdf_url":"https://arxiv.org/pdf/2410.01306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04927v3","updated":"2024-10-02T07:58:56Z","published":"2024-09-07T22:54:47Z","title":"Just ASR + LLM? A Study on Speech Large Language Models' Ability to\n  Identify and Understand Speaker in Spoken Dialogue","summary":"  In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. SpeechLLMs have demonstrated impressive spoken dialog\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ntranscript alone, i.e.\\ without speaker segmentation and identification. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered reliably with correct speaker identification. The results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that tasks focused on\nidentity-critical questions could offer a more accurate evaluation framework of\nSpeechLLMs in SQA.\n","authors":["Junkai Wu","Xulin Fan","Bo-Ru Lu","Xilin Jiang","Nima Mesgarani","Mark Hasegawa-Johnson","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2409.04927v3.pdf","comment":"Accepted to IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2410.01305v1","updated":"2024-10-02T07:57:33Z","published":"2024-10-02T07:57:33Z","title":"Revisiting Hierarchical Text Classification: Inference and Metrics","summary":"  Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}.\n","authors":["Roman Plaud","Matthieu Labeau","Antoine Saillenfest","Thomas Bonald"],"pdf_url":"https://arxiv.org/pdf/2410.01305v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2409.16911v2","updated":"2024-10-02T07:52:56Z","published":"2024-09-25T13:15:50Z","title":"Pruning Multilingual Large Language Models for Multilingual Inference","summary":"  Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.\n","authors":["Hwichan Kim","Jun Suzuki","Tosho Hirasawa","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2409.16911v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.04507v2","updated":"2024-10-02T07:52:37Z","published":"2024-09-06T16:32:46Z","title":"3D Data Long-Term Preservation in Cultural Heritage","summary":"  The report explores the challenges and strategies for preserving 3D digital\ndata in cultural heritage. It discusses the issue of technological\nobsolescence, emphasising the need for ustainable storage solutions and ongoing\ndata management strategies. Key topics include understanding technological\nobsolescence, the lifecycle of digital content, digital continuity, data\nmanagement plans (DMP), FAIR principles, and the use of public repositories.\nThe report also covers the importance of metadata in long-term digital\npreservation, including types of metadata and strategies for building valuable\nmetadata. It examines the evolving standards and interoperability in 3D format\npreservation and the importance of managing metadata and paradata. The document\nprovides a comprehensive overview of the challenges and solutions for\npreserving 3D cultural heritage data in the long term.\n","authors":["Nicola Amico","Achille Felicetti"],"pdf_url":"https://arxiv.org/pdf/2409.04507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16508v3","updated":"2024-10-02T07:51:47Z","published":"2024-02-26T11:42:29Z","title":"Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision","summary":"  Cross-lingual open domain question answering (CLQA) is a complex problem,\ncomprising cross-lingual retrieval from a multilingual knowledge base, followed\nby answer generation in the query language. Both steps are usually tackled by\nseparate models, requiring substantial annotated datasets, and typically\nauxiliary resources, like machine translation systems to bridge between\nlanguages. In this paper, we show that CLQA can be addressed using a single\nencoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural questions to supervise answer\ngeneration. Together, we show our approach, \\texttt{CLASS}, outperforms\ncomparable methods on both supervised and zero-shot language adaptation\nsettings, including those using machine translation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2402.16508v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01294v1","updated":"2024-10-02T07:40:56Z","published":"2024-10-02T07:40:56Z","title":"Endless Jailbreaks with Bijection Learning","summary":"  Despite extensive safety training, LLMs are vulnerable to adversarial inputs.\nIn this work, we introduce a simple but powerful attack paradigm, bijection\nlearning, that yields a practically endless set of jailbreak prompts. We\nexploit language models' advanced reasoning capabilities to teach them\ninvertible languages (bijections) in context, pass encoded queries to the model\nto bypass built-in safety mechanisms, and finally decode responses back into\nEnglish, yielding helpful replies to harmful requests. Our approach proves\neffective on a wide range of frontier language models and harm categories.\nBijection learning is an automated and universal attack that grows stronger\nwith scale: larger models with more advanced reasoning capabilities are more\nsusceptible to bijection learning jailbreaks despite stronger safety\nmechanisms.\n","authors":["Brian R. Y. Huang","Maximilian Li","Leonard Tang"],"pdf_url":"https://arxiv.org/pdf/2410.01294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15371v3","updated":"2024-10-02T07:38:02Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08700v3","updated":"2024-10-02T07:26:40Z","published":"2024-04-10T18:08:59Z","title":"DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs","summary":"  LLMs acquire knowledge from massive data snapshots collected at different\ntimestamps. Their knowledge is then commonly evaluated using static benchmarks.\nHowever, factual knowledge is generally subject to time-sensitive changes, and\nstatic benchmarks cannot address those cases. We present an approach to\ndynamically evaluate the knowledge in LLMs and their time-sensitiveness against\nWikidata, a publicly available up-to-date knowledge graph. We evaluate the\ntime-sensitive knowledge in twenty-four private and open-source LLMs, as well\nas the effectiveness of four editing methods in updating the outdated facts.\nOur results show that 1) outdatedness is a critical problem across\nstate-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with\nslight variations of the question prompt; and 3) the performance of the\nstate-of-the-art knowledge editing algorithms is very limited, as they can not\nreduce the cases of outdatedness and output inconsistency.\n","authors":["Seyed Mahed Mousavi","Simone Alghisi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2404.08700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01288v1","updated":"2024-10-02T07:18:16Z","published":"2024-10-02T07:18:16Z","title":"Mitigating Copy Bias in In-Context Learning through Neuron Pruning","summary":"  Large language models (LLMs) have demonstrated impressive few-shot in-context\nlearning (ICL) abilities. Still, we show that they are sometimes prone to a\n`copying bias', where they copy answers from provided examples instead of\nlearning the underlying patterns. In this work, we propose a novel and simple\nmethod to mitigate such copying bias. First, we create a synthetic task and use\nthe Integrated Gradients method to identify neurons that prioritize copying\nover generalization. We demonstrate that pruning these neurons consistently\nimproves performance across a diverse set of ICL tasks. We also show that our\nmethod is applicable across various LLM architectures, including Transformers\nand State-Space Models, without requiring modifications. In our analysis, we\nadopt a task-recognition perspective on ICL and examine task vectors (Hendel et\nal., 2023) induced by the model. We find that pruning enhances the quality of\nthese vectors, suggesting that the pruned neurons previously hindered effective\ntask recognition.\n","authors":["Ameen Ali","Lior Wolf","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2410.01288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01285v1","updated":"2024-10-02T07:14:26Z","published":"2024-10-02T07:14:26Z","title":"Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration","summary":"  The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.\n","authors":["Kangxi Wu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01285v1.pdf","comment":"Accepted to the EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2305.03977v3","updated":"2024-10-02T06:35:36Z","published":"2023-05-06T08:43:33Z","title":"Unlocking the Power of GANs in Non-Autoregressive Text Generation","summary":"  Generative Adversarial Networks (GANs) have been studied in text generation\nto tackle the exposure bias problem. Despite their remarkable development, they\nadopt autoregressive structures so suffering from high latency in both training\nand inference stages. Although GANs have potential to support efficient\ngeneration by adopting non-autoregressive (NAR) structures, their explorations\nin NAR models are extremely limited. In this work, we conduct pioneering study\nof building language GANs based on NAR structures. We identify two issues that\nconstrain the performance of GAN-based NAR models. Firstly, existing methods of\nincorporating latent variables provide highly similar representations which\ncannot describe the diversity of different words in sentences. We tackle this\nproblem by proposing Position-Aware Self-Modulation, providing more diverse and\neffective representations. Secondly, the attention mechanism in Transformer\ncannot accurately build word dependencies in the unstable training of GANs, and\nwe adopt Dependency Feed Forward Network to enhance the model capacity in\ndependency modeling. Armed with these two facilities, we propose a GAN-based\nNAR model, Adversarial Non-autoregressive Transformer (ANT). The experimental\nresults demonstrate that ANT can achieve comparable performance with mainstream\nmodels in a single forward pass and has great potential in various applications\nlike latent interpolation and semi-supervised learning.\n","authors":["Da Ren","Yi Cai","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2305.03977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02118v2","updated":"2024-10-02T06:32:10Z","published":"2024-07-02T10:06:41Z","title":"Breaking Language Barriers: Cross-Lingual Continual Pre-Training at\n  Scale","summary":"  In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.\n","authors":["Wenzhen Zheng","Wenbo Pan","Xu Xu","Libo Qin","Li Yue","Ming Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.02118v2.pdf","comment":"8 pages. Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01268v1","updated":"2024-10-02T06:24:51Z","published":"2024-10-02T06:24:51Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications","summary":"  This book serves as an introduction to deep learning and machine learning,\nfocusing on their applications in big data analytics. It covers essential\nconcepts, tools like ChatGPT and Claude, hardware recommendations, and\npractical guidance on setting up development environments using libraries like\nPyTorch and TensorFlow. Designed for beginners and advanced users alike, it\nprovides step-by-step instructions, hands-on projects, and insights into AI's\nfuture, including AutoML and edge computing.\n","authors":["Pohsun Feng","Ziqian Bi","Yizhu Wen","Xuanhe Pan","Benji Peng","Ming Liu","Jiawei Xu","Keyu Chen","Junyu Liu","Caitlyn Heqi Yin","Sen Zhang","Jinlang Wang","Qian Niu","Ming Li","Tianyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01268v1.pdf","comment":"This book contains 156 pages and 9 figures"},{"id":"http://arxiv.org/abs/2407.02834v3","updated":"2024-10-02T06:18:02Z","published":"2024-07-03T06:21:07Z","title":"Aspect-Based Sentiment Analysis Techniques: A Comparative Study","summary":"  Since the dawn of the digitalisation era, customer feedback and online\nreviews are unequivocally major sources of insights for businesses.\nConsequently, conducting comparative analyses of such sources has become the de\nfacto modus operandi of any business that wishes to give itself a competitive\nedge over its peers and improve customer loyalty. Sentiment analysis is one\nsuch method instrumental in gauging public interest, exposing market trends,\nand analysing competitors. While traditional sentiment analysis focuses on\noverall sentiment, as the needs advance with time, it has become important to\nexplore public opinions and sentiments on various specific subjects, products\nand services mentioned in the reviews on a finer-granular level. To this end,\nAspect-based Sentiment Analysis (ABSA), supported by advances in Artificial\nIntelligence (AI) techniques which have contributed to a paradigm shift from\nsimple word-level analysis to tone and context-aware analyses, focuses on\nidentifying specific aspects within the text and determining the sentiment\nassociated with each aspect. In this study, we compare several deep-NN methods\nfor ABSA on two benchmark datasets (Restaurant14 and Laptop-14) and found that\nFAST LSA obtains the best overall results of 87.6% and 82.6% accuracy but does\nnot pass LSA+DeBERTa which reports 90.33% and 86.21% accuracy respectively.\n","authors":["Dineth Jayakody","Koshila Isuranda","A V A Malkith","Nisansa de Silva","Sachintha Rajith Ponnamperuma","G G N Sandamali","K L K Sudheera"],"pdf_url":"https://arxiv.org/pdf/2407.02834v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13621v2","updated":"2024-10-02T06:14:17Z","published":"2024-09-20T16:32:54Z","title":"Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network","summary":"  Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.\n","authors":["Haoran Li","Qiang Gao","Hongmei Wu","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.13621v2.pdf","comment":"EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI"},{"id":"http://arxiv.org/abs/2410.01257v1","updated":"2024-10-02T06:05:52Z","published":"2024-10-02T06:05:52Z","title":"HelpSteer2-Preference: Complementing Ratings with Preferences","summary":"  Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We\nalso demonstrate the effectiveness of this reward model at aligning models to\nfollow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the\ntrained Reward Model at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward\n","authors":["Zhilin Wang","Alexander Bukharin","Olivier Delalleau","Daniel Egert","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2410.01257v1.pdf","comment":"26 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.03868v2","updated":"2024-10-02T05:51:53Z","published":"2024-04-05T02:53:51Z","title":"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge\n  Graph Construction","summary":"  In this work, we are interested in automated methods for knowledge graph\ncreation (KGC) from input text. Progress on large language models (LLMs) has\nprompted a series of recent works applying them to KGC, e.g., via zero/few-shot\nprompting. Despite successes on small domain-specific datasets, these models\nface difficulties scaling up to text common in many real-world applications. A\nprincipal issue is that, in prior methods, the KG schema has to be included in\nthe LLM prompt to generate valid triplets; larger and more complex schemas\neasily exceed the LLMs' context window length. Furthermore, there are scenarios\nwhere a fixed pre-defined schema is not available and we would like the method\nto construct a high-quality KG with a succinct self-generated schema. To\naddress these problems, we propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by\nschema definition and post-hoc canonicalization. EDC is flexible in that it can\nbe applied to settings where a pre-defined target schema is available and when\nit is not; in the latter case, it constructs a schema automatically and applies\nself-canonicalization. To further improve performance, we introduce a trained\ncomponent that retrieves schema elements relevant to the input text; this\nimproves the LLMs' extraction performance in a retrieval-augmented\ngeneration-like manner. We demonstrate on three KGC benchmarks that EDC is able\nto extract high-quality triplets without any parameter tuning and with\nsignificantly larger schemas compared to prior works. Code for EDC is available\nat https://github.com/clear-nus/edc.\n","authors":["Bowen Zhang","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2404.03868v2.pdf","comment":"18 pages, 3 figures, Proceedings of the 2024 Conference on Empirical\n  Methods in Natural Language Processing"},{"id":"http://arxiv.org/abs/2410.01246v1","updated":"2024-10-02T05:22:07Z","published":"2024-10-02T05:22:07Z","title":"AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended\n  Responses","summary":"  Question answering (QA) tasks have been extensively studied in the field of\nnatural language processing (NLP). Answers to open-ended questions are highly\ndiverse and difficult to quantify, and cannot be simply evaluated as correct or\nincorrect, unlike close-ended questions with definitive answers. While large\nlanguage models (LLMs) have demonstrated strong capabilities across various\ntasks, they exhibit relatively weaker performance in evaluating answers to\nopen-ended questions. In this study, we propose a method that leverages LLMs\nand the analytic hierarchy process (AHP) to assess answers to open-ended\nquestions. We utilized LLMs to generate multiple evaluation criteria for a\nquestion. Subsequently, answers were subjected to pairwise comparisons under\neach criterion with LLMs, and scores for each answer were calculated in the\nAHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and\nGPT-4. Our results indicate that our approach more closely aligns with human\njudgment compared to the four baselines. Additionally, we explored the impact\nof the number of criteria, variations in models, and differences in datasets on\nthe results.\n","authors":["Xiaotian Lu","Jiyi Li","Koh Takeuchi","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2410.01246v1.pdf","comment":"Accepted for EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.01242v1","updated":"2024-10-02T05:07:02Z","published":"2024-10-02T05:07:02Z","title":"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance","summary":"  Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.\n","authors":["Haolin Jin","Zechao Sun","Yiheng Yang","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01240v1","updated":"2024-10-02T05:04:06Z","published":"2024-10-02T05:04:06Z","title":"Automatic deductive coding in discourse analysis: an application of\n  large language models in learning analytics","summary":"  Deductive coding is a common discourse analysis method widely used by\nlearning science and learning analytics researchers for understanding teaching\nand learning interactions. It often requires researchers to manually label all\ndiscourses to be analyzed according to a theoretically guided coding scheme,\nwhich is time-consuming and labor-intensive. The emergence of large language\nmodels such as GPT has opened a new avenue for automatic deductive coding to\novercome the limitations of traditional deductive coding. To evaluate the\nusefulness of large language models in automatic deductive coding, we employed\nthree different classification methods driven by different artificial\nintelligence technologies, including the traditional text classification method\nwith text feature engineering, BERT-like pretrained language model and GPT-like\npretrained large language model (LLM). We applied these methods to two\ndifferent datasets and explored the potential of GPT and prompt engineering in\nautomatic deductive coding. By analyzing and comparing the accuracy and Kappa\nvalues of these three classification methods, we found that GPT with prompt\nengineering outperformed the other two methods on both datasets with limited\nnumber of training samples. By providing detailed prompt structures, the\nreported work demonstrated how large language models can be used in the\nimplementation of automatic deductive coding.\n","authors":["Lishan Zhang","Han Wu","Xiaoshan Huang","Tengfei Duan","Hanxiang Du"],"pdf_url":"https://arxiv.org/pdf/2410.01240v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.05152v2","updated":"2024-10-02T05:02:02Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v2.pdf","comment":"EMNLP 2024 Findings; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.04081v3","updated":"2024-10-02T05:00:57Z","published":"2024-09-06T07:44:44Z","title":"UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity","summary":"  Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.\n","authors":["Yicheng Fu","Raviteja Anantha","Prabal Vashisht","Jianpeng Cheng","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2409.04081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01035v2","updated":"2024-10-02T04:20:31Z","published":"2024-09-02T08:10:51Z","title":"Unleashing the Power of Task-Specific Directions in Parameter Efficient\n  Fine-tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet requiring extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs)-critical for transitioning large\nmodels from pretrained states to task-specific enhancements in PEFT. We propose\na framework to clearly define these directions and explore their properties,\nand practical utilization challenges. We then introduce a novel approach,\nLoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning\nprocess, thereby enhancing model performance on targeted tasks. Extensive\nexperiments have conclusively demonstrated the effectiveness of LoRA-Dash, and\nin-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The\ncode is available at https://github.com/Chongjie-Si/Subspace-Tuning.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v2.pdf","comment":"Revisions ongoing. Codes in\n  https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2401.05967v3","updated":"2024-10-02T04:17:36Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v3.pdf","comment":"EMNLP2024 findings (Long)"},{"id":"http://arxiv.org/abs/2409.19541v3","updated":"2024-10-02T04:15:11Z","published":"2024-09-29T03:56:50Z","title":"Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance\n  Regularization","summary":"  Language models frequently inherit societal biases from their training data.\nNumerous techniques have been proposed to mitigate these biases during both the\npre-training and fine-tuning stages. However, fine-tuning a pre-trained\ndebiased language model on a downstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing methods for downstream tasks either (i)\nrequire labels of protected attributes (e.g., age, race, or political views)\nthat are often not available or (ii) rely on indicators of bias, which\nrestricts their applicability to gender debiasing since they rely on\ngender-specific words. To address this, we introduce a novel debiasing\nregularization technique based on the class-wise variance of embeddings.\nCrucially, our method does not require attribute labels and targets any\nattribute, thus addressing the shortcomings of existing debiasing methods. Our\nexperiments on encoder language models and three datasets demonstrate that our\nmethod outperforms existing strong debiasing baselines that rely on target\nattribute labels while maintaining performance on the target task.\n","authors":["Shahed Masoudian","Markus Frohmann","Navid Rekabsaz","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2409.19541v3.pdf","comment":"Accepted to EMNLP 2024"}],"Sound":[{"id":"http://arxiv.org/abs/2410.01562v1","updated":"2024-10-02T14:00:41Z","published":"2024-10-02T14:00:41Z","title":"HRTF Estimation using a Score-based Prior","summary":"  We present a head-related transfer function (HRTF) estimation method which\nrelies on a data-driven prior given by a score-based diffusion model. The HRTF\nis estimated in reverberant environments using natural excitation signals, e.g.\nhuman speech. The impulse response of the room is estimated along with the HRTF\nby optimizing a parametric model of reverberation based on the statistical\nbehaviour of room acoustics. The posterior distribution of HRTF given the\nreverberant measurement and excitation signal is modelled using the score-based\nHRTF prior and a log-likelihood approximation. We show that the resulting\nmethod outperforms several baselines, including an oracle recommender system\nthat assigns the optimal HRTF in our training set based on the smallest\ndistance to the true HRTF at the given direction of arrival. In particular, we\nshow that the diffusion prior can account for the large variability of\nhigh-frequency content in HRTFs.\n","authors":["Etienne Thuillier","Jean-Marie Lemercier","Eloi Moliner","Timo Gerkmann","Vesa Välimäki"],"pdf_url":"https://arxiv.org/pdf/2410.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13049v2","updated":"2024-10-02T13:04:02Z","published":"2024-09-19T18:55:13Z","title":"DiffSSD: A Diffusion-Based Dataset For Speech Forensics","summary":"  Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.\n","authors":["Kratika Bhagtani","Amit Kumar Singh Yadav","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2409.13049v2.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2025"},{"id":"http://arxiv.org/abs/2410.01481v1","updated":"2024-10-02T12:33:59Z","published":"2024-10-02T12:33:59Z","title":"SonicSim: A customizable simulation platform for speech processing in\n  moving sound source scenarios","summary":"  The systematic evaluation of speech separation and enhancement models under\nmoving sound source conditions typically requires extensive data comprising\ndiverse scenarios. However, real-world datasets often contain insufficient data\nto meet the training and evaluation requirements of models. Although synthetic\ndatasets offer a larger volume of data, their acoustic simulations lack\nrealism. Consequently, neither real-world nor synthetic datasets effectively\nfulfill practical needs. To address these issues, we introduce SonicSim, a\nsynthetic toolkit de-designed to generate highly customizable data for moving\nsound sources. SonicSim is developed based on the embodied AI simulation\nplatform, Habitat-sim, supporting multi-level adjustments, including\nscene-level, microphone-level, and source-level, thereby generating more\ndiverse synthetic data. Leveraging SonicSim, we constructed a moving sound\nsource benchmark dataset, SonicSet, using the Librispeech, the Freesound\nDataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the\nMatterport3D to evaluate speech separation and enhancement models.\nAdditionally, to validate the differences between synthetic data and real-world\ndata, we randomly selected 5 hours of raw data without reverberation from the\nSonicSet validation set to record a real-world speech separation dataset, which\nwas then compared with the corresponding synthetic datasets. Similarly, we\nutilized the real-world speech enhancement dataset RealMAN to validate the\nacoustic gap between other synthetic datasets and the SonicSet dataset for\nspeech enhancement. The results indicate that the synthetic data generated by\nSonicSim can effectively generalize to real-world scenarios. Demo and code are\npublicly available at https://cslikai.cn/SonicSim/.\n","authors":["Kai Li","Wendi Sang","Chang Zeng","Runxuan Yang","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01481v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2410.01469v1","updated":"2024-10-02T12:21:06Z","published":"2024-10-02T12:21:06Z","title":"TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for\n  Efficient Speech Separation","summary":"  In recent years, much speech separation research has focused primarily on\nimproving model performance. However, for low-latency speech processing\nsystems, high efficiency is equally important. Therefore, we propose a speech\nseparation model with significantly reduced parameters and computational costs:\nTime-frequency Interleaved Gain Extraction and Reconstruction network (TIGER).\nTIGER leverages prior knowledge to divide frequency bands and compresses\nfrequency information. We employ a multi-scale selective attention module to\nextract contextual features, while introducing a full-frequency-frame attention\nmodule to capture both temporal and frequency contextual information.\nAdditionally, to more realistically evaluate the performance of speech\nseparation models in complex acoustic environments, we introduce a dataset\ncalled EchoSet. This dataset includes noise and more realistic reverberation\n(e.g., considering object occlusions and material properties), with speech from\ntwo speakers overlapping at random proportions. Experimental results showed\nthat models trained on EchoSet had better generalization ability than those\ntrained on other datasets to the data collected in the physical world, which\nvalidated the practical value of the EchoSet. On EchoSet and real-world data,\nTIGER significantly reduces the number of parameters by 94.3% and the MACs by\n95.3% while achieving performance surpassing state-of-the-art (SOTA) model\nTF-GridNet. This is the first speech separation model with fewer than 1 million\nparameters that achieves performance comparable to the SOTA model.\n","authors":["Mohan Xu","Kai Li","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01469v1.pdf","comment":"Technical report, demo page: https://cslikai.cn/TIGER/"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Défossez","Laurent Mazaré","Manu Orsini","Amélie Royer","Patrick Pérez","Hervé Jégou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01350v1","updated":"2024-10-02T09:07:33Z","published":"2024-10-02T09:07:33Z","title":"Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and\n  Memory-Augmented Context-Aware Timbre Modeling","summary":"  Zero-shot voice conversion (VC) aims to transform the source speaker timbre\ninto an arbitrary unseen one without altering the original speech content.While\nrecent advancements in zero-shot VC methods have shown remarkable progress,\nthere still remains considerable potential for improvement in terms of\nimproving speaker similarity and speech naturalness.In this paper, we propose\nTakin-VC, a novel zero-shot VC framework based on jointly hybrid content and\nmemory-augmented context-aware timbre modeling to tackle this challenge.\nSpecifically, an effective hybrid content encoder, guided by neural codec\ntraining, that leverages quantized features from pre-trained WavLM and\nHybridFormer is first presented to extract the linguistic content of the source\nspeech. Subsequently, we introduce an advanced cross-attention-based\ncontext-aware timbre modeling approach that learns the fine-grained,\nsemantically associated target timbre features. To further enhance both speaker\nsimilarity and real-time performance, we utilize a conditional flow matching\nmodel to reconstruct the Mel-spectrogram of the source speech. Additionally, we\nadvocate an efficient memory-augmented module designed to generate high-quality\nconditional target inputs for the flow matching process, thereby improving the\noverall performance of the proposed system. Experimental results demonstrate\nthat the proposed Takin-VC method surpasses state-of-the-art zero-shot VC\nsystems, delivering superior performance in terms of both speech naturalness\nand speaker similarity.\n","authors":["Yuguang Yang","Yu Pan","Jixun Yao","Xiang Zhang","Jianhao Ye","Hongbin Zhou","Lei Xie","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.01350v1.pdf","comment":"Work in Progress; Under Review"},{"id":"http://arxiv.org/abs/2410.00344v2","updated":"2024-10-02T04:06:59Z","published":"2024-10-01T02:43:14Z","title":"Integrating Text-to-Music Models with Language Models: Composing Long\n  Structured Music Pieces","summary":"  Recent music generation methods based on transformers have a context window\nof up to a minute. The music generated by these methods are largely\nunstructured beyond the context window. With a longer context window, learning\nlong scale structures from musical data is a prohibitively challenging problem.\nThis paper proposes integrating a text-to-music model with a large language\nmodel to generate music with form. We discuss our solutions to the challenges\nof such integration. The experimental results show that the proposed method can\ngenerate 2.5-minute-long music that is highly structured, strongly organized,\nand cohesive.\n","authors":["Lilac Atassi"],"pdf_url":"https://arxiv.org/pdf/2410.00344v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.11976"},{"id":"http://arxiv.org/abs/2409.01548v3","updated":"2024-10-02T02:25:30Z","published":"2024-09-03T02:37:34Z","title":"VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for\n  Taiwanese Hakka","summary":"  This paper introduces VoxHakka, a text-to-speech (TTS) system designed for\nTaiwanese Hakka, a critically under-resourced language spoken in Taiwan.\nLeveraging the YourTTS framework, VoxHakka achieves high naturalness and\naccuracy and low real-time factor in speech synthesis while supporting six\ndistinct Hakka dialects. This is achieved by training the model with\ndialect-specific data, allowing for the generation of speaker-aware Hakka\nspeech. To address the scarcity of publicly available Hakka speech corpora, we\nemployed a cost-effective approach utilizing a web scraping pipeline coupled\nwith automatic speech recognition (ASR)-based data cleaning techniques. This\nprocess ensured the acquisition of a high-quality, multi-speaker, multi-dialect\ndataset suitable for TTS training. Subjective listening tests conducted using\ncomparative mean opinion scores (CMOS) demonstrate that VoxHakka significantly\noutperforms existing publicly available Hakka TTS systems in terms of\npronunciation accuracy, tone correctness, and overall naturalness. This work\nrepresents a significant advancement in Hakka language technology and provides\na valuable resource for language preservation and revitalization efforts.\n","authors":["Li-Wei Chen","Hung-Shin Lee","Chen-Chi Chang"],"pdf_url":"https://arxiv.org/pdf/2409.01548v3.pdf","comment":"Accepted to O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2409.18680v2","updated":"2024-10-02T01:45:40Z","published":"2024-09-27T12:06:53Z","title":"Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models","summary":"  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n","authors":["Yiming Chen","Xianghu Yue","Xiaoxue Gao","Chen Zhang","Luis Fernando D'Haro","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2409.18680v2.pdf","comment":"EMNLP24 Findings"},{"id":"http://arxiv.org/abs/2410.01162v1","updated":"2024-10-02T01:32:47Z","published":"2024-10-02T01:32:47Z","title":"Frozen Large Language Models Can Perceive Paralinguistic Aspects of\n  Speech","summary":"  As speech becomes an increasingly common modality for interacting with large\nlanguage models (LLMs), it is becoming desirable to develop systems where LLMs\ncan take into account users' emotions or speaking styles when providing their\nresponses. In this work, we study the potential of an LLM to understand these\naspects of speech without fine-tuning its weights. To do this, we utilize an\nend-to-end system with a speech encoder; the encoder is trained to produce\ntoken embeddings such that the LLM's response to an expressive speech prompt is\naligned with its response to a semantically matching text prompt where the\nspeaker's emotion has also been specified. We find that this training framework\nallows the encoder to generate tokens that capture both semantic and\nparalinguistic information in speech and effectively convey it to the LLM, even\nwhen the LLM remains completely frozen. We also explore training on additional\nemotion and style-related response alignment tasks, finding that they further\nincrease the amount of paralinguistic information explicitly captured in the\nspeech tokens. Experiments demonstrate that our system is able to produce\nhigher quality and more empathetic responses to expressive speech prompts\ncompared to several baselines.\n","authors":["Wonjune Kang","Junteng Jia","Chunyang Wu","Wei Zhou","Egor Lakomkin","Yashesh Gaur","Leda Sari","Suyoun Kim","Ke Li","Jay Mahadeokar","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2410.01162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01150v1","updated":"2024-10-02T01:04:01Z","published":"2024-10-02T01:04:01Z","title":"Restorative Speech Enhancement: A Progressive Approach Using SE and\n  Codec Modules","summary":"  In challenging environments with significant noise and reverberation,\ntraditional speech enhancement (SE) methods often lead to over-suppressed\nspeech, creating artifacts during listening and harming downstream tasks\nperformance. To overcome these limitations, we propose a novel approach called\nRestorative SE (RestSE), which combines a lightweight SE module with a\ngenerative codec module to progressively enhance and restore speech quality.\nThe SE module initially reduces noise, while the codec module subsequently\nperforms dereverberation and restores speech using generative capabilities. We\nsystematically explore various quantization techniques within the codec module\nto optimize performance. Additionally, we introduce a weighted loss function\nand feature fusion that merges the SE output with the original mixture,\nparticularly at segments where the SE output is heavily distorted. Experimental\nresults demonstrate the effectiveness of our proposed method in enhancing\nspeech quality under adverse conditions. Audio demos are available at:\nhttps://sophie091524.github.io/RestorativeSE/.\n","authors":["Hsin-Tien Chiang","Hao Zhang","Yong Xu","Meng Yu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01150v1.pdf","comment":"Paper in submission"},{"id":"http://arxiv.org/abs/2410.02084v1","updated":"2024-10-02T23:10:21Z","published":"2024-10-02T23:10:21Z","title":"Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset","summary":"  Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, we leverage a pretrained large language model (LLM) to generate\npseudo natural language captions from the metadata. With the LLM-enhanced\nMetaScore, we train a text-conditioned music generation model that learns to\ngenerate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test, while the text-based system offers a\nmore natural interface that allows free-form natural language prompts.\n","authors":["Weihan Xu","Julian McAuley","Taylor Berg-Kirkpatrick","Shlomo Dubnov","Hao-Wen Dong"],"pdf_url":"https://arxiv.org/pdf/2410.02084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02060v1","updated":"2024-10-02T22:11:31Z","published":"2024-10-02T22:11:31Z","title":"PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and\n  Variations","summary":"  We introduce Cadenza, a new multi-stage generative framework for predicting\nexpressive variations of symbolic musical ideas as well as unconditional\ngenerations. To accomplish this we propose a novel MIDI encoding method, PerTok\n(Performance Tokenizer) that captures minute expressive details whilst reducing\nsequence length up to 59% and vocabulary size up to 95% for polyphonic,\nmonophonic and rhythmic tasks. The proposed framework comprises of two\nsequential stages: 1) Composer and 2) Performer. The Composer model is a\ntransformer-based Variational Autoencoder (VAE), with Rotary Positional\nEmbeddings (RoPE)ROPE and an autoregressive decoder modified to more\neffectively integrate the latent codes of the input musical idea. The Performer\nmodel is a bidirectional transformer encoder that is separately trained to\npredict velocities and microtimings on MIDI sequences. Objective and human\nevaluations demonstrate Cadenza's versatile capability in 1) matching other\nunconditional state-of-the-art symbolic models in musical quality whilst\nsounding more expressive, and 2) composing new, expressive ideas that are both\nstylistically related to the input whilst providing novel ideas to the user.\nOur framework is designed, researched and implemented with the objective of\nethically providing inspiration for musicians.\n","authors":["Julian Lenz","Anirudh Mani"],"pdf_url":"https://arxiv.org/pdf/2410.02060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18213v3","updated":"2024-10-02T21:00:34Z","published":"2024-09-26T18:46:20Z","title":"A Fly on the Wall -- Exploiting Acoustic Side-Channels in Differential\n  Pressure Sensors","summary":"  Differential Pressure Sensors are widely deployed to monitor critical\nenvironments. However, our research unveils a previously overlooked\nvulnerability: their high sensitivity to pressure variations makes them\nsusceptible to acoustic side-channel attacks. We demonstrate that the\npressure-sensing diaphragms in DPS can inadvertently capture subtle air\nvibrations caused by speech, which propagate through the sensor's components\nand affect the pressure readings. Exploiting this discovery, we introduce\nBaroVox, a novel attack that reconstructs speech from DPS readings, effectively\nturning DPS into a \"fly on the wall.\" We model the effect of sound on DPS,\nexploring the limits and challenges of acoustic leakage. To overcome these\nchallenges, we propose two solutions: a signal-processing approach using a\nunique spectral subtraction method and a deep learning-based approach for\nkeyword classification. Evaluations under various conditions demonstrate\nBaroVox's effectiveness, achieving a word error rate of 0.29 for manual\nrecognition and 90.51% accuracy for automatic recognition. Our findings\nhighlight the significant privacy implications of this vulnerability. We also\ndiscuss potential defense strategies to mitigate the risks posed by BaroVox.\n","authors":["Yonatan Gizachew Achamyeleh","Mohamad Habib Fakih","Gabriel Garcia","Anomadarshi Barua","Mohammad Al Faruque"],"pdf_url":"https://arxiv.org/pdf/2409.18213v3.pdf","comment":"Accepted to ACSAC 2024"},{"id":"http://arxiv.org/abs/2407.15835v2","updated":"2024-10-02T20:38:27Z","published":"2024-07-22T17:51:53Z","title":"dMel: Speech Tokenization made Simple","summary":"  Large language models have revolutionized natural language processing by\nleveraging self-supervised pretraining on vast textual data. Inspired by this\nsuccess, researchers have investigated complicated speech tokenization methods\nto discretize continuous speech signals so that language modeling techniques\ncan be applied to speech data. However, existing approaches either model\nsemantic (content) tokens, potentially losing acoustic information, or model\nacoustic tokens, risking the loss of semantic (content) information. Having\nmultiple token types also complicates the architecture and requires additional\npretraining. Here we show that discretizing mel-filterbank channels into\ndiscrete intensity bins produces a simple representation (dMel), that performs\nbetter than other existing speech tokenization methods. Using an LM-style\ntransformer architecture for speech-text modeling, we comprehensively evaluate\ndifferent speech tokenization methods on speech recognition (ASR) and speech\nsynthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving\nhigh performance on both tasks within a unified framework, paving the way for\nefficient and effective joint modeling of speech and text.\n","authors":["He Bai","Tatiana Likhomanenko","Ruixiang Zhang","Zijin Gu","Zakaria Aldeneh","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2407.15835v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2309.10719v3","updated":"2024-10-02T20:14:46Z","published":"2023-09-18T16:11:48Z","title":"Harmony and Duality: An introduction to Music Theory","summary":"  We develop aspects of music theory related to harmony, such as scales, chord\nformation and improvisation from a combinatorial perspective. The goal is to\nprovide a foundation for this subject by deriving the basic structure from a\nfew assumptions, rather than writing down long lists of chords/scales to\nmemorize without an underlying principle. Our approach involves introducing\nconstraints that limit the possible scales we can consider. For example, we may\nimpose the constraint that two voices cannot be only a semitone apart as this\nis too dissonant. We can then study scales that do not contain notes that are a\nsemitone apart. A more refined constraint avoids three voices colliding by\nstudying scales that do not have three notes separated only by semitones.\nAdditionally, we require that our scales are complete, which roughly means that\nthey are the maximal sets of tones that satisfy these constraints. As it turns\nout, completeness as applied to these simple two/three voice constraints\ncharacterizes the types of scales that are commonly used in music composition.\nSurprisingly, there is a correspondence between scales subject to the two-voice\nconstraint and those subject to the three-voice constraint. We formulate this\ncorrespondence as a duality statement that provides a way to understand scales\nsubject to one type of constraint in terms of scales subject to the other.\nFinally, we combine these constraint ideas to provide a classification of\nchords.\n","authors":["Maksim Lipyanskiy"],"pdf_url":"https://arxiv.org/pdf/2309.10719v3.pdf","comment":"75 pages, 72 figures"},{"id":"http://arxiv.org/abs/2410.03752v1","updated":"2024-10-02T01:54:35Z","published":"2024-10-02T01:54:35Z","title":"Efficient Streaming LLM for Speech Recognition","summary":"  Recent works have shown that prompting large language models with audio\nencodings can unlock speech recognition capabilities. However, existing\ntechniques do not scale efficiently, especially while handling long form\nstreaming audio inputs -- not only do they extrapolate poorly beyond the audio\nlength seen during training, but they are also computationally inefficient due\nto the quadratic cost of attention.\n  In this work, we introduce SpeechLLM-XL, a linear scaling decoder-only model\nfor streaming speech recognition. We process audios in configurable chunks\nusing limited attention window for reduced computation, and the text tokens for\neach audio chunk are generated auto-regressively until an EOS is predicted.\nDuring training, the transcript is segmented into chunks, using a CTC forced\nalignment estimated from encoder output. SpeechLLM-XL with 1.28 seconds chunk\nsize achieves 2.7%/6.7% WER on LibriSpeech test clean/other, and it shows no\nquality degradation on long form utterances 10x longer than the training\nutterances.\n","authors":["Junteng Jia","Gil Keren","Wei Zhou","Egor Lakomkin","Xiaohui Zhang","Chunyang Wu","Frank Seide","Jay Mahadeokar","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2410.03752v1.pdf","comment":null}],"Speech Processing":[{"id":"http://arxiv.org/abs/2410.01562v1","updated":"2024-10-02T14:00:41Z","published":"2024-10-02T14:00:41Z","title":"HRTF Estimation using a Score-based Prior","summary":"  We present a head-related transfer function (HRTF) estimation method which\nrelies on a data-driven prior given by a score-based diffusion model. The HRTF\nis estimated in reverberant environments using natural excitation signals, e.g.\nhuman speech. The impulse response of the room is estimated along with the HRTF\nby optimizing a parametric model of reverberation based on the statistical\nbehaviour of room acoustics. The posterior distribution of HRTF given the\nreverberant measurement and excitation signal is modelled using the score-based\nHRTF prior and a log-likelihood approximation. We show that the resulting\nmethod outperforms several baselines, including an oracle recommender system\nthat assigns the optimal HRTF in our training set based on the smallest\ndistance to the true HRTF at the given direction of arrival. In particular, we\nshow that the diffusion prior can account for the large variability of\nhigh-frequency content in HRTFs.\n","authors":["Etienne Thuillier","Jean-Marie Lemercier","Eloi Moliner","Timo Gerkmann","Vesa Välimäki"],"pdf_url":"https://arxiv.org/pdf/2410.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13049v2","updated":"2024-10-02T13:04:02Z","published":"2024-09-19T18:55:13Z","title":"DiffSSD: A Diffusion-Based Dataset For Speech Forensics","summary":"  Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.\n","authors":["Kratika Bhagtani","Amit Kumar Singh Yadav","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2409.13049v2.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2025"},{"id":"http://arxiv.org/abs/2410.01481v1","updated":"2024-10-02T12:33:59Z","published":"2024-10-02T12:33:59Z","title":"SonicSim: A customizable simulation platform for speech processing in\n  moving sound source scenarios","summary":"  The systematic evaluation of speech separation and enhancement models under\nmoving sound source conditions typically requires extensive data comprising\ndiverse scenarios. However, real-world datasets often contain insufficient data\nto meet the training and evaluation requirements of models. Although synthetic\ndatasets offer a larger volume of data, their acoustic simulations lack\nrealism. Consequently, neither real-world nor synthetic datasets effectively\nfulfill practical needs. To address these issues, we introduce SonicSim, a\nsynthetic toolkit de-designed to generate highly customizable data for moving\nsound sources. SonicSim is developed based on the embodied AI simulation\nplatform, Habitat-sim, supporting multi-level adjustments, including\nscene-level, microphone-level, and source-level, thereby generating more\ndiverse synthetic data. Leveraging SonicSim, we constructed a moving sound\nsource benchmark dataset, SonicSet, using the Librispeech, the Freesound\nDataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the\nMatterport3D to evaluate speech separation and enhancement models.\nAdditionally, to validate the differences between synthetic data and real-world\ndata, we randomly selected 5 hours of raw data without reverberation from the\nSonicSet validation set to record a real-world speech separation dataset, which\nwas then compared with the corresponding synthetic datasets. Similarly, we\nutilized the real-world speech enhancement dataset RealMAN to validate the\nacoustic gap between other synthetic datasets and the SonicSet dataset for\nspeech enhancement. The results indicate that the synthetic data generated by\nSonicSim can effectively generalize to real-world scenarios. Demo and code are\npublicly available at https://cslikai.cn/SonicSim/.\n","authors":["Kai Li","Wendi Sang","Chang Zeng","Runxuan Yang","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01481v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2410.01469v1","updated":"2024-10-02T12:21:06Z","published":"2024-10-02T12:21:06Z","title":"TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for\n  Efficient Speech Separation","summary":"  In recent years, much speech separation research has focused primarily on\nimproving model performance. However, for low-latency speech processing\nsystems, high efficiency is equally important. Therefore, we propose a speech\nseparation model with significantly reduced parameters and computational costs:\nTime-frequency Interleaved Gain Extraction and Reconstruction network (TIGER).\nTIGER leverages prior knowledge to divide frequency bands and compresses\nfrequency information. We employ a multi-scale selective attention module to\nextract contextual features, while introducing a full-frequency-frame attention\nmodule to capture both temporal and frequency contextual information.\nAdditionally, to more realistically evaluate the performance of speech\nseparation models in complex acoustic environments, we introduce a dataset\ncalled EchoSet. This dataset includes noise and more realistic reverberation\n(e.g., considering object occlusions and material properties), with speech from\ntwo speakers overlapping at random proportions. Experimental results showed\nthat models trained on EchoSet had better generalization ability than those\ntrained on other datasets to the data collected in the physical world, which\nvalidated the practical value of the EchoSet. On EchoSet and real-world data,\nTIGER significantly reduces the number of parameters by 94.3% and the MACs by\n95.3% while achieving performance surpassing state-of-the-art (SOTA) model\nTF-GridNet. This is the first speech separation model with fewer than 1 million\nparameters that achieves performance comparable to the SOTA model.\n","authors":["Mohan Xu","Kai Li","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01469v1.pdf","comment":"Technical report, demo page: https://cslikai.cn/TIGER/"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Défossez","Laurent Mazaré","Manu Orsini","Amélie Royer","Patrick Pérez","Hervé Jégou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01350v1","updated":"2024-10-02T09:07:33Z","published":"2024-10-02T09:07:33Z","title":"Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and\n  Memory-Augmented Context-Aware Timbre Modeling","summary":"  Zero-shot voice conversion (VC) aims to transform the source speaker timbre\ninto an arbitrary unseen one without altering the original speech content.While\nrecent advancements in zero-shot VC methods have shown remarkable progress,\nthere still remains considerable potential for improvement in terms of\nimproving speaker similarity and speech naturalness.In this paper, we propose\nTakin-VC, a novel zero-shot VC framework based on jointly hybrid content and\nmemory-augmented context-aware timbre modeling to tackle this challenge.\nSpecifically, an effective hybrid content encoder, guided by neural codec\ntraining, that leverages quantized features from pre-trained WavLM and\nHybridFormer is first presented to extract the linguistic content of the source\nspeech. Subsequently, we introduce an advanced cross-attention-based\ncontext-aware timbre modeling approach that learns the fine-grained,\nsemantically associated target timbre features. To further enhance both speaker\nsimilarity and real-time performance, we utilize a conditional flow matching\nmodel to reconstruct the Mel-spectrogram of the source speech. Additionally, we\nadvocate an efficient memory-augmented module designed to generate high-quality\nconditional target inputs for the flow matching process, thereby improving the\noverall performance of the proposed system. Experimental results demonstrate\nthat the proposed Takin-VC method surpasses state-of-the-art zero-shot VC\nsystems, delivering superior performance in terms of both speech naturalness\nand speaker similarity.\n","authors":["Yuguang Yang","Yu Pan","Jixun Yao","Xiang Zhang","Jianhao Ye","Hongbin Zhou","Lei Xie","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.01350v1.pdf","comment":"Work in Progress; Under Review"},{"id":"http://arxiv.org/abs/2409.04927v3","updated":"2024-10-02T07:58:56Z","published":"2024-09-07T22:54:47Z","title":"Just ASR + LLM? A Study on Speech Large Language Models' Ability to\n  Identify and Understand Speaker in Spoken Dialogue","summary":"  In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. SpeechLLMs have demonstrated impressive spoken dialog\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ntranscript alone, i.e.\\ without speaker segmentation and identification. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered reliably with correct speaker identification. The results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that tasks focused on\nidentity-critical questions could offer a more accurate evaluation framework of\nSpeechLLMs in SQA.\n","authors":["Junkai Wu","Xulin Fan","Bo-Ru Lu","Xilin Jiang","Nima Mesgarani","Mark Hasegawa-Johnson","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2409.04927v3.pdf","comment":"Accepted to IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2410.00344v2","updated":"2024-10-02T04:06:59Z","published":"2024-10-01T02:43:14Z","title":"Integrating Text-to-Music Models with Language Models: Composing Long\n  Structured Music Pieces","summary":"  Recent music generation methods based on transformers have a context window\nof up to a minute. The music generated by these methods are largely\nunstructured beyond the context window. With a longer context window, learning\nlong scale structures from musical data is a prohibitively challenging problem.\nThis paper proposes integrating a text-to-music model with a large language\nmodel to generate music with form. We discuss our solutions to the challenges\nof such integration. The experimental results show that the proposed method can\ngenerate 2.5-minute-long music that is highly structured, strongly organized,\nand cohesive.\n","authors":["Lilac Atassi"],"pdf_url":"https://arxiv.org/pdf/2410.00344v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.11976"},{"id":"http://arxiv.org/abs/2409.01548v3","updated":"2024-10-02T02:25:30Z","published":"2024-09-03T02:37:34Z","title":"VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for\n  Taiwanese Hakka","summary":"  This paper introduces VoxHakka, a text-to-speech (TTS) system designed for\nTaiwanese Hakka, a critically under-resourced language spoken in Taiwan.\nLeveraging the YourTTS framework, VoxHakka achieves high naturalness and\naccuracy and low real-time factor in speech synthesis while supporting six\ndistinct Hakka dialects. This is achieved by training the model with\ndialect-specific data, allowing for the generation of speaker-aware Hakka\nspeech. To address the scarcity of publicly available Hakka speech corpora, we\nemployed a cost-effective approach utilizing a web scraping pipeline coupled\nwith automatic speech recognition (ASR)-based data cleaning techniques. This\nprocess ensured the acquisition of a high-quality, multi-speaker, multi-dialect\ndataset suitable for TTS training. Subjective listening tests conducted using\ncomparative mean opinion scores (CMOS) demonstrate that VoxHakka significantly\noutperforms existing publicly available Hakka TTS systems in terms of\npronunciation accuracy, tone correctness, and overall naturalness. This work\nrepresents a significant advancement in Hakka language technology and provides\na valuable resource for language preservation and revitalization efforts.\n","authors":["Li-Wei Chen","Hung-Shin Lee","Chen-Chi Chang"],"pdf_url":"https://arxiv.org/pdf/2409.01548v3.pdf","comment":"Accepted to O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2409.18680v2","updated":"2024-10-02T01:45:40Z","published":"2024-09-27T12:06:53Z","title":"Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models","summary":"  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n","authors":["Yiming Chen","Xianghu Yue","Xiaoxue Gao","Chen Zhang","Luis Fernando D'Haro","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2409.18680v2.pdf","comment":"EMNLP24 Findings"},{"id":"http://arxiv.org/abs/2410.01162v1","updated":"2024-10-02T01:32:47Z","published":"2024-10-02T01:32:47Z","title":"Frozen Large Language Models Can Perceive Paralinguistic Aspects of\n  Speech","summary":"  As speech becomes an increasingly common modality for interacting with large\nlanguage models (LLMs), it is becoming desirable to develop systems where LLMs\ncan take into account users' emotions or speaking styles when providing their\nresponses. In this work, we study the potential of an LLM to understand these\naspects of speech without fine-tuning its weights. To do this, we utilize an\nend-to-end system with a speech encoder; the encoder is trained to produce\ntoken embeddings such that the LLM's response to an expressive speech prompt is\naligned with its response to a semantically matching text prompt where the\nspeaker's emotion has also been specified. We find that this training framework\nallows the encoder to generate tokens that capture both semantic and\nparalinguistic information in speech and effectively convey it to the LLM, even\nwhen the LLM remains completely frozen. We also explore training on additional\nemotion and style-related response alignment tasks, finding that they further\nincrease the amount of paralinguistic information explicitly captured in the\nspeech tokens. Experiments demonstrate that our system is able to produce\nhigher quality and more empathetic responses to expressive speech prompts\ncompared to several baselines.\n","authors":["Wonjune Kang","Junteng Jia","Chunyang Wu","Wei Zhou","Egor Lakomkin","Yashesh Gaur","Leda Sari","Suyoun Kim","Ke Li","Jay Mahadeokar","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2410.01162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01150v1","updated":"2024-10-02T01:04:01Z","published":"2024-10-02T01:04:01Z","title":"Restorative Speech Enhancement: A Progressive Approach Using SE and\n  Codec Modules","summary":"  In challenging environments with significant noise and reverberation,\ntraditional speech enhancement (SE) methods often lead to over-suppressed\nspeech, creating artifacts during listening and harming downstream tasks\nperformance. To overcome these limitations, we propose a novel approach called\nRestorative SE (RestSE), which combines a lightweight SE module with a\ngenerative codec module to progressively enhance and restore speech quality.\nThe SE module initially reduces noise, while the codec module subsequently\nperforms dereverberation and restores speech using generative capabilities. We\nsystematically explore various quantization techniques within the codec module\nto optimize performance. Additionally, we introduce a weighted loss function\nand feature fusion that merges the SE output with the original mixture,\nparticularly at segments where the SE output is heavily distorted. Experimental\nresults demonstrate the effectiveness of our proposed method in enhancing\nspeech quality under adverse conditions. Audio demos are available at:\nhttps://sophie091524.github.io/RestorativeSE/.\n","authors":["Hsin-Tien Chiang","Hao Zhang","Yong Xu","Meng Yu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01150v1.pdf","comment":"Paper in submission"},{"id":"http://arxiv.org/abs/2410.02084v1","updated":"2024-10-02T23:10:21Z","published":"2024-10-02T23:10:21Z","title":"Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset","summary":"  Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, we leverage a pretrained large language model (LLM) to generate\npseudo natural language captions from the metadata. With the LLM-enhanced\nMetaScore, we train a text-conditioned music generation model that learns to\ngenerate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test, while the text-based system offers a\nmore natural interface that allows free-form natural language prompts.\n","authors":["Weihan Xu","Julian McAuley","Taylor Berg-Kirkpatrick","Shlomo Dubnov","Hao-Wen Dong"],"pdf_url":"https://arxiv.org/pdf/2410.02084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02060v1","updated":"2024-10-02T22:11:31Z","published":"2024-10-02T22:11:31Z","title":"PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and\n  Variations","summary":"  We introduce Cadenza, a new multi-stage generative framework for predicting\nexpressive variations of symbolic musical ideas as well as unconditional\ngenerations. To accomplish this we propose a novel MIDI encoding method, PerTok\n(Performance Tokenizer) that captures minute expressive details whilst reducing\nsequence length up to 59% and vocabulary size up to 95% for polyphonic,\nmonophonic and rhythmic tasks. The proposed framework comprises of two\nsequential stages: 1) Composer and 2) Performer. The Composer model is a\ntransformer-based Variational Autoencoder (VAE), with Rotary Positional\nEmbeddings (RoPE)ROPE and an autoregressive decoder modified to more\neffectively integrate the latent codes of the input musical idea. The Performer\nmodel is a bidirectional transformer encoder that is separately trained to\npredict velocities and microtimings on MIDI sequences. Objective and human\nevaluations demonstrate Cadenza's versatile capability in 1) matching other\nunconditional state-of-the-art symbolic models in musical quality whilst\nsounding more expressive, and 2) composing new, expressive ideas that are both\nstylistically related to the input whilst providing novel ideas to the user.\nOur framework is designed, researched and implemented with the objective of\nethically providing inspiration for musicians.\n","authors":["Julian Lenz","Anirudh Mani"],"pdf_url":"https://arxiv.org/pdf/2410.02060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02056v1","updated":"2024-10-02T22:05:36Z","published":"2024-10-02T22:05:36Z","title":"Synthio: Augmenting Small-Scale Audio Classification Datasets with\n  Synthetic Data","summary":"  We present Synthio, a novel approach for augmenting small-scale audio\nclassification datasets with synthetic data. Our goal is to improve audio\nclassification accuracy with limited labeled data. Traditional data\naugmentation techniques, which apply artificial transformations (e.g., adding\nrandom noise or masking segments), struggle to create data that captures the\ntrue diversity present in real-world audios. To address this shortcoming, we\npropose to augment the dataset with synthetic audio generated from\ntext-to-audio (T2A) diffusion models. However, synthesizing effective\naugmentations is challenging because not only should the generated data be\nacoustically consistent with the underlying small-scale dataset, but they\nshould also have sufficient compositional diversity. To overcome the first\nchallenge, we align the generations of the T2A model with the small-scale\ndataset using preference optimization. This ensures that the acoustic\ncharacteristics of the generated data remain consistent with the small-scale\ndataset. To address the second challenge, we propose a novel caption generation\ntechnique that leverages the reasoning capabilities of Large Language Models to\n(1) generate diverse and meaningful audio captions and (2) iteratively refine\ntheir quality. The generated captions are then used to prompt the aligned T2A\nmodel. We extensively evaluate Synthio on ten datasets and four simulated\nlimited-data settings. Results indicate our method consistently outperforms all\nbaselines by 0.1%-39% using a T2A model trained only on weakly-captioned\nAudioSet.\n","authors":["Sreyan Ghosh","Sonal Kumar","Zhifeng Kong","Rafael Valle","Bryan Catanzaro","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2410.02056v1.pdf","comment":"Code and Checkpoints will be soon available here:\n  https://github.com/Sreyan88/Synthio"},{"id":"http://arxiv.org/abs/2409.18213v3","updated":"2024-10-02T21:00:34Z","published":"2024-09-26T18:46:20Z","title":"A Fly on the Wall -- Exploiting Acoustic Side-Channels in Differential\n  Pressure Sensors","summary":"  Differential Pressure Sensors are widely deployed to monitor critical\nenvironments. However, our research unveils a previously overlooked\nvulnerability: their high sensitivity to pressure variations makes them\nsusceptible to acoustic side-channel attacks. We demonstrate that the\npressure-sensing diaphragms in DPS can inadvertently capture subtle air\nvibrations caused by speech, which propagate through the sensor's components\nand affect the pressure readings. Exploiting this discovery, we introduce\nBaroVox, a novel attack that reconstructs speech from DPS readings, effectively\nturning DPS into a \"fly on the wall.\" We model the effect of sound on DPS,\nexploring the limits and challenges of acoustic leakage. To overcome these\nchallenges, we propose two solutions: a signal-processing approach using a\nunique spectral subtraction method and a deep learning-based approach for\nkeyword classification. Evaluations under various conditions demonstrate\nBaroVox's effectiveness, achieving a word error rate of 0.29 for manual\nrecognition and 90.51% accuracy for automatic recognition. Our findings\nhighlight the significant privacy implications of this vulnerability. We also\ndiscuss potential defense strategies to mitigate the risks posed by BaroVox.\n","authors":["Yonatan Gizachew Achamyeleh","Mohamad Habib Fakih","Gabriel Garcia","Anomadarshi Barua","Mohammad Al Faruque"],"pdf_url":"https://arxiv.org/pdf/2409.18213v3.pdf","comment":"Accepted to ACSAC 2024"},{"id":"http://arxiv.org/abs/2407.15835v2","updated":"2024-10-02T20:38:27Z","published":"2024-07-22T17:51:53Z","title":"dMel: Speech Tokenization made Simple","summary":"  Large language models have revolutionized natural language processing by\nleveraging self-supervised pretraining on vast textual data. Inspired by this\nsuccess, researchers have investigated complicated speech tokenization methods\nto discretize continuous speech signals so that language modeling techniques\ncan be applied to speech data. However, existing approaches either model\nsemantic (content) tokens, potentially losing acoustic information, or model\nacoustic tokens, risking the loss of semantic (content) information. Having\nmultiple token types also complicates the architecture and requires additional\npretraining. Here we show that discretizing mel-filterbank channels into\ndiscrete intensity bins produces a simple representation (dMel), that performs\nbetter than other existing speech tokenization methods. Using an LM-style\ntransformer architecture for speech-text modeling, we comprehensively evaluate\ndifferent speech tokenization methods on speech recognition (ASR) and speech\nsynthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving\nhigh performance on both tasks within a unified framework, paving the way for\nefficient and effective joint modeling of speech and text.\n","authors":["He Bai","Tatiana Likhomanenko","Ruixiang Zhang","Zijin Gu","Zakaria Aldeneh","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2407.15835v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2309.10719v3","updated":"2024-10-02T20:14:46Z","published":"2023-09-18T16:11:48Z","title":"Harmony and Duality: An introduction to Music Theory","summary":"  We develop aspects of music theory related to harmony, such as scales, chord\nformation and improvisation from a combinatorial perspective. The goal is to\nprovide a foundation for this subject by deriving the basic structure from a\nfew assumptions, rather than writing down long lists of chords/scales to\nmemorize without an underlying principle. Our approach involves introducing\nconstraints that limit the possible scales we can consider. For example, we may\nimpose the constraint that two voices cannot be only a semitone apart as this\nis too dissonant. We can then study scales that do not contain notes that are a\nsemitone apart. A more refined constraint avoids three voices colliding by\nstudying scales that do not have three notes separated only by semitones.\nAdditionally, we require that our scales are complete, which roughly means that\nthey are the maximal sets of tones that satisfy these constraints. As it turns\nout, completeness as applied to these simple two/three voice constraints\ncharacterizes the types of scales that are commonly used in music composition.\nSurprisingly, there is a correspondence between scales subject to the two-voice\nconstraint and those subject to the three-voice constraint. We formulate this\ncorrespondence as a duality statement that provides a way to understand scales\nsubject to one type of constraint in terms of scales subject to the other.\nFinally, we combine these constraint ideas to provide a classification of\nchords.\n","authors":["Maksim Lipyanskiy"],"pdf_url":"https://arxiv.org/pdf/2309.10719v3.pdf","comment":"75 pages, 72 figures"},{"id":"http://arxiv.org/abs/2410.03752v1","updated":"2024-10-02T01:54:35Z","published":"2024-10-02T01:54:35Z","title":"Efficient Streaming LLM for Speech Recognition","summary":"  Recent works have shown that prompting large language models with audio\nencodings can unlock speech recognition capabilities. However, existing\ntechniques do not scale efficiently, especially while handling long form\nstreaming audio inputs -- not only do they extrapolate poorly beyond the audio\nlength seen during training, but they are also computationally inefficient due\nto the quadratic cost of attention.\n  In this work, we introduce SpeechLLM-XL, a linear scaling decoder-only model\nfor streaming speech recognition. We process audios in configurable chunks\nusing limited attention window for reduced computation, and the text tokens for\neach audio chunk are generated auto-regressively until an EOS is predicted.\nDuring training, the transcript is segmented into chunks, using a CTC forced\nalignment estimated from encoder output. SpeechLLM-XL with 1.28 seconds chunk\nsize achieves 2.7%/6.7% WER on LibriSpeech test clean/other, and it shows no\nquality degradation on long form utterances 10x longer than the training\nutterances.\n","authors":["Junteng Jia","Gil Keren","Wei Zhou","Egor Lakomkin","Xiaohui Zhang","Chunyang Wu","Frank Seide","Jay Mahadeokar","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2410.03752v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.01806v1","updated":"2024-10-02T17:59:57Z","published":"2024-10-02T17:59:57Z","title":"Samba: Synchronized Set-of-Sequences Modeling for Multiple Object\n  Tracking","summary":"  Multiple object tracking in complex scenarios - such as coordinated dance\nperformances, team sports, or dynamic animal groups - presents unique\nchallenges. In these settings, objects frequently move in coordinated patterns,\nocclude each other, and exhibit long-term dependencies in their trajectories.\nHowever, it remains a key open research question on how to model long-range\ndependencies within tracklets, interdependencies among tracklets, and the\nassociated temporal occlusions. To this end, we introduce Samba, a novel\nlinear-time set-of-sequences model designed to jointly process multiple\ntracklets by synchronizing the multiple selective state-spaces used to model\neach tracklet. Samba autoregressively predicts the future track query for each\nsequence while maintaining synchronized long-term memory representations across\ntracklets. By integrating Samba into a tracking-by-propagation framework, we\npropose SambaMOTR, the first tracker effectively addressing the aforementioned\nissues, including long-range dependencies, tracklet interdependencies, and\ntemporal occlusions. Additionally, we introduce an effective technique for\ndealing with uncertain observations (MaskObs) and an efficient training recipe\nto scale SambaMOTR to longer sequences. By modeling long-range dependencies and\ninteractions among tracked objects, SambaMOTR implicitly learns to track\nobjects accurately through occlusions without any hand-crafted heuristics. Our\napproach significantly surpasses prior state-of-the-art on the DanceTrack, BFT,\nand SportsMOT datasets.\n","authors":["Mattia Segu","Luigi Piccinelli","Siyuan Li","Yung-Hsu Yang","Bernt Schiele","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2410.01806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01801v1","updated":"2024-10-02T17:57:12Z","published":"2024-10-02T17:57:12Z","title":"FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images","summary":"  We introduce FabricDiffusion, a method for transferring fabric textures from\na single clothing image to 3D garments of arbitrary shapes. Existing approaches\ntypically synthesize textures on the garment surface through 2D-to-3D texture\nmapping or depth-aware inpainting via generative models. Unfortunately, these\nmethods often struggle to capture and preserve texture details, particularly\ndue to challenging occlusions, distortions, or poses in the input image.\nInspired by the observation that in the fashion industry, most garments are\nconstructed by stitching sewing patterns with flat, repeatable textures, we\ncast the task of clothing texture transfer as extracting distortion-free,\ntileable texture materials that are subsequently mapped onto the UV space of\nthe garment. Building upon this insight, we train a denoising diffusion model\nwith a large-scale synthetic dataset to rectify distortions in the input\ntexture image. This process yields a flat texture map that enables a tight\ncoupling with existing Physically-Based Rendering (PBR) material generation\npipelines, allowing for realistic relighting of the garment under various\nlighting conditions. We show that FabricDiffusion can transfer various features\nfrom a single clothing image including texture patterns, material properties,\nand detailed prints and logos. Extensive experiments demonstrate that our model\nsignificantly outperforms state-to-the-art methods on both synthetic data and\nreal-world, in-the-wild clothing images while generalizing to unseen textures\nand garment shapes.\n","authors":["Cheng Zhang","Yuanhao Wang","Francisco Vicente Carrasco","Chenglei Wu","Jinlong Yang","Thabo Beeler","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2410.01801v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024. Project page:\n  https://humansensinglab.github.io/fabric-diffusion"},{"id":"http://arxiv.org/abs/2403.17918v2","updated":"2024-10-02T17:56:21Z","published":"2024-03-26T17:54:15Z","title":"AgentStudio: A Toolkit for Building General Virtual Agents","summary":"  General virtual agents need to handle multimodal observations, master complex\naction spaces, and self-improve in dynamic, open-domain environments. However,\nexisting environments are often domain-specific and require complex setups,\nwhich limits agent development and evaluation in real-world settings. As a\nresult, current evaluations lack in-depth analyses that decompose fundamental\nagent capabilities. We introduce AgentStudio, a trinity of environments, tools,\nand benchmarks to address these issues. AgentStudio provides a lightweight,\ninteractive environment with highly generic observation and action spaces,\ne.g., video observations and GUI/API actions. It integrates tools for creating\nonline benchmark tasks, annotating GUI elements, and labeling actions in\nvideos. Based on our environment and tools, we curate an online task suite that\nbenchmarks both GUI interactions and function calling with efficient\nauto-evaluation. We also reorganize existing datasets and collect new ones\nusing our tools to establish three datasets: GroundUI, IDMBench, and\nCriticBench. These datasets evaluate fundamental agent abilities, including GUI\ngrounding, learning from videos, and success detection, pointing to the\ndesiderata for robust, general, and open-ended virtual agents.\n","authors":["Longtao Zheng","Zhiyuan Huang","Zhenghai Xue","Xinrun Wang","Bo An","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17918v2.pdf","comment":"42 pages, 22 figures, 15 tables"},{"id":"http://arxiv.org/abs/2410.01798v1","updated":"2024-10-02T17:55:46Z","published":"2024-10-02T17:55:46Z","title":"Windowed MAPF with Completeness Guarantees","summary":"  Traditional multi-agent path finding (MAPF) methods try to compute entire\nstart-goal paths which are collision free. However, computing an entire path\ncan take too long for MAPF systems where agents need to replan fast. Methods\nthat address this typically employ a \"windowed\" approach and only try to find\ncollision free paths for a small windowed timestep horizon. This adaptation\ncomes at the cost of incompleteness; all current windowed approaches can become\nstuck in deadlock or livelock. Our main contribution is to introduce our\nframework, WinC-MAPF, for Windowed MAPF that enables completeness. Our\nframework uses heuristic update insights from single-agent real-time heuristic\nsearch algorithms as well as agent independence ideas from MAPF algorithms. We\nalso develop Single-Step CBS (SS-CBS), an instantiation of this framework using\na novel modification to CBS. We show how SS-CBS, which only plans a single step\nand updates heuristics, can effectively solve tough scenarios where existing\nwindowed approaches fail.\n","authors":["Rishi Veerapaneni","Muhammad Suhail Saleem","Jiaoyang Li","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2410.01798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08710v2","updated":"2024-10-02T17:54:17Z","published":"2024-04-11T19:13:24Z","title":"Do Large Language Models Learn Human-Like Strategic Preferences?","summary":"  In this paper, we evaluate whether LLMs learn to make human-like preference\njudgements in strategic scenarios as compared with known empirical results.\nSolar and Mistral are shown to exhibit stable value-based preference consistent\nwith humans and exhibit human-like preference for cooperation in the prisoner's\ndilemma (including stake-size effect) and traveler's dilemma (including\npenalty-size effect). We establish a relationship between model size,\nvalue-based preference, and superficiality. Finally, results here show that\nmodels tending to be less brittle have relied on sliding window attention\nsuggesting a potential link. Additionally, we contribute a novel method for\nconstructing preference relations from arbitrary LLMs and support for a\nhypothesis regarding human behavior in the traveler's dilemma.\n","authors":["Jesse Roberts","Kyle Moore","Doug Fisher"],"pdf_url":"https://arxiv.org/pdf/2404.08710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01792v1","updated":"2024-10-02T17:50:19Z","published":"2024-10-02T17:50:19Z","title":"When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1","summary":"  In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.\n","authors":["R. Thomas McCoy","Shunyu Yao","Dan Friedman","Mathew D. Hardy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2410.01792v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.01791v1","updated":"2024-10-02T17:49:07Z","published":"2024-10-02T17:49:07Z","title":"DreamGarden: A Designer Assistant for Growing Games from a Single Prompt","summary":"  Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.\n","authors":["Sam Earle","Samyak Parajuli","Andrzej Banburski-Fahey"],"pdf_url":"https://arxiv.org/pdf/2410.01791v1.pdf","comment":"21 pages + appendix, 11 figures"},{"id":"http://arxiv.org/abs/2410.01789v1","updated":"2024-10-02T17:46:22Z","published":"2024-10-02T17:46:22Z","title":"Investigating on RLHF methodology","summary":"  In this article, we investigate the alignment of Large Language Models\naccording to human preferences. We discuss the features of training a\nPreference Model, which simulates human preferences, and the methods and\ndetails we found essential for achieving the best results. We also discuss\nusing Reinforcement Learning to fine-tune Large Language Models and describe\nthe challenges we faced and the ways to overcome them. Additionally, we present\nour experience with the Direct Preference Optimization method, which enables us\nto align a Large Language Model with human preferences without creating a\nseparate Preference Model. As our contribution, we introduce the approach for\ncollecting a preference dataset through perplexity filtering, which makes the\nprocess of creating such a dataset for a specific Language Model much easier\nand more cost-effective.\n","authors":["Alexey Kutalev","Sergei Markoff"],"pdf_url":"https://arxiv.org/pdf/2410.01789v1.pdf","comment":"23 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02449v2","updated":"2024-10-02T17:40:25Z","published":"2024-09-04T05:08:23Z","title":"What is lost in Normalization? Exploring Pitfalls in Multilingual ASR\n  Model Evaluations","summary":"  This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.\n","authors":["Kavya Manohar","Leena G Pillai"],"pdf_url":"https://arxiv.org/pdf/2409.02449v2.pdf","comment":"Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.00274v2","updated":"2024-10-02T17:34:41Z","published":"2024-09-30T23:02:51Z","title":"Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds","summary":"  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n","authors":["Amina Kobenova","Cyan DeVeaux","Samyak Parajuli","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2410.00274v2.pdf","comment":"27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12492v2","updated":"2024-10-02T17:29:54Z","published":"2024-07-17T11:18:49Z","title":"Temporal Test-Time Adaptation with State-Space Models","summary":"  Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.\n","authors":["Mona Schirmer","Dan Zhang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2407.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01772v1","updated":"2024-10-02T17:29:34Z","published":"2024-10-02T17:29:34Z","title":"DeFine: Enhancing LLM Decision-Making with Factor Profiles and\n  Analogical Reasoning","summary":"  LLMs are ideal for decision-making due to their ability to reason over long\ncontexts and identify critical factors. However, challenges arise when\nprocessing transcripts of spoken speech describing complex scenarios. These\ntranscripts often contain ungrammatical or incomplete sentences, repetitions,\nhedging, and vagueness. For example, during a company's earnings call, an\nexecutive might project a positive revenue outlook to reassure investors,\ndespite significant uncertainty regarding future earnings. It is crucial for\nLLMs to incorporate this uncertainty systematically when making decisions. In\nthis paper, we introduce DeFine, a new framework that constructs probabilistic\nfactor profiles from complex scenarios. DeFine then integrates these profiles\nwith analogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in novel situations. Our framework\nseparates the tasks of quantifying uncertainty in complex scenarios and\nincorporating it into LLM decision-making. This approach is particularly useful\nin fields such as medical consultations, negotiations, and political debates,\nwhere making decisions under uncertainty is vital.\n","authors":["Yebowen Hu","Xiaoyang Wang","Wenlin Yao","Yiming Lu","Daoan Zhang","Hassan Foroosh","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17253v2","updated":"2024-10-02T17:21:47Z","published":"2024-08-30T12:51:55Z","title":"VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters","summary":"  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time-series domain, the proposed VisionTS could achieve\nsuperior zero-shot forecasting performance compared to existing TSF foundation\nmodels. With fine-tuning for one epoch, VisionTS could further improve the\nforecasting and achieve state-of-the-art performance in most cases. Extensive\nexperiments reveal intrinsic similarities between images and real-world time\nseries, suggesting visual models may offer a ``free lunch'' for TSF and\nhighlight the potential for future cross-modality research. Our code is\npublicly available at https://github.com/Keytoyze/VisionTS.\n","authors":["Mouxiang Chen","Lefei Shen","Zhuo Li","Xiaoyun Joy Wang","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17253v2.pdf","comment":"v2: add more experiments"},{"id":"http://arxiv.org/abs/2403.05334v2","updated":"2024-10-02T17:05:24Z","published":"2024-03-08T14:10:25Z","title":"WatChat: Explaining perplexing programs by debugging mental models","summary":"  Often, a good explanation for a program's unexpected behavior is a bug in the\nprogrammer's code. But sometimes, an even better explanation is a bug in the\nprogrammer's mental model of the language or API they are using. Instead of\nmerely debugging our current code (\"giving the programmer a fish\"), what if our\ntools could directly debug our mental models (\"teaching the programmer to\nfish\")? In this paper, we apply recent ideas from computational cognitive\nscience to offer a principled framework for doing exactly that. Given a \"why?\"\nquestion about a program, we automatically infer potential misconceptions about\nthe language/API that might cause the user to be surprised by the program's\nbehavior -- and then analyze those misconceptions to provide explanations of\nthe program's behavior. Our key idea is to formally represent misconceptions as\ncounterfactual (erroneous) semantics for the language/API, which can be\ninferred and debugged using program synthesis techniques. We demonstrate our\nframework, WatChat, by building systems for explanation in two domains:\nJavaScript type coercion, and the Git version control system. We evaluate\nWatChatJS and WatChatGit by comparing their outputs to experimentally-collected\nhuman-written explanations in these two domains: we show that WatChat's\nexplanations exhibit key features of human-written explanation, unlike those of\na state-of-the-art language model.\n","authors":["Kartik Chandra","Katherine M. Collins","Will Crichton","Tony Chen","Tzu-Mao Li","Adrian Weller","Rachit Nigam","Joshua Tenenbaum","Jonathan Ragan-Kelley"],"pdf_url":"https://arxiv.org/pdf/2403.05334v2.pdf","comment":"This is a preprint of work presented in early-stage non-archival form\n  at the ACL Natural Language Reasoning and Structured Explanations Workshop"},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04194v2","updated":"2024-10-02T17:01:58Z","published":"2024-09-06T11:24:25Z","title":"Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models","summary":"  Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.\n","authors":["Malte Luttermann","Ralf Möller","Mattis Hartwig"],"pdf_url":"https://arxiv.org/pdf/2409.04194v2.pdf","comment":"Accepted to the Proceedings of the 47th German Conference on\n  Artificial Intelligence (KI 2024)"},{"id":"http://arxiv.org/abs/2402.19085v2","updated":"2024-10-02T16:54:33Z","published":"2024-02-29T12:12:30Z","title":"Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment","summary":"  Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nPareto improvements in multi-objective alignment.\n","authors":["Yiju Guo","Ganqu Cui","Lifan Yuan","Ning Ding","Zexu Sun","Bowen Sun","Huimin Chen","Ruobing Xie","Jie Zhou","Yankai Lin","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19085v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2312.15561v4","updated":"2024-10-02T16:52:30Z","published":"2023-12-24T23:01:00Z","title":"README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP","summary":"  The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.\n","authors":["Zonghai Yao","Nandyala Siddharth Kantu","Guanghao Wei","Hieu Tran","Zhangqi Duan","Sunjae Kwon","Zhichao Yang","README annotation team","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.15561v4.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01739v1","updated":"2024-10-02T16:50:29Z","published":"2024-10-02T16:50:29Z","title":"Mimicking Human Intuition: Cognitive Belief-Driven Q-Learning","summary":"  Reinforcement learning encounters challenges in various environments related\nto robustness and explainability. Traditional Q-learning algorithms cannot\neffectively make decisions and utilize the historical learning experience. To\novercome these limitations, we propose Cognitive Belief-Driven Q-Learning\n(CBDQ), which integrates subjective belief modeling into the Q-learning\nframework, enhancing decision-making accuracy by endowing agents with\nhuman-like learning and reasoning capabilities. Drawing inspiration from\ncognitive science, our method maintains a subjective belief distribution over\nthe expectation of actions, leveraging a cluster-based subjective belief model\nthat enables agents to reason about the potential probability associated with\neach decision. CBDQ effectively mitigates overestimated phenomena and optimizes\ndecision-making policies by integrating historical experiences with current\ncontextual information, mimicking the dynamics of human decision-making. We\nevaluate the proposed method on discrete control benchmark tasks in various\ncomplicate environments. The results demonstrate that CBDQ exhibits stronger\nadaptability, robustness, and human-like characteristics in handling these\nenvironments, outperforming other baselines. We hope this work will give\nresearchers a fresh perspective on understanding and explaining Q-learning.\n","authors":["Xingrui Gu","Guanren Qiao","Chuyi Jiang","Tianqing Xia","Hangyu Mao"],"pdf_url":"https://arxiv.org/pdf/2410.01739v1.pdf","comment":"Under review by ICLR 25"},{"id":"http://arxiv.org/abs/2410.01738v1","updated":"2024-10-02T16:48:47Z","published":"2024-10-02T16:48:47Z","title":"VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch\n  Diffusion Models","summary":"  Artistic typography is a technique to visualize the meaning of input\ncharacter in an imaginable and readable manner. With powerful text-to-image\ndiffusion models, existing methods directly design the overall geometry and\ntexture of input character, making it challenging to ensure both creativity and\nlegibility. In this paper, we introduce a dual-branch and training-free method,\nnamely VitaGlyph, enabling flexible artistic typography along with controllable\ngeometry change to maintain the readability. The key insight of VitaGlyph is to\ntreat input character as a scene composed of Subject and Surrounding, followed\nby rendering them under varying degrees of geometry transformation. The subject\nflexibly expresses the essential concept of input character, while the\nsurrounding enriches relevant background without altering the shape.\nSpecifically, we implement VitaGlyph through a three-phase framework: (i)\nKnowledge Acquisition leverages large language models to design text\ndescriptions of subject and surrounding. (ii) Regional decomposition detects\nthe part that most matches the subject description and divides input glyph\nimage into subject and surrounding regions. (iii) Typography Stylization\nfirstly refines the structure of subject region via Semantic Typography, and\nthen separately renders the textures of Subject and Surrounding regions through\nControllable Compositional Generation. Experimental results demonstrate that\nVitaGlyph not only achieves better artistry and readability, but also manages\nto depict multiple customize concepts, facilitating more creative and pleasing\nartistic typography generation. Our code will be made publicly at\nhttps://github.com/Carlofkl/VitaGlyph.\n","authors":["Kailai Feng","Yabo Zhang","Haodong Yu","Zhilong Ji","Jinfeng Bai","Hongzhi Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2410.01738v1.pdf","comment":"https://github.com/Carlofkl/VitaGlyph"},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09828v2","updated":"2024-10-02T16:42:46Z","published":"2024-09-15T19:04:50Z","title":"Latent Diffusion Models for Controllable RNA Sequence Generation","summary":"  This work presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences of variable lengths. RNA is a key\nintermediary between DNA and protein, exhibiting high sequence diversity and\ncomplex three-dimensional structures to support a wide range of functions. We\nutilize pretrained BERT-type models to encode raw RNA sequences into\ntoken-level, biologically meaningful representations. A Query Transformer is\nemployed to compress such representations into a set of fixed-length latent\nvectors, with an autoregressive decoder trained to reconstruct RNA sequences\nfrom these latent variables. We then develop a continuous diffusion model\nwithin this latent space. To enable optimization, we integrate the gradients of\nreward models--surrogates for RNA functional properties--into the backward\ndiffusion process, thereby generating RNAs with high reward scores. Empirical\nresults confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological metrics. Further, we fine-tune\nthe diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize\nsequences for high translation efficiencies. Our guided diffusion model\neffectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and\nTranslation Efficiency (TE), outperforming baselines in balancing rewards and\nstructural stability trade-off. Our findings hold potential for advancing RNA\nsequence-function research and therapeutic RNA design.\n","authors":["Kaixuan Huang","Yukang Yang","Kaidi Fu","Yanyi Chu","Le Cong","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14012v2","updated":"2024-10-02T16:40:10Z","published":"2024-09-21T04:40:08Z","title":"Test Time Learning for Time Series Forecasting","summary":"  Time-series forecasting has seen significant advancements with the\nintroduction of token prediction mechanisms such as multi-head attention.\nHowever, these methods often struggle to achieve the same performance as in\nlanguage modeling, primarily due to the quadratic computational cost and the\ncomplexity of capturing long-range dependencies in time-series data.\nState-space models (SSMs), such as Mamba, have shown promise in addressing\nthese challenges by offering efficient solutions with linear RNNs capable of\nmodeling long sequences with larger context windows. However, there remains\nroom for improvement in accuracy and scalability.\n  We propose the use of Test-Time Training (TTT) modules in a parallel\narchitecture to enhance performance in long-term time series forecasting.\nThrough extensive experiments on standard benchmark datasets, we demonstrate\nthat TTT modules consistently outperform state-of-the-art models, including the\nMamba-based TimeMachine, particularly in scenarios involving extended sequence\nand prediction lengths. Our results show significant improvements in Mean\nSquared Error (MSE) and Mean Absolute Error (MAE), especially on larger\ndatasets such as Electricity, Traffic, and Weather, underscoring the\neffectiveness of TTT in capturing long-range dependencies. Additionally, we\nexplore various convolutional architectures within the TTT framework, showing\nthat even simple configurations like 1D convolution with small filters can\nachieve competitive results. This work sets a new benchmark for time-series\nforecasting and lays the groundwork for future research in scalable,\nhigh-performance forecasting models.\n","authors":["Panayiotis Christou","Shichu Chen","Xupeng Chen","Parijat Dube"],"pdf_url":"https://arxiv.org/pdf/2409.14012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01724v1","updated":"2024-10-02T16:34:40Z","published":"2024-10-02T16:34:40Z","title":"Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for\n  Enhanced Batch Prompting","summary":"  Batch prompting is a common technique in large language models (LLMs) used to\nprocess multiple inputs simultaneously, aiming to improve computational\nefficiency. However, as batch sizes increase, performance degradation often\noccurs due to the model's difficulty in handling lengthy context inputs.\nExisting methods that attempt to mitigate these issues rely solely on batch\ndata arrangement and majority voting rather than improving the design of the\nbatch prompt itself. In this paper, we address these limitations by proposing\n\"Auto-Demo Prompting,\" a novel approach that leverages the question-output\npairs from earlier questions within a batch as demonstrations for subsequent\nanswer inference. We provide a formal theoretical analysis of how Auto-Demo\nPrompting functions within the autoregressive generation process of LLMs,\nillustrating how it utilizes prior outputs to optimize the model's internal\nrepresentations. Our method effectively bridges the gap between batch prompting\nand few-shot prompting, enhancing performance with only a slight compromise in\ntoken usage. Experimental results across five NLP tasks demonstrate its\neffectiveness in mitigating performance degradation and occasionally\noutperforming single prompts. Furthermore, it opens new avenues for applying\nfew-shot learning techniques, such as demonstration selection, within batch\nprompting, making it a robust solution for real-world applications.\n","authors":["Longyu Feng","Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v2","updated":"2024-10-02T16:30:34Z","published":"2024-01-01T13:53:53Z","title":"LogicAsker: Evaluating and Improving the Logical Reasoning Ability of\n  Large Language Models","summary":"  We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01706v1","updated":"2024-10-02T16:15:26Z","published":"2024-10-02T16:15:26Z","title":"Performant, Memory Efficient and Scalable Multi-Agent Reinforcement\n  Learning","summary":"  As the field of multi-agent reinforcement learning (MARL) progresses towards\nlarger and more complex environments, achieving strong performance while\nmaintaining memory efficiency and scalability to many agents becomes\nincreasingly important. Although recent research has led to several advanced\nalgorithms, to date, none fully address all of these key properties\nsimultaneously. In this work, we introduce Sable, a novel and theoretically\nsound algorithm that adapts the retention mechanism from Retentive Networks to\nMARL. Sable's retention-based sequence modelling architecture allows for\ncomputationally efficient scaling to a large number of agents, as well as\nmaintaining a long temporal context, making it well-suited for large-scale\npartially observable environments. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in the majority of tasks (34 out\nof 45, roughly 75\\%). Furthermore, Sable demonstrates stable performance as we\nscale the number of agents, handling environments with more than a thousand\nagents while exhibiting a linear increase in memory usage. Finally, we conduct\nablation studies to isolate the source of Sable's performance gains and confirm\nits efficient computational memory usage. Our results highlight Sable's\nperformance and efficiency, positioning it as a leading approach to MARL at\nscale.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01696v1","updated":"2024-10-02T16:05:01Z","published":"2024-10-02T16:05:01Z","title":"CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving\n  Long-Range Reasoning Problems using LLMs","summary":"  Large language models (LLMs) have demonstrated limitations in handling\ncombinatorial optimization problems involving long-range reasoning, partially\ndue to causal hallucinations and huge search space. As for causal\nhallucinations, i.e., the inconsistency between reasoning and corresponding\nstate transition, this paper introduces the Causal Relationship Enhancement\n(CRE) mechanism combining cause-effect interventions and the Individual\nTreatment Effect (ITE) to guarantee the solid causal rightness between each\nstep of reasoning and state transition. As for the long causal range and huge\nsearch space limiting the performances of existing models featuring\nsingle-direction search, a Dual-End Searching (DES) approach is proposed to\nseek solutions by simultaneously starting from both the initial and goal states\non the causal probability tree. By integrating CRE and DES (CreDes), our model\nhas realized simultaneous multi-step reasoning, circumventing the\ninefficiencies from cascading multiple one-step reasoning like the\nChain-of-Thought (CoT). Experiments demonstrate that CreDes significantly\noutperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning\ntasks in terms of both accuracy and time efficiency.\n","authors":["Kangsheng Wang","Xiao Zhang","Hao Liu","Songde Han","Huimin Ma","Tianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01695v1","updated":"2024-10-02T16:04:33Z","published":"2024-10-02T16:04:33Z","title":"From Prohibition to Adoption: How Hong Kong Universities Are Navigating\n  ChatGPT in Academic Workflows","summary":"  This paper aims at comparing the time when Hong Kong universities used to ban\nChatGPT to the current periods where it has become integrated in the academic\nprocesses. Bolted by concerns of integrity and ethical issues in technologies,\ninstitutions have adapted by moving towards the center adopting AI literacy and\nresponsibility policies. This study examines new paradigms which have been\ndeveloped to help implement these positives while preventing negative effects\non academia. Keywords: ChatGPT, Academic Integrity, AI Literacy, Ethical AI\nUse, Generative AI in Education, University Policy, AI Integration in Academia,\nHigher Education and Technology\n","authors":["Junjun Huang","Jifan Wu","Qing Wang","Kemeng Yuan","Jiefeng Li","Di Lu"],"pdf_url":"https://arxiv.org/pdf/2410.01695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v1","updated":"2024-10-02T16:03:49Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where performance seems to stagnate at first and then\nimprove sharply and unpredictably with scale beyond a threshold. By dividing\nquestions in the datasets according to difficulty level by average performance,\nwe observe U-shaped scaling for hard questions, and inverted-U scaling followed\nby steady improvement for easy questions. Moreover, the emergence threshold\nroughly coincides with the point at which performance on easy questions reverts\nfrom inverse scaling to standard scaling. Capitalizing on the observable though\nopposing scaling trend on easy and hard questions, we propose a simple yet\neffective pipeline, called Slice-and-Sandwich, to predict both the emergence\nthreshold and model performance beyond the threshold.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.01691v1","updated":"2024-10-02T16:03:13Z","published":"2024-10-02T16:03:13Z","title":"FactAlign: Long-form Factuality Alignment of Large Language Models","summary":"  Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01691v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.01690v1","updated":"2024-10-02T16:02:02Z","published":"2024-10-02T16:02:02Z","title":"Why context matters in VQA and Reasoning: Semantic interventions for VLM\n  input modalities","summary":"  The various limitations of Generative AI, such as hallucinations and model\nfailures, have made it crucial to understand the role of different modalities\nin Visual Language Model (VLM) predictions. Our work investigates how the\nintegration of information from image and text modalities influences the\nperformance and behavior of VLMs in visual question answering (VQA) and\nreasoning tasks. We measure this effect through answer accuracy, reasoning\nquality, model uncertainty, and modality relevance. We study the interplay\nbetween text and image modalities in different configurations where visual\ncontent is essential for solving the VQA task. Our contributions include (1)\nthe Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various\nVLM architectures under different modality configurations, and (3) the\nInteractive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the\nfoundation for the benchmark, while the ISI tool provides an interface to test\nand apply semantic interventions in image and text inputs, enabling more\nfine-grained analysis. Our results show that complementary information between\nmodalities improves answer and reasoning quality, while contradictory\ninformation harms model performance and confidence. Image text annotations have\nminimal impact on accuracy and uncertainty, slightly increasing image\nrelevance. Attention analysis confirms the dominant role of image inputs over\ntext in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow\nus to extract attention coefficients for each modality. A key finding is\nPaliGemma's harmful overconfidence, which poses a higher risk of silent\nfailures compared to the LLaVA models. This work sets the foundation for\nrigorous analysis of modality integration, supported by datasets specifically\ndesigned for this purpose.\n","authors":["Kenza Amara","Lukas Klein","Carsten Lüth","Paul Jäger","Hendrik Strobelt","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2410.01690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03807v2","updated":"2024-10-02T16:00:39Z","published":"2024-06-06T07:30:14Z","title":"Tool-Planner: Task Planning with Clusters across Multiple Tools","summary":"  Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\n\\url{https://github.com/OceannTwT/Tool-Planner}\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yuwei Zhang","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2406.03807v2.pdf","comment":"48pages second version"},{"id":"http://arxiv.org/abs/2409.19546v3","updated":"2024-10-02T15:57:57Z","published":"2024-09-29T04:16:24Z","title":"Almost Sure Convergence of Average Reward Temporal Difference Learning","summary":"  Tabular average reward Temporal Difference (TD) learning is perhaps the\nsimplest and the most fundamental policy evaluation algorithm in average reward\nreinforcement learning. After at least 25 years since its discovery, we are\nfinally able to provide a long-awaited almost sure convergence analysis.\nNamely, we are the first to prove that, under very mild conditions, tabular\naverage reward TD converges almost surely to a sample path dependent fixed\npoint. Key to this success is a new general stochastic approximation result\nconcerning nonexpansive mappings with Markovian and additive noise, built on\nrecent advances in stochastic Krasnoselskii-Mann iterations.\n","authors":["Ethan Blaser","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19546v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01687v1","updated":"2024-10-02T15:57:18Z","published":"2024-10-02T15:57:18Z","title":"Uncertainty Quantification with Bayesian Higher Order ReLU KANs","summary":"  We introduce the first method of uncertainty quantification in the domain of\nKolmogorov-Arnold Networks, specifically focusing on (Higher Order) ReLUKANs to\nenhance computational efficiency given the computational demands of Bayesian\nmethods. The method we propose is general in nature, providing access to both\nepistemic and aleatoric uncertainties. It is also capable of generalization to\nother various basis functions. We validate our method through a series of\nclosure tests, including simple one-dimensional functions and application to\nthe domain of (Stochastic) Partial Differential Equations. Referring to the\nlatter, we demonstrate the method's ability to correctly identify functional\ndependencies introduced through the inclusion of a stochastic term. The code\nsupporting this work can be found at\nhttps://github.com/wmdataphys/Bayesian-HR-KAN\n","authors":["James Giroux","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2410.01687v1.pdf","comment":"13 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2410.01686v1","updated":"2024-10-02T15:55:08Z","published":"2024-10-02T15:55:08Z","title":"Positional Attention: Out-of-Distribution Generalization and\n  Expressivity for Neural Algorithmic Reasoning","summary":"  There has been a growing interest in the ability of neural networks to solve\nalgorithmic tasks, such as arithmetic, summary statistics, and sorting. While\nstate-of-the-art models like Transformers have demonstrated good generalization\nperformance on in-distribution tasks, their out-of-distribution (OOD)\nperformance is poor when trained end-to-end. In this paper, we focus on value\ngeneralization, a common instance of OOD generalization where the test\ndistribution has the same input sequence length as the training distribution,\nbut the value ranges in the training and test distributions do not necessarily\noverlap. To address this issue, we propose that using fixed positional\nencodings to determine attention weights-referred to as positional\nattention-enhances empirical OOD performance while maintaining expressivity. We\nsupport our claim about expressivity by proving that Transformers with\npositional attention can effectively simulate parallel algorithms.\n","authors":["Artur Back de Luca","George Giapitzakis","Shenghao Yang","Petar Veličković","Kimon Fountoulakis"],"pdf_url":"https://arxiv.org/pdf/2410.01686v1.pdf","comment":"37 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.01680v1","updated":"2024-10-02T15:50:35Z","published":"2024-10-02T15:50:35Z","title":"PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation","summary":"  Various visual foundation models have distinct strengths and weaknesses, both\nof which can be improved through heterogeneous multi-teacher knowledge\ndistillation without labels, termed \"agglomerative models.\" We build upon this\nbody of work by studying the effect of the teachers' activation statistics,\nparticularly the impact of the loss function on the resulting student model\nquality. We explore a standard toolkit of statistical normalization techniques\nto better align the different distributions and assess their effects. Further,\nwe examine the impact on downstream teacher-matching metrics, which motivates\nthe use of Hadamard matrices. With these matrices, we demonstrate useful\nproperties, showing how they can be used for isotropic standardization, where\neach dimension of a multivariate distribution is standardized using the same\nscale. We call this technique \"PHI Standardization\" (PHI-S) and empirically\ndemonstrate that it produces the best student model across the suite of methods\nstudied.\n","authors":["Mike Ranzinger","Jon Barker","Greg Heinrich","Pavlo Molchanov","Bryan Catanzaro","Andrew Tao"],"pdf_url":"https://arxiv.org/pdf/2410.01680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01677v1","updated":"2024-10-02T15:47:25Z","published":"2024-10-02T15:47:25Z","title":"Mind Scramble: Unveiling Large Language Model Psychology Via\n  Typoglycemia","summary":"  Research into the external behaviors and internal mechanisms of large\nlanguage models (LLMs) has shown promise in addressing complex tasks in the\nphysical world. Studies suggest that powerful LLMs, like GPT-4, are beginning\nto exhibit human-like cognitive abilities, including planning, reasoning, and\nreflection. In this paper, we introduce a research line and methodology called\nLLM Psychology, leveraging human psychology experiments to investigate the\ncognitive behaviors and mechanisms of LLMs. We migrate the Typoglycemia\nphenomenon from psychology to explore the \"mind\" of LLMs. Unlike human brains,\nwhich rely on context and word patterns to comprehend scrambled text, LLMs use\ndistinct encoding and decoding processes. Through Typoglycemia experiments at\nthe character, word, and sentence levels, we observe: (I) LLMs demonstrate\nhuman-like behaviors on a macro scale, such as lower task accuracy and higher\ntoken/time consumption; (II) LLMs exhibit varying robustness to scrambled\ninput, making Typoglycemia a benchmark for model evaluation without new\ndatasets; (III) Different task types have varying impacts, with complex logical\ntasks (e.g., math) being more challenging in scrambled form; (IV) Each LLM has\na unique and consistent \"cognitive pattern\" across tasks, revealing general\nmechanisms in its psychology process. We provide an in-depth analysis of hidden\nlayers to explain these phenomena, paving the way for future research in LLM\nPsychology and deeper interpretability.\n","authors":["Miao Yu","Junyuan Mao","Guibin Zhang","Jingheng Ye","Junfeng Fang","Aoxiao Zhong","Yang Liu","Yuxuan Liang","Kun Wang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.01677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01675v1","updated":"2024-10-02T15:46:40Z","published":"2024-10-02T15:46:40Z","title":"Trying to be human: Linguistic traces of stochastic empathy in language\n  models","summary":"  Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.\n","authors":["Bennett Kleinberg","Jari Zegers","Jonas Festor","Stefana Vida","Julian Präsent","Riccardo Loconte","Sanne Peereboom"],"pdf_url":"https://arxiv.org/pdf/2410.01675v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.01671v1","updated":"2024-10-02T15:39:55Z","published":"2024-10-02T15:39:55Z","title":"Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yanxin Shen","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2410.01671v1.pdf","comment":"Underreview version of LQCA, Bridge context gap for long context"},{"id":"http://arxiv.org/abs/2410.01665v1","updated":"2024-10-02T15:32:01Z","published":"2024-10-02T15:32:01Z","title":"Towards a vision foundation model for comprehensive assessment of\n  Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (CMR), considered the gold standard for\nnoninvasive cardiac assessment, is a diverse and complex modality requiring a\nwide variety of image processing tasks for comprehensive assessment of cardiac\nmorphology and function. Advances in deep learning have enabled the development\nof state-of-the-art (SoTA) models for these tasks. However, model training is\nchallenging due to data and label scarcity, especially in the less common\nimaging sequences. Moreover, each model is often trained for a specific task,\nwith no connection between related tasks. In this work, we introduce a vision\nfoundation model trained for CMR assessment, that is trained in a\nself-supervised fashion on 36 million CMR images. We then finetune the model in\nsupervised way for 9 clinical tasks typical to a CMR workflow, across\nclassification, segmentation, landmark localization, and pathology detection.\nWe demonstrate improved accuracy and robustness across all tasks, over a range\nof available labeled dataset sizes. We also demonstrate improved few-shot\nlearning with fewer labeled samples, a common challenge in medical image\nanalyses. We achieve an out-of-box performance comparable to SoTA for most\nclinical tasks. The proposed method thus presents a resource-efficient, unified\nframework for CMR assessment, with the potential to accelerate the development\nof deep learning-based solutions for image analysis tasks, even with few\nannotated data available.\n","authors":["Athira J Jacob","Indraneel Borgohain","Teodora Chitiboi","Puneet Sharma","Dorin Comaniciu","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2410.01665v1.pdf","comment":"11 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.01661v1","updated":"2024-10-02T15:29:42Z","published":"2024-10-02T15:29:42Z","title":"Finding path and cycle counting formulae in graphs with Deep\n  Reinforcement Learning","summary":"  This paper presents Grammar Reinforcement Learning (GRL), a reinforcement\nlearning algorithm that uses Monte Carlo Tree Search (MCTS) and a transformer\narchitecture that models a Pushdown Automaton (PDA) within a context-free\ngrammar (CFG) framework. Taking as use case the problem of efficiently counting\npaths and cycles in graphs, a key challenge in network analysis, computer\nscience, biology, and social sciences, GRL discovers new matrix-based formulas\nfor path/cycle counting that improve computational efficiency by factors of two\nto six w.r.t state-of-the-art approaches. Our contributions include: (i) a\nframework for generating gramformers that operate within a CFG, (ii) the\ndevelopment of GRL for optimizing formulas within grammatical structures, and\n(iii) the discovery of novel formulas for graph substructure counting, leading\nto significant computational improvements.\n","authors":["Jason Piquenot","Maxime Bérar","Pierre Héroux","Jean-Yves Ramel","Romain Raveaux","Sébastien Adam"],"pdf_url":"https://arxiv.org/pdf/2410.01661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01660v1","updated":"2024-10-02T15:26:52Z","published":"2024-10-02T15:26:52Z","title":"Conformal Generative Modeling with Improved Sample Efficiency through\n  Sequential Greedy Filtering","summary":"  Generative models lack rigorous statistical guarantees for their outputs and\nare therefore unreliable in safety-critical applications. In this work, we\npropose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a\nsequential conformal prediction method producing prediction sets that satisfy a\nrigorous statistical guarantee called conformal admissibility control. This\nguarantee states that with high probability, the prediction sets contain at\nleast one admissible (or valid) example. To this end, our method first samples\nan initial set of i.i.d. examples from a black box generative model. Then, this\nset is iteratively pruned via so-called greedy filters. As a consequence of the\niterative generation procedure, admissibility of the final prediction set\nfactorizes as a Markov chain. This factorization is crucial, because it allows\nto control each factor separately, using conformal prediction. In comparison to\nprior work, our method demonstrates a large reduction in the number of\nadmissibility evaluations during calibration. This reduction is important in\nsafety-critical applications, where these evaluations must be conducted\nmanually by domain experts and are therefore costly and time consuming. We\nhighlight the advantages of our method in terms of admissibility evaluations\nand cardinality of the prediction sets through experiments in natural language\ngeneration and molecular graph extension tasks.\n","authors":["Klaus-Rudolf Kladny","Bernhard Schölkopf","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2410.01660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00118v3","updated":"2024-10-02T15:22:49Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Léonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ramé","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogozińska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Plucińska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Görner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Cogan","Sarah Perrin","Sébastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01651v1","updated":"2024-10-02T15:18:34Z","published":"2024-10-02T15:18:34Z","title":"Efficient Long-range Language Modeling with Self-supervised Causal\n  Retrieval","summary":"  Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.\n","authors":["Xiang Hu","Zhihao Teng","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.01651v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.14302v2","updated":"2024-10-02T15:17:35Z","published":"2024-09-22T03:13:38Z","title":"Reliable and diverse evaluation of LLM medical knowledge mastery","summary":"  Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.\n","authors":["Yuxuan Zhou","Xien Liu","Chen Ning","Xiao Zhang","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.14302v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.01649v1","updated":"2024-10-02T15:16:53Z","published":"2024-10-02T15:16:53Z","title":"shapiq: Shapley Interactions for Machine Learning","summary":"  Originally rooted in game theory, the Shapley Value (SV) has recently become\nan important tool in machine learning research. Perhaps most notably, it is\nused for feature attribution and data valuation in explainable artificial\nintelligence. Shapley Interactions (SIs) naturally extend the SV and address\nits limitations by assigning joint contributions to groups of entities, which\nenhance understanding of black box machine learning models. Due to the\nexponential complexity of computing SVs and SIs, various methods have been\nproposed that exploit structural assumptions or yield probabilistic estimates\ngiven limited resources. In this work, we introduce shapiq, an open-source\nPython package that unifies state-of-the-art algorithms to efficiently compute\nSVs and any-order SIs in an application-agnostic framework. Moreover, it\nincludes a benchmarking suite containing 11 machine learning applications of\nSIs with pre-computed games and ground-truth values to systematically assess\ncomputational performance across domains. For practitioners, shapiq is able to\nexplain and visualize any-order feature interactions in predictions of models,\nincluding vision transformers, language models, as well as XGBoost and LightGBM\nwith TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and\nconsolidate the application of SVs and SIs in machine learning that facilitates\nfuture research. The source code and documentation are available at\nhttps://github.com/mmschlk/shapiq.\n","authors":["Maximilian Muschalik","Hubert Baniecki","Fabian Fumagalli","Patrick Kolpaczki","Barbara Hammer","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2410.01649v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01643v1","updated":"2024-10-02T15:13:25Z","published":"2024-10-02T15:13:25Z","title":"Stable Offline Value Function Learning with Bisimulation-based\n  Representations","summary":"  In reinforcement learning, offline value function learning is the procedure\nof using an offline dataset to estimate the expected discounted return from\neach state when taking actions according to a fixed target policy. The\nstability of this procedure, i.e., whether it converges to its fixed-point,\ncritically depends on the representations of the state-action pairs. Poorly\nlearned representations can make value function learning unstable, or even\ndivergent. Therefore, it is critical to stabilize value function learning by\nexplicitly shaping the state-action representations. Recently, the class of\nbisimulation-based algorithms have shown promise in shaping representations for\ncontrol. However, it is still unclear if this class of methods can stabilize\nvalue function learning. In this work, we investigate this question and answer\nit affirmatively. We introduce a bisimulation-based algorithm called kernel\nrepresentations for offline policy evaluation (KROPE). KROPE uses a kernel to\nshape state-action representations such that state-action pairs that have\nsimilar immediate rewards and lead to similar next state-action pairs under the\ntarget policy also have similar representations. We show that KROPE: 1) learns\nstable representations and 2) leads to lower value error than baselines. Our\nanalysis provides new theoretical insight into the stability properties of\nbisimulation-based methods and suggests that practitioners can use these\nmethods for stable and accurate evaluation of offline reinforcement learning\nagents.\n","authors":["Brahma S. Pavse","Yudong Chen","Qiaomin Xie","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2410.01643v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.01639v1","updated":"2024-10-02T15:09:36Z","published":"2024-10-02T15:09:36Z","title":"Moral Alignment for LLM Agents","summary":"  Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2410.01639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01638v1","updated":"2024-10-02T15:08:47Z","published":"2024-10-02T15:08:47Z","title":"Data Extrapolation for Text-to-image Generation on Small Datasets","summary":"  Text-to-image generation requires large amount of training data to\nsynthesizing high-quality images. For augmenting training data, previous\nmethods rely on data interpolations like cropping, flipping, and mixing up,\nwhich fail to introduce new information and yield only marginal improvements.\nIn this paper, we propose a new data augmentation method for text-to-image\ngeneration using linear extrapolation. Specifically, we apply linear\nextrapolation only on text feature, and new image data are retrieved from the\ninternet by search engines. For the reliability of new text-image pairs, we\ndesign two outlier detectors to purify retrieved images. Based on\nextrapolation, we construct training samples dozens of times larger than the\noriginal dataset, resulting in a significant improvement in text-to-image\nperformance. Moreover, we propose a NULL-guidance to refine score estimation,\nand apply recurrent affine transformation to fuse text information. Our model\nachieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO\ndatasets. The code and data will be available on GitHub\n(https://github.com/senmaoy/RAT-Diffusion).\n","authors":["Senmao Ye","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01635v1","updated":"2024-10-02T15:07:13Z","published":"2024-10-02T15:07:13Z","title":"Does Graph Prompt Work? A Data Operation Perspective with Theoretical\n  Analysis","summary":"  In recent years, graph prompting has emerged as a promising research\ndirection, enabling the learning of additional tokens or subgraphs appended to\nthe original graphs without requiring retraining of pre-trained graph models\nacross various applications. This novel paradigm, shifting from the traditional\npretraining and finetuning to pretraining and prompting has shown significant\nempirical success in simulating graph data operations, with applications\nranging from recommendation systems to biological networks and graph\ntransferring. However, despite its potential, the theoretical underpinnings of\ngraph prompting remain underexplored, raising critical questions about its\nfundamental effectiveness. The lack of rigorous theoretical proof of why and\nhow much it works is more like a dark cloud over the graph prompt area to go\nfurther. To fill this gap, this paper introduces a theoretical framework that\nrigorously analyzes graph prompting from a data operation perspective. Our\ncontributions are threefold: First, we provide a formal guarantee theorem,\ndemonstrating graph prompts capacity to approximate graph transformation\noperators, effectively linking upstream and downstream tasks. Second, we derive\nupper bounds on the error of these data operations by graph prompts for a\nsingle graph and extend this discussion to batches of graphs, which are common\nin graph model training. Third, we analyze the distribution of data operation\nerrors, extending our theoretical findings from linear graph models (e.g., GCN)\nto non-linear graph models (e.g., GAT). Extensive experiments support our\ntheoretical results and confirm the practical implications of these guarantees.\n","authors":["Qunzhong Wang","Xiangguo Sun","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01628v1","updated":"2024-10-02T15:02:32Z","published":"2024-10-02T15:02:32Z","title":"Entropy-Based Uncertainty Modeling for Trajectory Prediction in\n  Autonomous Driving","summary":"  In autonomous driving, accurate motion prediction is essential for safe and\nefficient motion planning. To ensure safety, planners must rely on reliable\nuncertainty information about the predicted future behavior of surrounding\nagents, yet this aspect has received limited attention. This paper addresses\nthe so-far neglected problem of uncertainty modeling in trajectory prediction.\nWe adopt a holistic approach that focuses on uncertainty quantification,\ndecomposition, and the influence of model composition. Our method is based on a\ntheoretically grounded information-theoretic approach to measure uncertainty,\nallowing us to decompose total uncertainty into its aleatoric and epistemic\ncomponents. We conduct extensive experiments on the nuScenes dataset to assess\nhow different model architectures and configurations affect uncertainty\nquantification and model robustness.\n","authors":["Aron Distelzweig","Andreas Look","Eitan Kosman","Faris Janjoš","Jörg Wagner","Abhinav Valadaa"],"pdf_url":"https://arxiv.org/pdf/2410.01628v1.pdf","comment":"10 pages, 5 figures, submitted to International Conference on\n  Learning Representations (2025)"},{"id":"http://arxiv.org/abs/2409.07480v2","updated":"2024-10-02T15:02:13Z","published":"2024-09-02T10:03:03Z","title":"EEG-Language Modeling for Pathology Detection","summary":"  Multimodal language modeling constitutes a recent breakthrough which\nleverages advances in large language models to pretrain capable multimodal\nmodels. The integration of natural language during pretraining has been shown\nto significantly improve learned representations, particularly in computer\nvision. However, the efficacy of multimodal language modeling in the realm of\nfunctional brain data, specifically for advancing pathology detection, remains\nunexplored. This study pioneers EEG-language models trained on clinical reports\nand 15000 EEGs. We extend methods for multimodal alignment to this novel domain\nand investigate which textual information in reports is useful for training\nEEG-language models. Our results indicate that models learn richer\nrepresentations from being exposed to a variety of report segments, including\nthe patient's clinical history, description of the EEG, and the physician's\ninterpretation. Compared to models exposed to narrower clinical text\ninformation, we find such models to retrieve EEGs based on clinical reports\n(and vice versa) with substantially higher accuracy. Yet, this is only observed\nwhen using a contrastive learning approach. Particularly in regimes with few\nannotations, we observe that representations of EEG-language models can\nsignificantly improve pathology detection compared to those of EEG-only models,\nas demonstrated by both zero-shot classification and linear probes. In sum,\nthese results highlight the potential of integrating brain activity data with\nclinical text, suggesting that EEG-language models represent significant\nprogress for clinical applications.\n","authors":["Sam Gijsen","Kerstin Ritter"],"pdf_url":"https://arxiv.org/pdf/2409.07480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01623v1","updated":"2024-10-02T14:58:27Z","published":"2024-10-02T14:58:27Z","title":"Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank\n  Constraint?","summary":"  Low-rank training has emerged as a promising approach for reducing memory\nusage in training Large Language Models (LLMs). Previous methods either rely on\ndecomposing weight matrices (e.g., LoRA), or seek to decompose gradient\nmatrices (e.g., GaLore) to ensure reduced memory consumption. However, both of\nthem constrain the training in a low-rank subspace, thus inevitably leading to\nsub-optimal performance. This raises a question: whether it is possible to\nconsistently preserve the low-rank constraint for memory efficiency, while\nachieving full-rank training (i.e., training with full-rank gradients of\nfull-rank weights) to avoid inferior outcomes? In this paper, we propose a new\nplug-and-play training framework for LLMs called Fira, as the first attempt to\nachieve this goal. First, we observe an interesting phenomenon during LLM\ntraining: the scaling impact of adaptive optimizers (e.g., Adam) on the\ngradient norm remains similar from low-rank to full-rank training. Based on\nthis observation, we propose a norm-based scaling method, which utilizes the\nscaling impact of low-rank optimizers as substitutes for that of original\nfull-rank optimizers to enable full-rank training. In this way, we can preserve\nthe low-rank constraint in the optimizer while achieving full-rank training for\nbetter performance. Moreover, we find that there are sudden gradient rises\nduring the optimization process, potentially causing loss spikes. To address\nthis, we further put forward a norm-growth limiter to smooth the gradient via\nregulating the relative increase of gradient norms. Extensive experiments on\nthe pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA\nand GaLore, achieving performance that is comparable to or even better than\nfull-rank training.\n","authors":["Xi Chen","Kaituo Feng","Changsheng Li","Xunhao Lai","Xiangyu Yue","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01623v1.pdf","comment":"Code is available at: https://github.com/xichen-fy/Fira"},{"id":"http://arxiv.org/abs/2405.14953v3","updated":"2024-10-02T14:56:33Z","published":"2024-05-23T18:01:11Z","title":"MallowsPO: Fine-Tune Your LLM with Preference Dispersions","summary":"  Direct Preference Optimization (DPO) has recently emerged as a popular\napproach to improve reinforcement learning with human feedback (RLHF), leading\nto better techniques to fine-tune large language models (LLM). A weakness of\nDPO, however, lies in its lack of capability to characterize the diversity of\nhuman preferences. Inspired by Mallows' theory of preference ranking, we\ndevelop in this paper a new approach, the MallowsPO. A distinct feature of this\napproach is a dispersion index, which reflects the dispersion of human\npreference to prompts. We show that existing DPO models can be reduced to\nspecial cases of this dispersion index, thus unified with MallowsPO. More\nimportantly, we demonstrate (empirically) how to use this dispersion index to\nenhance the performance of DPO in a broad array of benchmark tasks, from\nsynthetic bandit selection to controllable generations and dialogues, while\nmaintaining great generalization capabilities. MallowsPO is also compatible\nwith other SOTA offline preference optimization methods, boosting nearly 2\\%\nextra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.\n","authors":["Haoxian Chen","Hanyang Zhao","Henry Lam","David Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2405.14953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03869v2","updated":"2024-10-02T14:52:13Z","published":"2024-04-05T03:02:57Z","title":"Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable\n  Collaboration","summary":"  The emergence of multi-agent reinforcement learning (MARL) is significantly\ntransforming various fields like autonomous vehicle networks. However,\nreal-world multi-agent systems typically contain multiple roles, and the scale\nof these systems dynamically fluctuates. Consequently, in order to achieve\nzero-shot scalable collaboration, it is essential that strategies for different\nroles can be updated flexibly according to the scales, which is still a\nchallenge for current MARL frameworks. To address this, we propose a novel MARL\nframework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. We first leverage a latent network to learn strategy patterns for\neach agent adaptively. Second, we introduce a heterogeneous layer to be\ninserted into decision-making networks, whose parameters are specifically\ngenerated by the learned latent variables. Our approach is scalable as all the\nparameters are shared except for the heterogeneous layer, and gains both\ninter-individual and temporal heterogeneity, allowing SHPPO to adapt\neffectively to varying scales. SHPPO exhibits superior performance in classic\nMARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google\nResearch Football (GRF), showcasing enhanced zero-shot scalability, and\noffering insights into the learned latent variables' impact on team performance\nby visualization.\n","authors":["Xudong Guo","Daming Shi","Junjie Yu","Wenhui Fan"],"pdf_url":"https://arxiv.org/pdf/2404.03869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01611v1","updated":"2024-10-02T14:49:05Z","published":"2024-10-02T14:49:05Z","title":"DRUPI: Dataset Reduction Using Privileged Information","summary":"  Dataset reduction (DR) seeks to select or distill samples from large datasets\ninto smaller subsets while preserving performance on target tasks. Existing\nmethods primarily focus on pruning or synthesizing data in the same format as\nthe original dataset, typically the input data and corresponding labels.\nHowever, in DR settings, we find it is possible to synthesize more information\nbeyond the data-label pair as an additional learning target to facilitate model\ntraining. In this paper, we introduce Dataset Reduction Using Privileged\nInformation (DRUPI), which enriches DR by synthesizing privileged information\nalongside the reduced dataset. This privileged information can take the form of\nfeature labels or attention labels, providing auxiliary supervision to improve\nmodel learning. Our findings reveal that effective feature labels must balance\nbetween being overly discriminative and excessively diverse, with a moderate\nlevel proving optimal for improving the reduced dataset's efficacy. Extensive\nexperiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI\nintegrates seamlessly with existing dataset reduction methods, offering\nsignificant performance gains.\n","authors":["Shaobo Wang","Yantai Yang","Shuaiyu Zhang","Chenghao Sun","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01610v1","updated":"2024-10-02T14:48:22Z","published":"2024-10-02T14:48:22Z","title":"Upcycling Instruction Tuning from Dense to Mixture-of-Experts via\n  Parameter Merging","summary":"  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.\n","authors":["Tingfeng Hui","Zhenyu Zhang","Shuohuan Wang","Yu Sun","Hua Wu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2410.01610v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.01606v1","updated":"2024-10-02T14:47:05Z","published":"2024-10-02T14:47:05Z","title":"Automated Red Teaming with GOAT: the Generative Offensive Agent Tester","summary":"  Red teaming assesses how large language models (LLMs) can produce content\nthat violates norms, policies, and rules set during their safety training.\nHowever, most existing automated methods in the literature are not\nrepresentative of the way humans tend to interact with AI models. Common users\nof AI models may not have advanced knowledge of adversarial machine learning\nmethods or access to model internals, and they do not spend a lot of time\ncrafting a single highly effective adversarial prompt. Instead, they are likely\nto make use of techniques commonly shared online and exploit the multiturn\nconversational nature of LLMs. While manual testing addresses this gap, it is\nan inefficient and often expensive process. To address these limitations, we\nintroduce the Generative Offensive Agent Tester (GOAT), an automated agentic\nred teaming system that simulates plain language adversarial conversations\nwhile leveraging multiple adversarial prompting techniques to identify\nvulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by\nprompting a general-purpose model in a way that encourages reasoning through\nthe choices of methods available, the current target model's response, and the\nnext steps. Our approach is designed to be extensible and efficient, allowing\nhuman testers to focus on exploring new areas of risk while automation covers\nthe scaled adversarial stress-testing of known risk territory. We present the\ndesign and evaluation of GOAT, demonstrating its effectiveness in identifying\nvulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama\n3.1 and 88% against GPT-4 on the JailbreakBench dataset.\n","authors":["Maya Pavlova","Erik Brinkman","Krithika Iyer","Vitor Albiero","Joanna Bitton","Hailey Nguyen","Joe Li","Cristian Canton Ferrer","Ivan Evtimov","Aaron Grattafiori"],"pdf_url":"https://arxiv.org/pdf/2410.01606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16869v2","updated":"2024-10-02T14:42:10Z","published":"2024-05-27T06:36:17Z","title":"Multiple Heads are Better than One: Mixture of Modality Knowledge\n  Experts for Entity Representation Learning","summary":"  Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Binbin Hu","Ziqi Liu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.16869v2.pdf","comment":"Work in progress. Code and data will be released at\n  https://github.com/zjukg/MoMoK"},{"id":"http://arxiv.org/abs/2405.02355v2","updated":"2024-10-02T14:37:01Z","published":"2024-05-03T02:48:55Z","title":"CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation","summary":"  Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. The implementation is\navailable at https://anonymous.4open.science/r/Code-5970/.\n","authors":["Kounianhua Du","Jizheng Chen","Renting Rui","Huacan Chai","Lingyue Fu","Wei Xia","Yasheng Wang","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.02355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01598v1","updated":"2024-10-02T14:36:18Z","published":"2024-10-02T14:36:18Z","title":"Elaborative Subtopic Query Reformulation for Broad and Indirect Queries\n  in Travel Destination Recommendation","summary":"  In Query-driven Travel Recommender Systems (RSs), it is crucial to understand\nthe user intent behind challenging natural language(NL) destination queries\nsuch as the broadly worded \"youth-friendly activities\" or the indirect\ndescription \"a high school graduation trip\". Such queries are challenging due\nto the wide scope and subtlety of potential user intents that confound the\nability of retrieval methods to infer relevant destinations from available\ntextual descriptions such as WikiVoyage. While query reformulation (QR) has\nproven effective in enhancing retrieval by addressing user intent, existing QR\nmethods tend to focus only on expanding the range of potentially matching query\nsubtopics (breadth) or elaborating on the potential meaning of a query (depth),\nbut not both. In this paper, we introduce Elaborative Subtopic Query\nReformulation (EQR), a large language model-based QR method that combines both\nbreadth and depth by generating potential query subtopics with information-rich\nelaborations. We also release TravelDest, a novel dataset for query-driven\ntravel destination RSs. Experiments on TravelDest show that EQR achieves\nsignificant improvements in recall and precision over existing state-of-the-art\nQR methods.\n","authors":["Qianfeng Wen","Yifan Liu","Joshua Zhang","George Saad","Anton Korikov","Yury Sambale","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2410.01598v1.pdf","comment":"9 pages, 7 figures,The 1st Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommender Systems (ROEGEN@RecSys 2024),\n  October 2024, Bari, Italy"},{"id":"http://arxiv.org/abs/2405.15317v3","updated":"2024-10-02T14:34:08Z","published":"2024-05-24T07:59:02Z","title":"NuwaTS: a Foundation Model Mending Every Incomplete Time Series","summary":"  Time series imputation is critical for many real-world applications and has\nbeen widely studied. However, existing models often require specialized designs\ntailored to specific missing patterns, variables, or domains which limits their\ngeneralizability. In addition, current evaluation frameworks primarily focus on\ndomain-specific tasks and often rely on time-wise train/validation/test data\nsplits, which fail to rigorously assess a model's ability to generalize across\nunseen variables or domains. In this paper, we present \\textbf{NuwaTS}, a novel\nframework that repurposes Pre-trained Language Models (PLMs) for general time\nseries imputation. Once trained, NuwaTS can be applied to impute missing data\nacross any domain. We introduce specialized embeddings for each sub-series\npatch, capturing information about the patch, its missing data patterns, and\nits statistical characteristics. By combining contrastive learning with the\nimputation task, we train PLMs to create a versatile, one-for-all imputation\nmodel. Additionally, we employ a plug-and-play fine-tuning approach, enabling\nefficient adaptation to domain-specific tasks with minimal adjustments. To\nevaluate cross-variable and cross-domain generalization, we propose a new\nbenchmarking protocol that partitions the datasets along the variable\ndimension. Experimental results on over seventeen million time series samples\nfrom diverse domains demonstrate that NuwaTS outperforms state-of-the-art\ndomain-specific models across various datasets under the proposed benchmarking\nprotocol. Furthermore, we show that NuwaTS generalizes to other time series\ntasks, such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n","authors":["Jinguo Cheng","Chunwei Yang","Wanlin Cai","Yuxuan Liang","Qingsong Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2405.15317v3.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.01595v1","updated":"2024-10-02T14:33:12Z","published":"2024-10-02T14:33:12Z","title":"KnobGen: Controlling the Sophistication of Artwork in Sketch-Based\n  Diffusion Models","summary":"  Recent advances in diffusion models have significantly improved text-to-image\n(T2I) generation, but they often struggle to balance fine-grained precision\nwith high-level control. Methods like ControlNet and T2I-Adapter excel at\nfollowing sketches by seasoned artists but tend to be overly rigid, replicating\nunintentional flaws in sketches from novice users. Meanwhile, coarse-grained\nmethods, such as sketch-based abstraction frameworks, offer more accessible\ninput handling but lack the precise control needed for detailed, professional\nuse. To address these limitations, we propose KnobGen, a dual-pathway framework\nthat democratizes sketch-based image generation by seamlessly adapting to\nvarying levels of sketch complexity and user skill. KnobGen uses a\nCoarse-Grained Controller (CGC) module for high-level semantics and a\nFine-Grained Controller (FGC) module for detailed refinement. The relative\nstrength of these two modules can be adjusted through our knob inference\nmechanism to align with the user's specific needs. These mechanisms ensure that\nKnobGen can flexibly generate images from both novice sketches and those drawn\nby seasoned artists. This maintains control over the final output while\npreserving the natural appearance of the image, as evidenced on the\nMultiGen-20M dataset and a newly collected sketch dataset.\n","authors":["Pouyan Navard","Amin Karimi Monsefi","Mengxi Zhou","Wei-Lun Chao","Alper Yilmaz","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2410.01595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01591v1","updated":"2024-10-02T14:25:02Z","published":"2024-10-02T14:25:02Z","title":"Imaging foundation model for universal enhancement of non-ideal\n  measurement CT","summary":"  Non-ideal measurement computed tomography (NICT), which sacrifices optimal\nimaging standards for new advantages in CT imaging, is expanding the clinical\napplication scope of CT images. However, with the reduction of imaging\nstandards, the image quality has also been reduced, extremely limiting the\nclinical acceptability. Although numerous studies have demonstrated the\nfeasibility of deep learning for the NICT enhancement in specific scenarios,\ntheir high data cost and limited generalizability have become large obstacles.\nThe recent research on the foundation model has brought new opportunities for\nbuilding a universal NICT enhancement model - bridging the image quality\ndegradation with minimal data cost. However, owing to the challenges in the\ncollection of large pre-training datasets and the compatibility of data\nvariation, no success has been reported. In this paper, we propose a\nmulti-scale integrated Transformer AMPlifier (TAMP), the first imaging\nfoundation model for universal NICT enhancement. It has been pre-trained on a\nlarge-scale physical-driven simulation dataset with 3.6 million NICT-ICT image\npairs, and is able to directly generalize to the NICT enhancement tasks with\nvarious non-ideal settings and body regions. Via the adaptation with few data,\nit can further achieve professional performance in real-world specific\nscenarios. Our extensive experiments have demonstrated that the proposed TAMP\nhas significant potential for promoting the exploration and application of NICT\nand serving a wider range of medical scenarios.\n","authors":["Yuxin Liu","Rongjun Ge","Yuting He","Zhan Wu","Chenyu You","Shuo Li","Yang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09031v3","updated":"2024-10-02T14:24:50Z","published":"2024-06-13T12:04:40Z","title":"A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and\n  Generalizability","summary":"  Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 17 graph\npooling methods and 28 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis, backbone analysis, parameter analysis and visualization to\nprovide more evidence. Extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can\nprovide valuable insights and guidance for deep geometric learning research.\nThe source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.\n","authors":["Pengyun Wang","Junyu Luo","Yanxin Shen","Ming Zhang","Siyu Heng","Xiao Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11176v3","updated":"2024-10-02T14:20:29Z","published":"2024-02-17T02:54:32Z","title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models","summary":"  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n","authors":["Yougang Lyu","Lingyong Yan","Shuaiqiang Wang","Haibo Shi","Dawei Yin","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2402.11176v3.pdf","comment":"EMNLP 2024 main paper"},{"id":"http://arxiv.org/abs/2410.01583v1","updated":"2024-10-02T14:17:50Z","published":"2024-10-02T14:17:50Z","title":"Iterated Local Search with Linkage Learning","summary":"  In pseudo-Boolean optimization, a variable interaction graph represents\nvariables as vertices, and interactions between pairs of variables as edges. In\nblack-box optimization, the variable interaction graph may be at least\npartially discovered by using empirical linkage learning techniques. These\nmethods never report false variable interactions, but they are computationally\nexpensive. The recently proposed local search with linkage learning discovers\nthe partial variable interaction graph as a side-effect of iterated local\nsearch. However, information about the strength of the interactions is not\nlearned by the algorithm. We propose local search with linkage learning 2,\nwhich builds a weighted variable interaction graph that stores information\nabout the strength of the interaction between variables. The weighted variable\ninteraction graph can provide new insights about the optimization problem and\nbehavior of optimizers. Experiments with NK landscapes, knapsack problem, and\nfeature selection show that local search with linkage learning 2 is able to\nefficiently build weighted variable interaction graphs. In particular,\nexperiments with feature selection show that the weighted variable interaction\ngraphs can be used for visualizing the feature interactions in machine\nlearning. Additionally, new transformation operators that exploit the\ninteractions between variables can be designed. We illustrate this ability by\nproposing a new perturbation operator for iterated local search.\n","authors":["Renato Tinós","Michal W. Przewozniczek","Darrell Whitley","Francisco Chicano"],"pdf_url":"https://arxiv.org/pdf/2410.01583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01579v1","updated":"2024-10-02T14:15:13Z","published":"2024-10-02T14:15:13Z","title":"Spoken Grammar Assessment Using LLM","summary":"  Spoken language assessment (SLA) systems restrict themselves to evaluating\nthe pronunciation and oral fluency of a speaker by analysing the read and\nspontaneous spoken utterances respectively. The assessment of language grammar\nor vocabulary is relegated to written language assessment (WLA) systems. Most\nWLA systems present a set of sentences from a curated finite-size database of\nsentences thereby making it possible to anticipate the test questions and train\noneself. In this paper, we propose a novel end-to-end SLA system to assess\nlanguage grammar from spoken utterances thus making WLA systems redundant;\nadditionally, we make the assessment largely unteachable by employing a large\nlanguage model (LLM) to bring in variations in the test. We further demonstrate\nthat a hybrid automatic speech recognition (ASR) with a custom-built language\nmodel outperforms the state-of-the-art ASR engine for spoken grammar\nassessment.\n","authors":["Sunil Kumar Kopparapu","Chitralekha Bhat","Ashish Panda"],"pdf_url":"https://arxiv.org/pdf/2410.01579v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.01575v1","updated":"2024-10-02T14:12:00Z","published":"2024-10-02T14:12:00Z","title":"Computing Ex Ante Equilibrium in Heterogeneous Zero-Sum Team Games","summary":"  The ex ante equilibrium for two-team zero-sum games, where agents within each\nteam collaborate to compete against the opposing team, is known to be the best\na team can do for coordination. Many existing works on ex ante equilibrium\nsolutions are aiming to extend the scope of ex ante equilibrium solving to\nlarge-scale team games based on Policy Space Response Oracle (PSRO). However,\nthe joint team policy space constructed by the most prominent method, Team\nPSRO, cannot cover the entire team policy space in heterogeneous team games\nwhere teammates play distinct roles. Such insufficient policy expressiveness\ncauses Team PSRO to be trapped into a sub-optimal ex ante equilibrium with\nsignificantly higher exploitability and never converges to the global ex ante\nequilibrium. To find the global ex ante equilibrium without introducing\nadditional computational complexity, we first parameterize heterogeneous\npolicies for teammates, and we prove that optimizing the heterogeneous\nteammates' policies sequentially can guarantee a monotonic improvement in team\nrewards. We further propose Heterogeneous-PSRO (H-PSRO), a novel framework for\nheterogeneous team games, which integrates the sequential correlation mechanism\ninto the PSRO framework and serves as the first PSRO framework for\nheterogeneous team games. We prove that H-PSRO achieves lower exploitability\nthan Team PSRO in heterogeneous team games. Empirically, H-PSRO achieves\nconvergence in matrix heterogeneous games that are unsolvable by\nnon-heterogeneous baselines. Further experiments reveal that H-PSRO outperforms\nnon-heterogeneous baselines in both heterogeneous team games and homogeneous\nsettings.\n","authors":["Naming Liu","Mingzhi Wang","Xihuai Wang","Weinan Zhang","Yaodong Yang","Youzhi Zhang","Bo An","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2410.01575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v2","updated":"2024-10-02T13:59:40Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01556v1","updated":"2024-10-02T13:52:55Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04368v2","updated":"2024-10-02T13:51:37Z","published":"2024-09-06T15:59:30Z","title":"The Impact of Scanner Domain Shift on Deep Learning Performance in\n  Medical Imaging: an Experimental Study","summary":"  Purpose: Medical images acquired using different scanners and protocols can\ndiffer substantially in their appearance. This phenomenon, scanner domain\nshift, can result in a drop in the performance of deep neural networks which\nare trained on data acquired by one scanner and tested on another. This\nsignificant practical issue is well-acknowledged, however, no systematic study\nof the issue is available across different modalities and diagnostic tasks.\nMaterials and Methods: In this paper, we present a broad experimental study\nevaluating the impact of scanner domain shift on convolutional neural network\nperformance for different automated diagnostic tasks. We evaluate this\nphenomenon in common radiological modalities, including X-ray, CT, and MRI.\nResults: We find that network performance on data from a different scanner is\nalmost always worse than on same-scanner data, and we quantify the degree of\nperformance drop across different datasets. Notably, we find that this drop is\nmost severe for MRI, moderate for X-ray, and quite small for CT, on average,\nwhich we attribute to the standardized nature of CT acquisition systems which\nis not present in MRI or X-ray. We also study how injecting varying amounts of\ntarget domain data into the training set, as well as adding noise to the\ntraining data, helps with generalization. Conclusion: Our results provide\nextensive experimental evidence and quantification of the extent of performance\ndrop caused by scanner domain shift in deep learning across different\nmodalities, with the goal of guiding the future development of robust deep\nlearning models for medical image analysis.\n","authors":["Brian Guo","Darui Lu","Gregory Szumel","Rongze Gui","Tingyu Wang","Nicholas Konz","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2409.04368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10588v6","updated":"2024-10-02T13:50:36Z","published":"2024-09-16T14:56:27Z","title":"Opponent Shaping for Antibody Development","summary":"  Anti-viral therapies are typically designed to target only the current\nstrains of a virus. Game theoretically, this corresponds to a short-sighted, or\nmyopic, response. However, therapy-induced selective pressures act on viruses\nto drive the emergence of mutated strains, against which initial therapies have\nreduced efficacy. Building on a computational model of binding between\nantibodies and viral antigens (the Absolut! framework), we design and implement\na genetic simulation of viral evolutionary escape. Crucially, this allows our\nantibody optimisation algorithm to consider and influence the entire escape\ncurve of the virus, i.e. to guide (or \"shape\") the viral evolution. This is\ninspired by opponent shaping which, in general-sum learning, accounts for the\nadaptation of the co-player rather than playing a myopic best response. Hence\nwe call the optimised antibodies shapers. Within our simulations, we\ndemonstrate that our shapers target both current and simulated future viral\nvariants, outperforming the antibodies chosen in a myopic way. Furthermore, we\nshow that shapers exert specific evolutionary pressure on the virus compared to\nmyopic antibodies. Altogether, shapers modify the evolutionary trajectories of\nviral strains and minimise the viral escape compared to their myopic\ncounterparts. While this is a simplified model, we hope that our proposed\nparadigm will facilitate the discovery of better long-lived vaccines and\nantibody therapies in the future, enabled by rapid advancements in the\ncapabilities of simulation tools. Our code is available at\nhttps://github.com/olakalisz/antibody-shapers.\n","authors":["Sebastian Towers","Aleksandra Kalisz","Philippe A. Robert","Alicia Higueruelo","Francesca Vianello","Ming-Han Chloe Tsai","Harrison Steel","Jakob N. Foerster"],"pdf_url":"https://arxiv.org/pdf/2409.10588v6.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.01553v1","updated":"2024-10-02T13:47:17Z","published":"2024-10-02T13:47:17Z","title":"MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an\n  AI-SCE Framework","summary":"  Artificial intelligence (AI) and large language models (LLMs) in healthcare\nrequire advanced clinical skills (CS), yet current benchmarks fail to evaluate\nthese comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by\nmedical education's Objective Structured Clinical Examinations (OSCEs), to\naddress this gap. MedQA-CS evaluates LLMs through two instruction-following\ntasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real\nclinical scenarios. Our contributions include developing MedQA-CS, a\ncomprehensive evaluation framework with publicly available data and expert\nannotations, and providing the quantitative and qualitative assessment of LLMs\nas reliable judges in CS evaluation. Our experiments show that MedQA-CS is a\nmore challenging benchmark for evaluating clinical skills than traditional\nmultiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,\nMedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities\nfor both open- and closed-source LLMs.\n","authors":["Zonghai Yao","Zihao Zhang","Chaolong Tang","Xingyu Bian","Youxia Zhao","Zhichao Yang","Junda Wang","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2406.04886v2","updated":"2024-10-02T13:40:10Z","published":"2024-06-07T12:32:44Z","title":"Unveiling the Invisible: Captioning Videos with Metaphors","summary":"  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Sumit Shekhar"],"pdf_url":"https://arxiv.org/pdf/2406.04886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03179v2","updated":"2024-10-02T13:32:56Z","published":"2024-07-03T14:59:46Z","title":"Motion meets Attention: Video Motion Prompts","summary":"  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.\n","authors":["Qixiang Chen","Lei Wang","Piotr Koniusz","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2407.03179v2.pdf","comment":"Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)"},{"id":"http://arxiv.org/abs/2410.01540v1","updated":"2024-10-02T13:29:52Z","published":"2024-10-02T13:29:52Z","title":"Edge-preserving noise for diffusion models","summary":"  Classical generative diffusion models learn an isotropic Gaussian denoising\nprocess, treating all spatial regions uniformly, thus neglecting potentially\nvaluable structural information in the data. Inspired by the long-established\nwork on anisotropic diffusion in image processing, we present a novel\nedge-preserving diffusion model that is a generalization of denoising diffusion\nprobablistic models (DDPM). In particular, we introduce an edge-aware noise\nscheduler that varies between edge-preserving and isotropic Gaussian noise. We\nshow that our model's generative process converges faster to results that more\nclosely match the target distribution. We demonstrate its capability to better\nlearn the low-to-mid frequencies within the dataset, which plays a crucial role\nin representing shapes and structural information. Our edge-preserving\ndiffusion process consistently outperforms state-of-the-art baselines in\nunconditional image generation. It is also more robust for generative tasks\nguided by a shape-based prior, such as stroke-to-image generation. We present\nqualitative and quantitative results showing consistent improvements (FID\nscore) of up to 30% for both tasks.\n","authors":["Jente Vandersanden","Sascha Holl","Xingchang Huang","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2410.01540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01532v1","updated":"2024-10-02T13:24:56Z","published":"2024-10-02T13:24:56Z","title":"Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for\n  Large Language Models","summary":"  Advancements in Natural Language Processing (NLP), have led to the emergence\nof Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which\nexcel across a range of tasks but require extensive fine-tuning to align their\noutputs with human expectations. A widely used method for achieving this\nalignment is Reinforcement Learning from Human Feedback (RLHF), which, despite\nits success, faces challenges in accurately modelling human preferences. In\nthis paper, we introduce GazeReward, a novel framework that integrates implicit\nfeedback -- and specifically eye-tracking (ET) data -- into the Reward Model\n(RM). In addition, we explore how ET-based features can provide insights into\nuser preferences. Through ablation studies we test our framework with different\nintegration methods, LLMs, and ET generator models, demonstrating that our\napproach significantly improves the accuracy of the RM on established human\npreference datasets. This work advances the ongoing discussion on optimizing AI\nalignment with human values, exploring the potential of cognitive data for\nshaping future NLP research.\n","authors":["Angela Lopez-Cardona","Carlos Segura","Alexandros Karatzoglou","Sergi Abadal","Ioannis Arapakis"],"pdf_url":"https://arxiv.org/pdf/2410.01532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01531v1","updated":"2024-10-02T13:24:24Z","published":"2024-10-02T13:24:24Z","title":"TiVaT: Joint-Axis Attention for Time Series Forecasting with Lead-Lag\n  Dynamics","summary":"  Multivariate time series (MTS) forecasting plays a crucial role in various\nreal-world applications, yet simultaneously capturing both temporal and\ninter-variable dependencies remains a challenge. Conventional Channel-Dependent\n(CD) models handle these dependencies separately, limiting their ability to\nmodel complex interactions such as lead-lag dynamics. To address these\nlimitations, we propose TiVaT (Time-Variable Transformer), a novel architecture\nthat integrates temporal and variate dependencies through its Joint-Axis (JA)\nattention mechanism. TiVaT's ability to capture intricate variate-temporal\ndependencies, including asynchronous interactions, is further enhanced by the\nincorporation of Distance-aware Time-Variable (DTV) Sampling, which reduces\nnoise and improves accuracy through a learned 2D map that focuses on key\ninteractions. TiVaT effectively models both temporal and variate dependencies,\nconsistently delivering strong performance across diverse datasets. Notably, it\nexcels in capturing complex patterns within multivariate time series, enabling\nit to surpass or remain competitive with state-of-the-art methods. This\npositions TiVaT as a new benchmark in MTS forecasting, particularly in handling\ndatasets characterized by intricate and challenging dependencies.\n","authors":["Junwoo Ha","Hyukjae Kwon","Sungsoo Kim","Kisu Lee","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01531v1.pdf","comment":"15pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.16587v2","updated":"2024-10-02T13:22:27Z","published":"2024-05-26T14:38:24Z","title":"Cost-Effective Online Multi-LLM Selection with Versatile Reward Models","summary":"  With the rapid advancement of large language models (LLMs), the diversity of\nmulti-LLM tasks and the variability in their pricing structures have become\nincreasingly important, as costs can vary greatly between different LLMs. To\ntackle these challenges, we introduce the \\textit{C2MAB-V}, a\n\\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed\n\\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM\nselection and usage. This online model differs from traditional static\napproaches or those reliant on a single LLM without cost consideration. With\nmultiple LLMs deployed on a scheduling cloud and a local server dedicated to\nhandling user queries, \\textit{C2MAB-V} facilitates the selection of multiple\nLLMs over a combinatorial search space, specifically tailored for various\ncollaborative task types with different reward models. Based on our designed\nonline feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can\neffectively address the multi-LLM selection challenge by managing the\nexploration-exploitation trade-off across different models, while also\nbalancing cost and reward for diverse tasks. The NP-hard integer linear\nprogramming problem for selecting multiple LLMs with trade-off dilemmas is\naddressed by: i) decomposing the integer problem into a relaxed form by the\nlocal server, ii) utilizing a discretization rounding scheme that provides\noptimal LLM combinations by the scheduling cloud, and iii) continual online\nupdates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers\nstrict guarantees over versatile reward models, matching state-of-the-art\nresults for regret and violations in some degenerate cases. Empirically, we\nshow that \\textit{C2MAB-V} effectively balances performance and cost-efficiency\nwith nine LLMs for three application scenarios.\n","authors":["Xiangxiang Dai","Jin Li","Xutong Liu","Anqi Yu","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2405.16587v2.pdf","comment":"32 pages, 14 figures, conference"},{"id":"http://arxiv.org/abs/2408.13131v2","updated":"2024-10-02T13:21:50Z","published":"2024-08-23T14:57:46Z","title":"DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction","summary":"  Long-horizon event forecasting is critical across various domains, including\nretail, finance, healthcare, and social networks. Traditional methods, such as\nMarked Temporal Point Processes (MTPP), often rely on autoregressive models to\npredict multiple future events. However, these models frequently suffer from\nissues like converging to constant or repetitive outputs, which limits their\neffectiveness and general applicability. To address these challenges, we\nintroduce DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection techniques from computer vision. DeTPP employs a\nunique matching-based loss function that selectively prioritizes reliably\npredictable events, improving the accuracy and diversity of predictions during\ninference. Our method establishes a new state-of-the-art in long-horizon event\nforecasting, achieving up to a 77% relative improvement over existing MTPP and\nnext-K methods. The proposed hybrid approach enhances the accuracy of next\nevent prediction by up to 2.7% on a large transactional dataset. Notably, DeTPP\nis also among the fastest methods for inference. The implementation of DeTPP is\npublicly available on GitHub.\n","authors":["Ivan Karpukhin","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.13131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07791v5","updated":"2024-10-02T13:08:43Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs","summary":"  LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.\n","authors":["Lin Shi","Chiyu Ma","Wenhua Liang","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01512v1","updated":"2024-10-02T13:02:23Z","published":"2024-10-02T13:02:23Z","title":"InstaTrans: An Instruction-Aware Translation Framework for Non-English\n  Instruction Datasets","summary":"  It is challenging to generate high-quality instruction datasets for\nnon-English languages due to tail phenomena, which limit performance on less\nfrequently observed data. To mitigate this issue, we propose translating\nexisting high-quality English instruction datasets as a solution, emphasizing\nthe need for complete and instruction-aware translations to maintain the\ninherent attributes of these datasets. We claim that fine-tuning LLMs with\ndatasets translated in this way can improve their performance in the target\nlanguage. To this end, we introduces a new translation framework tailored for\ninstruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through\nextensive experiments, we demonstrate the superiority of InstaTrans over other\ncompetitors in terms of completeness and instruction-awareness of translation,\nhighlighting its potential to broaden the accessibility of LLMs across diverse\nlanguages at a relatively low cost. Furthermore, we have validated that\nfine-tuning LLMs with datasets translated by InstaTrans can effectively improve\ntheir performance in the target language.\n","authors":["Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.01512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00535v2","updated":"2024-10-02T13:02:06Z","published":"2024-10-01T09:21:29Z","title":"Optimal Causal Representations and the Causal Information Bottleneck","summary":"  To effectively study complex causal systems, it is often useful to construct\nrepresentations that simplify parts of the system by discarding irrelevant\ndetails while preserving key features. The Information Bottleneck (IB) method\nis a widely used approach in representation learning that compresses random\nvariables while retaining information about a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces representations which are causally interpretable, and which can\nbe used when reasoning about interventions. We present experimental results\ndemonstrating that the learned representations accurately capture causality as\nintended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v2.pdf","comment":"Submitted to ICLR 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2410.01506v1","updated":"2024-10-02T12:58:55Z","published":"2024-10-02T12:58:55Z","title":"LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature\n  Fusion","summary":"  In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.\n","authors":["Dexuan Ding","Lei Wang","Liyun Zhu","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2410.01506v1.pdf","comment":"Research paper"},{"id":"http://arxiv.org/abs/2402.05569v4","updated":"2024-10-02T12:57:32Z","published":"2024-02-08T11:10:39Z","title":"Training-Free Message Passing for Learning on Hypergraphs","summary":"  Hypergraphs are crucial for modelling higher-order interactions in real-world\ndata. Hypergraph neural networks (HNNs) effectively utilise these structures by\nmessage passing to generate informative node features for various downstream\ntasks like node classification. However, the message passing module in existing\nHNNs typically requires a computationally intensive training process, which\nlimits their practical use. To tackle this challenge, we propose an alternative\napproach by decoupling the usage of hypergraph structural information from the\nmodel learning stage. This leads to a novel training-free message passing\nmodule, named TF-MP-Module, which can be precomputed in the data preprocessing\nstage, thereby reducing the computational burden. We refer to the hypergraph\nneural network equipped with our TF-MP-Module as TF-HNN. We theoretically\nsupport the efficiency and effectiveness of TF-HNN by showing that: 1) It is\nmore training-efficient compared to existing HNNs; 2) It utilises as much\ninformation as existing HNNs for node feature generation; and 3) It is robust\nagainst the oversmoothing issue while using long-range interactions.\nExperiments based on seven real-world hypergraph benchmarks in node\nclassification and hyperlink prediction show that, compared to state-of-the-art\nHNNs, TF-HNN exhibits both competitive performance and superior training\nefficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN\noutperforms the node classification accuracy of the best baseline by 10% with\njust 1% of the training time of that baseline.\n","authors":["Bohan Tang","Zexi Liu","Keyue Jiang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2402.05569v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00488v2","updated":"2024-10-02T12:55:53Z","published":"2024-08-31T15:47:31Z","title":"Rapid Gyroscope Calibration: A Deep Learning Approach","summary":"  Low-cost gyroscope calibration is essential for ensuring the accuracy and\nreliability of gyroscope measurements. Stationary calibration estimates the\ndeterministic parts of measurement errors. To this end, a common practice is to\naverage the gyroscope readings during a predefined period and estimate the\ngyroscope bias. Calibration duration plays a crucial role in performance,\ntherefore, longer periods are preferred. However, some applications require\nquick startup times and calibration is therefore allowed only for a short time.\nIn this work, we focus on reducing low-cost gyroscope calibration time using\ndeep learning methods. We propose a deep-learning framework and explore the\npossibilities of using multiple real and virtual gyroscopes to improve the\ncalibration performance of single gyroscopes. To train and validate our\napproach, we recorded a dataset consisting of 169 hours of gyroscope readings,\nusing 24 gyroscopes of two different brands. We also created a virtual dataset\nconsisting of simulated gyroscope readings. The two datasets were used to\nevaluate our proposed approach. One of our key achievements in this work is\nreducing gyroscope calibration time by up to 89% using three low-cost\ngyroscopes.\n","authors":["Yair Stolero","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2409.00488v2.pdf","comment":"10 Pages, 14 Figures"},{"id":"http://arxiv.org/abs/2410.01500v1","updated":"2024-10-02T12:51:25Z","published":"2024-10-02T12:51:25Z","title":"Discrete Diffusion Schrödinger Bridge Matching for Graph\n  Transformation","summary":"  Transporting between arbitrary distributions is a fundamental goal in\ngenerative modeling. Recently proposed diffusion bridge models provide a\npotential solution, but they rely on a joint distribution that is difficult to\nobtain in practice. Furthermore, formulations based on continuous domains limit\ntheir applicability to discrete domains such as graphs. To overcome these\nlimitations, we propose Discrete Diffusion Schr\\\"odinger Bridge Matching\n(DDSBM), a novel framework that utilizes continuous-time Markov chains to solve\nthe SB problem in a high-dimensional discrete state space. Our approach extends\nIterative Markovian Fitting to discrete domains, and we have proved its\nconvergence to the SB. Furthermore, we adapt our framework for the graph\ntransformation and show that our design choice of underlying dynamics\ncharacterized by independent modifications of nodes and edges can be\ninterpreted as the entropy-regularized version of optimal transport with a cost\nfunction described by the graph edit distance. To demonstrate the effectiveness\nof our framework, we have applied DDSBM to molecular optimization in the field\nof chemistry. Experimental results demonstrate that DDSBM effectively optimizes\nmolecules' property-of-interest with minimal graph transformation, successfully\nretaining other features.\n","authors":["Jun Hyeong Kim","Seonghwan Kim","Seokhyun Moon","Hyeongwoo Kim","Jeheon Woo","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04102v3","updated":"2024-10-02T12:48:44Z","published":"2024-08-07T21:44:29Z","title":"ArtVLM: Attribute Recognition Through Vision-Based Prefix Language\n  Modeling","summary":"  Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).\n","authors":["William Yicheng Zhu","Keren Ye","Junjie Ke","Jiahui Yu","Leonidas Guibas","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04102v3.pdf","comment":"Accepted at ECCV 2024. Contact: zhuwilliam[at]google[dot]com. GitHub:\n  https://github.com/google-research/google-research/tree/master/attribute_with_prefixlm"},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.14219v2","updated":"2024-10-02T12:45:50Z","published":"2024-05-23T06:28:44Z","title":"Understanding the Training and Generalization of Pretrained Transformer\n  for Sequential Decision Making","summary":"  In this paper, we consider the supervised pre-trained transformer for a class\nof sequential decision-making problems. The class of considered problems is a\nsubset of the general formulation of reinforcement learning in that there is no\ntransition probability matrix; though seemingly restrictive, the subset class\nof problems covers bandits, dynamic pricing, and newsvendor problems as special\ncases. Such a structure enables the use of optimal actions/decisions in the\npre-training phase, and the usage also provides new insights for the training\nand generalization of the pre-trained transformer. We first note the training\nof the transformer model can be viewed as a performative prediction problem,\nand the existing methods and theories largely ignore or cannot resolve an\nout-of-distribution issue. We propose a natural solution that includes the\ntransformer-generated action sequences in the training procedure, and it enjoys\nbetter properties both numerically and theoretically. The availability of the\noptimal actions in the considered tasks also allows us to analyze the\nproperties of the pre-trained transformer as an algorithm and explains why it\nmay lack exploration and how this can be automatically resolved. Numerically,\nwe categorize the advantages of pre-trained transformers over the structured\nalgorithms such as UCB and Thompson sampling into three cases: (i) it better\nutilizes the prior knowledge in the pre-training data; (ii) it can elegantly\nhandle the misspecification issue suffered by the structured algorithms; (iii)\nfor short time horizon such as $T\\le50$, it behaves more greedy and enjoys much\nbetter regret than the structured algorithms designed for asymptotic\noptimality.\n","authors":["Hanzhao Wang","Yu Pan","Fupeng Sun","Shang Liu","Kalyan Talluri","Guanting Chen","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.14219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01482v1","updated":"2024-10-02T12:34:04Z","published":"2024-10-02T12:34:04Z","title":"One Wave to Explain Them All: A Unifying Perspective on Post-hoc\n  Explainability","summary":"  Despite the growing use of deep neural networks in safety-critical\ndecision-making, their inherent black-box nature hinders transparency and\ninterpretability. Explainable AI (XAI) methods have thus emerged to understand\na model's internal workings, and notably attribution methods also called\nsaliency maps. Conventional attribution methods typically identify the\nlocations -- the where -- of significant regions within an input. However,\nbecause they overlook the inherent structure of the input data, these methods\noften fail to interpret what these regions represent in terms of structural\ncomponents (e.g., textures in images or transients in sounds). Furthermore,\nexisting methods are usually tailored to a single data modality, limiting their\ngeneralizability. In this paper, we propose leveraging the wavelet domain as a\nrobust mathematical foundation for attribution. Our approach, the Wavelet\nAttribution Method (WAM) extends the existing gradient-based feature\nattributions into the wavelet domain, providing a unified framework for\nexplaining classifiers across images, audio, and 3D shapes. Empirical\nevaluations demonstrate that WAM matches or surpasses state-of-the-art methods\nacross faithfulness metrics and models in image, audio, and 3D explainability.\nFinally, we show how our method explains not only the where -- the important\nparts of the input -- but also the what -- the relevant patterns in terms of\nstructural components.\n","authors":["Gabriel Kasmi","Amandine Brunetto","Thomas Fel","Jayneel Parekh"],"pdf_url":"https://arxiv.org/pdf/2410.01482v1.pdf","comment":"main: 10 pages, appendix: 14 pages, 5 Tables, 25 Figures"},{"id":"http://arxiv.org/abs/2410.01481v1","updated":"2024-10-02T12:33:59Z","published":"2024-10-02T12:33:59Z","title":"SonicSim: A customizable simulation platform for speech processing in\n  moving sound source scenarios","summary":"  The systematic evaluation of speech separation and enhancement models under\nmoving sound source conditions typically requires extensive data comprising\ndiverse scenarios. However, real-world datasets often contain insufficient data\nto meet the training and evaluation requirements of models. Although synthetic\ndatasets offer a larger volume of data, their acoustic simulations lack\nrealism. Consequently, neither real-world nor synthetic datasets effectively\nfulfill practical needs. To address these issues, we introduce SonicSim, a\nsynthetic toolkit de-designed to generate highly customizable data for moving\nsound sources. SonicSim is developed based on the embodied AI simulation\nplatform, Habitat-sim, supporting multi-level adjustments, including\nscene-level, microphone-level, and source-level, thereby generating more\ndiverse synthetic data. Leveraging SonicSim, we constructed a moving sound\nsource benchmark dataset, SonicSet, using the Librispeech, the Freesound\nDataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the\nMatterport3D to evaluate speech separation and enhancement models.\nAdditionally, to validate the differences between synthetic data and real-world\ndata, we randomly selected 5 hours of raw data without reverberation from the\nSonicSet validation set to record a real-world speech separation dataset, which\nwas then compared with the corresponding synthetic datasets. Similarly, we\nutilized the real-world speech enhancement dataset RealMAN to validate the\nacoustic gap between other synthetic datasets and the SonicSet dataset for\nspeech enhancement. The results indicate that the synthetic data generated by\nSonicSim can effectively generalize to real-world scenarios. Demo and code are\npublicly available at https://cslikai.cn/SonicSim/.\n","authors":["Kai Li","Wendi Sang","Chang Zeng","Runxuan Yang","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01481v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01470v1","updated":"2024-10-02T12:21:31Z","published":"2024-10-02T12:21:31Z","title":"Peeling Back the Layers: An In-Depth Evaluation of Encoder Architectures\n  in Neural News Recommenders","summary":"  Encoder architectures play a pivotal role in neural news recommenders by\nembedding the semantic and contextual information of news and users. Thus,\nresearch has heavily focused on enhancing the representational capabilities of\nnews and user encoders to improve recommender performance. Despite the\nsignificant impact of encoder architectures on the quality of news and user\nrepresentations, existing analyses of encoder designs focus only on the overall\ndownstream recommendation performance. This offers a one-sided assessment of\nthe encoders' similarity, ignoring more nuanced differences in their behavior,\nand potentially resulting in sub-optimal model selection. In this work, we\nperform a comprehensive analysis of encoder architectures in neural news\nrecommender systems. We systematically evaluate the most prominent news and\nuser encoder architectures, focusing on their (i) representational similarity,\nmeasured with the Central Kernel Alignment, (ii) overlap of generated\nrecommendation lists, quantified with the Jaccard similarity, and (iii) the\noverall recommendation performance. Our analysis reveals that the complexity of\ncertain encoding techniques is often empirically unjustified, highlighting the\npotential for simpler, more efficient architectures. By isolating the effects\nof individual components, we provide valuable insights for researchers and\npractitioners to make better informed decisions about encoder selection and\navoid unnecessary complexity in the design of news recommenders.\n","authors":["Andreea Iana","Goran Glavaš","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2410.01470v1.pdf","comment":"Accepted at the 12th International Workshop on News Recommendation\n  and Analytics (INRA 2024) in conjunction with ACM RecSys 2024"},{"id":"http://arxiv.org/abs/2410.01469v1","updated":"2024-10-02T12:21:06Z","published":"2024-10-02T12:21:06Z","title":"TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for\n  Efficient Speech Separation","summary":"  In recent years, much speech separation research has focused primarily on\nimproving model performance. However, for low-latency speech processing\nsystems, high efficiency is equally important. Therefore, we propose a speech\nseparation model with significantly reduced parameters and computational costs:\nTime-frequency Interleaved Gain Extraction and Reconstruction network (TIGER).\nTIGER leverages prior knowledge to divide frequency bands and compresses\nfrequency information. We employ a multi-scale selective attention module to\nextract contextual features, while introducing a full-frequency-frame attention\nmodule to capture both temporal and frequency contextual information.\nAdditionally, to more realistically evaluate the performance of speech\nseparation models in complex acoustic environments, we introduce a dataset\ncalled EchoSet. This dataset includes noise and more realistic reverberation\n(e.g., considering object occlusions and material properties), with speech from\ntwo speakers overlapping at random proportions. Experimental results showed\nthat models trained on EchoSet had better generalization ability than those\ntrained on other datasets to the data collected in the physical world, which\nvalidated the practical value of the EchoSet. On EchoSet and real-world data,\nTIGER significantly reduces the number of parameters by 94.3% and the MACs by\n95.3% while achieving performance surpassing state-of-the-art (SOTA) model\nTF-GridNet. This is the first speech separation model with fewer than 1 million\nparameters that achieves performance comparable to the SOTA model.\n","authors":["Mohan Xu","Kai Li","Guo Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01469v1.pdf","comment":"Technical report, demo page: https://cslikai.cn/TIGER/"},{"id":"http://arxiv.org/abs/2401.07656v4","updated":"2024-10-02T12:12:31Z","published":"2024-01-15T12:52:56Z","title":"Learning Explainable and Better Performing Representations of POMDP\n  Strategies","summary":"  Strategies for partially observable Markov decision processes (POMDP)\ntypically require memory. One way to represent this memory is via automata. We\npresent a method to learn an automaton representation of a strategy using a\nmodification of the L*-algorithm. Compared to the tabular representation of a\nstrategy, the resulting automaton is dramatically smaller and thus also more\nexplainable. Moreover, in the learning process, our heuristics may even improve\nthe strategy's performance. In contrast to approaches that synthesize an\nautomaton directly from the POMDP thereby solving it, our approach is\nincomparably more scalable.\n","authors":["Alexander Bork","Debraj Chakraborty","Kush Grover","Jan Kretinsky","Stefanie Mohr"],"pdf_url":"https://arxiv.org/pdf/2401.07656v4.pdf","comment":"Technical report for the submission to TACAS 24"},{"id":"http://arxiv.org/abs/2410.01458v1","updated":"2024-10-02T12:10:07Z","published":"2024-10-02T12:10:07Z","title":"From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with\n  LLM-Guided Knowledge","summary":"  Q-shaping is an extension of Q-value initialization and serves as an\nalternative to reward shaping for incorporating domain knowledge to accelerate\nagent training, thereby improving sample efficiency by directly shaping\nQ-values. This approach is both general and robust across diverse tasks,\nallowing for immediate impact assessment while guaranteeing optimality. We\nevaluated Q-shaping across 20 different environments using a large language\nmodel (LLM) as the heuristic provider. The results demonstrate that Q-shaping\nsignificantly enhances sample efficiency, achieving a \\textbf{16.87\\%}\nimprovement over the best baseline in each environment and a \\textbf{253.80\\%}\nimprovement compared to LLM-based reward shaping methods. These findings\nestablish Q-shaping as a superior and unbiased alternative to conventional\nreward shaping in reinforcement learning.\n","authors":["Xiefeng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01458v1.pdf","comment":"q-shaping, reinforcement learning, reward shaping"},{"id":"http://arxiv.org/abs/2410.01450v1","updated":"2024-10-02T12:01:32Z","published":"2024-10-02T12:01:32Z","title":"Agent-Driven Large Language Models for Mandarin Lyric Generation","summary":"  Generative Large Language Models have shown impressive in-context learning\nabilities, performing well across various tasks with just a prompt. Previous\nmelody-to-lyric research has been limited by scarce high-quality aligned data\nand unclear standard for creativeness. Most efforts focused on general themes\nor emotions, which are less valuable given current language model capabilities.\nIn tonal contour languages like Mandarin, pitch contours are influenced by both\nmelody and tone, leading to variations in lyric-melody fit. Our study,\nvalidated by the Mpop600 dataset, confirms that lyricists and melody writers\nconsider this fit during their composition process. In this research, we\ndeveloped a multi-agent system that decomposes the melody-to-lyric task into\nsub-tasks, with each agent controlling rhyme, syllable count, lyric-melody\nalignment, and consistency. Listening tests were conducted via a\ndiffusion-based singing voice synthesizer to evaluate the quality of lyrics\ngenerated by different agent groups.\n","authors":["Hong-Hsiang Liu","Yi-Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01450v1.pdf","comment":"6 pages, figures, Accepted at O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2404.03887v4","updated":"2024-10-02T11:56:35Z","published":"2024-04-05T04:25:47Z","title":"SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models","summary":"  This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.\n","authors":["Hyeonwoo Kim","Gyoungjin Gim","Yungi Kim","Jihoo Kim","Byungju Kim","Wonseok Lee","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.03887v4.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01423v1","updated":"2024-10-02T11:16:11Z","published":"2024-10-02T11:16:11Z","title":"Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data\n  Free Distillation","summary":"  This work presents Fair4Free, a novel generative model to generate synthetic\nfair data using data-free distillation in the latent space. Fair4Free can work\non the situation when the data is private or inaccessible. In our approach, we\nfirst train a teacher model to create fair representation and then distil the\nknowledge to a student model (using a smaller architecture). The process of\ndistilling the student model is data-free, i.e. the student model does not have\naccess to the training dataset while distilling. After the distillation, we use\nthe distilled model to generate fair synthetic samples. Our extensive\nexperiments show that our synthetic samples outperform state-of-the-art models\nin all three criteria (fairness, utility and synthetic quality) with a\nperformance increase of 5% for fairness, 8% for utility and 12% in synthetic\nquality for both tabular and image datasets.\n","authors":["Md Fahim Sikder","Daniel de Leng","Fredrik Heintz"],"pdf_url":"https://arxiv.org/pdf/2410.01423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06835v4","updated":"2024-10-02T11:15:25Z","published":"2023-11-12T13:25:28Z","title":"Open-Set Graph Anomaly Detection via Normal Structure Regularisation","summary":"  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). Those labelled training data provide crucial prior\nknowledge about abnormalities for GAD models, enabling substantially reduced\ndetection errors. However, current supervised GAD methods tend to\nover-emphasise fitting the seen anomalies, leading to many errors of detecting\nthe unseen anomalies as normal nodes. Further, existing open-set AD models were\nintroduced to handle Euclidean data, failing to effectively capture\ndiscriminative features from graph structure and node attributes for GAD. In\nthis work, we propose a novel open-set GAD approach, namely normal structure\nregularisation (NSReg), to achieve generalised detection ability to unseen\nanomalies, while maintaining its effectiveness on detecting seen anomalies. The\nkey idea in NSReg is to introduce a regularisation term that enforces the\nlearning of compact, semantically-rich representations of normal nodes based on\ntheir structural relations to other nodes. When being optimised with supervised\nanomaly detection losses, the regularisation term helps incorporate strong\nnormality into the modelling, and thus, it effectively avoids over-fitting the\nseen anomalies and learns a better normality decision boundary, largely\nreducing the false negatives of detecting unseen anomalies as normal. Extensive\nempirical results on seven real-world datasets show that NSReg significantly\noutperforms state-of-the-art competing methods by at least 14% AUC-ROC on the\nunseen anomaly classes and by 10% AUC-ROC on all anomaly classes.\n","authors":["Qizhou Wang","Guansong Pang","Mahsa Salehi","Xiaokun Xia","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2311.06835v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01241v2","updated":"2024-10-02T11:11:11Z","published":"2024-09-02T13:14:50Z","title":"CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and\n  Complex Automation","summary":"  The underlying framework for controlling autonomous robots and complex\nautomation applications are Operating Systems (OS) capable of scheduling\nperception-and-control tasks, as well as providing real-time data communication\nto other robotic peers and remote cloud computers. In this paper, we introduce\nCyberCortex AI, a robotics OS designed to enable heterogeneous AI-based\nrobotics and complex automation applications. CyberCortex AI is a decentralized\ndistributed OS which enables robots to talk to each other, as well as to High\nPerformance Computers (HPC) in the cloud. Sensory and control data from the\nrobots is streamed towards HPC systems with the purpose of training AI\nalgorithms, which are afterwards deployed on the robots. Each functionality of\na robot (e.g. sensory data acquisition, path planning, motion control, etc.) is\nexecuted within a so-called DataBlock of Filters shared through the internet,\nwhere each filter is computed either locally on the robot itself, or remotely\non a different robotic system. The data is stored and accessed via a so-called\nTemporal Addressable Memory (TAM), which acts as a gateway between each\nfilter's input and output. CyberCortex.AI has two main components: i) the\nCyberCortex AI inference system, which is a real-time implementation of the\nDataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI\ndojo, which runs on an HPC computer in the cloud, and it is used to design,\ntrain and deploy AI algorithms. We present a quantitative and qualitative\nperformance analysis of the proposed approach using two collaborative robotics\napplications: i) a forest fires prevention system based on an Unitree A1 legged\nrobot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system\nwhich uses CyberCortex.AI for collaborative perception and motion control.\n","authors":["Sorin Grigorescu","Mihai Zaha"],"pdf_url":"https://arxiv.org/pdf/2409.01241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18618v3","updated":"2024-10-02T11:08:29Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.14760v2","updated":"2024-10-02T11:03:16Z","published":"2024-06-20T22:10:52Z","title":"An LLM Feature-based Framework for Dialogue Constructiveness Assessment","summary":"  Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.\n","authors":["Lexin Zhou","Youmna Farag","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.14760v2.pdf","comment":"Paper accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01417v1","updated":"2024-10-02T10:58:54Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01413v1","updated":"2024-10-02T10:48:23Z","published":"2024-10-02T10:48:23Z","title":"Improving Fuzzy Rule Classifier with Brain Storm Optimization and Rule\n  Modification","summary":"  The expanding complexity and dimensionality in the search space can adversely\naffect inductive learning in fuzzy rule classifiers, thus impacting the\nscalability and accuracy of fuzzy systems. This research specifically addresses\nthe challenge of diabetic classification by employing the Brain Storm\nOptimization (BSO) algorithm to propose a novel fuzzy system that redefines\nrule generation for this context. An exponential model is integrated into the\nstandard BSO algorithm to enhance rule derivation, tailored specifically for\ndiabetes-related data. The innovative fuzzy system is then applied to\nclassification tasks involving diabetic datasets, demonstrating a substantial\nimprovement in classification accuracy, as evidenced by our experiments.\n","authors":["Yan Huang","Wei Liu","Xiaogang Zang"],"pdf_url":"https://arxiv.org/pdf/2410.01413v1.pdf","comment":"9 pages,8 figures"},{"id":"http://arxiv.org/abs/2402.09177v2","updated":"2024-10-02T10:43:07Z","published":"2024-02-14T13:45:19Z","title":"Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks","summary":"  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.\n","authors":["Yixin Cheng","Markos Georgopoulos","Volkan Cevher","Grigorios G. Chrysos"],"pdf_url":"https://arxiv.org/pdf/2402.09177v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.01410v1","updated":"2024-10-02T10:42:27Z","published":"2024-10-02T10:42:27Z","title":"On the Convergence of FedProx with Extrapolation and Inexact Prox","summary":"  Enhancing the FedProx federated learning algorithm (Li et al., 2020) with\nserver-side extrapolation, Li et al. (2024a) recently introduced the FedExProx\nmethod. Their theoretical analysis, however, relies on the assumption that each\nclient computes a certain proximal operator exactly, which is impractical since\nthis is virtually never possible to do in real settings. In this paper, we\ninvestigate the behavior of FedExProx without this exactness assumption in the\nsmooth and globally strongly convex setting. We establish a general convergence\nresult, showing that inexactness leads to convergence to a neighborhood of the\nsolution. Additionally, we demonstrate that, with careful control, the adverse\neffects of this inexactness can be mitigated. By linking inexactness to biased\ncompression (Beznosikov et al., 2023), we refine our analysis, highlighting\nrobustness of extrapolation to inexact proximal updates. We also examine the\nlocal iteration complexity required by each client to achieved the required\nlevel of inexactness using various local optimizers. Our theoretical insights\nare validated through comprehensive numerical experiments.\n","authors":["Hanmin Li","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2410.01410v1.pdf","comment":"36 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.07024v2","updated":"2024-10-02T10:37:58Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14857v3","updated":"2024-10-02T10:34:09Z","published":"2024-05-23T17:58:03Z","title":"Conditional Diffusion on Web-Scale Image Pairs leads to Diverse Image\n  Variations","summary":"  Generating image variations, where a model produces variations of an input\nimage while preserving the semantic context has gained increasing attention.\nCurrent image variation techniques involve adapting a text-to-image model to\nreconstruct an input image conditioned on the same image. We first demonstrate\nthat a diffusion model trained to reconstruct an input image from frozen\nembeddings, can reconstruct the image with minor variations. Second, inspired\nby how text-to-image models learn from web-scale text-image pairs, we explore a\nnew pretraining strategy to generate image variations using a large collection\nof image pairs. Our diffusion model \\textit{Semantica} receives a random\n(encoded) image from a webpage as conditional input and denoises another noisy\nrandom image from the same webpage. We carefully examine various design choices\nfor the image encoder, given its crucial role in extracting relevant context\nfrom the input image. Once trained, \\textit{Semantica} can adaptively generate\nnew images from a dataset by simply using images from that dataset as input.\nFinally, we identify limitations in standard image consistency metrics for\nevaluating image variations and propose alternative metrics based on few-shot\ngeneration.\n","authors":["Manoj Kumar","Neil Houlsby","Emiel Hoogeboom"],"pdf_url":"https://arxiv.org/pdf/2405.14857v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11394v2","updated":"2024-10-02T10:28:14Z","published":"2024-07-16T05:26:14Z","title":"DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation","summary":"  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is their conflict with the sampling dynamics of\ndiffusion models. Addressing this conflict allows us to treat SDS as a\ndiffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n","authors":["Jiwook Kim","Seonho Lee","Jaeyo Shin","Jiho Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.11394v2.pdf","comment":"ProjectPage: https://dream-catalyst.github.io Code:\n  https://github.com/kaist-cvml/DreamCatalyst (Appendix included)"},{"id":"http://arxiv.org/abs/2409.17171v2","updated":"2024-10-02T10:28:02Z","published":"2024-09-19T21:45:13Z","title":"Cross-Domain Content Generation with Domain-Specific Small Language\n  Models","summary":"  Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.\n","authors":["Ankit Maloo","Abhinav Garg"],"pdf_url":"https://arxiv.org/pdf/2409.17171v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.01396v1","updated":"2024-10-02T10:16:54Z","published":"2024-10-02T10:16:54Z","title":"Can We Delegate Learning to Automation?: A Comparative Study of LLM\n  Chatbots, Search Engines, and Books","summary":"  Learning is a key motivator behind information search behavior. With the\nemergence of LLM-based chatbots, students are increasingly turning to these\ntools as their primary resource for acquiring knowledge. However, the\ntransition from traditional resources like textbooks and web searches raises\nconcerns among educators. They worry that these fully-automated LLMs might lead\nstudents to delegate critical steps of search as learning. In this paper, we\nsystematically uncover three main concerns from educators' perspectives. In\nresponse to these concerns, we conducted a mixed-methods study with 92\nuniversity students to compare three learning sources with different automation\nlevels. Our results show that LLMs support comprehensive understanding of key\nconcepts without promoting passive learning, though their effectiveness in\nknowledge retention was limited. Additionally, we found that academic\nperformance impacted both learning outcomes and search patterns. Notably,\nhigher-competence learners engaged more deeply with content through\nreading-intensive behaviors rather than relying on search activities.\n","authors":["Yeonsun Yang","Ahyeon Shin","Mincheol Kang","Jiheon Kang","Jean Young Song"],"pdf_url":"https://arxiv.org/pdf/2410.01396v1.pdf","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.01386v1","updated":"2024-10-02T09:55:58Z","published":"2024-10-02T09:55:58Z","title":"FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated\n  Learning Deployments","summary":"  This paper presents Federated Learning with Adaptive Monitoring and\nElimination (FLAME), a novel solution capable of detecting and mitigating\nconcept drift in Federated Learning (FL) Internet of Things (IoT) environments.\nConcept drift poses significant challenges for FL models deployed in dynamic\nand real-world settings. FLAME leverages an FL architecture, considers a\nreal-world FL pipeline, and proves capable of maintaining model performance and\naccuracy while addressing bandwidth and privacy constraints. Introducing\nvarious features and extensions on previous works, FLAME offers a robust\nsolution to concept drift, significantly reducing computational load and\ncommunication overhead. Compared to well-known lightweight mitigation methods,\nFLAME demonstrates superior performance in maintaining high F1 scores and\nreducing resource utilisation in large-scale IoT deployments, making it a\npromising approach for real-world applications.\n","authors":["Ioannis Mavromatis","Stefano De Feo","Aftab Khan"],"pdf_url":"https://arxiv.org/pdf/2410.01386v1.pdf","comment":"Accepted for Publication at EMERGE Workshop - EWSN 2024"},{"id":"http://arxiv.org/abs/2404.17644v3","updated":"2024-10-02T09:55:25Z","published":"2024-04-26T18:08:15Z","title":"A Conditional Independence Test in the Presence of Discretization","summary":"  Testing conditional independence has many applications, such as in Bayesian\nnetwork learning and causal discovery. Different test methods have been\nproposed. However, existing methods generally can not work when only\ndiscretized observations are available. Specifically, consider $X_1$,\n$\\tilde{X}_2$ and $X_3$ are observed variables, where $\\tilde{X}_2$ is a\ndiscretization of latent variables $X_2$. Applying existing test methods to the\nobservations of $X_1$, $\\tilde{X}_2$ and $X_3$ can lead to a false conclusion\nabout the underlying conditional independence of variables $X_1$, $X_2$ and\n$X_3$. Motivated by this, we propose a conditional independence test\nspecifically designed to accommodate the presence of such discretization. To\nachieve this, we design the bridge equations to recover the parameter\nreflecting the statistical information of the underlying latent continuous\nvariables. An appropriate test statistic and its asymptotic distribution under\nthe null hypothesis of conditional independence have also been derived. Both\ntheoretical results and empirical validation have been provided, demonstrating\nthe effectiveness of our test methods.\n","authors":["Boyang Sun","Yu Yao","Huangyuan Hao","Yumou Qiu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.17644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19433v2","updated":"2024-10-02T09:53:48Z","published":"2024-09-28T18:38:21Z","title":"RMLR: Extending Multinomial Logistic Regression into General Geometries","summary":"  Riemannian neural networks, which extend deep learning techniques to\nRiemannian spaces, have gained significant attention in machine learning. To\nbetter classify the manifold-valued features, researchers have started\nextending Euclidean multinomial logistic regression (MLR) into Riemannian\nmanifolds. However, existing approaches suffer from limited applicability due\nto their strong reliance on specific geometric properties. This paper proposes\na framework for designing Riemannian MLR over general geometries, referred to\nas RMLR. Our framework only requires minimal geometric properties, thus\nexhibiting broad applicability and enabling its use with a wide range of\ngeometries. Specifically, we showcase our framework on the Symmetric Positive\nDefinite (SPD) manifold and special orthogonal group, i.e., the set of rotation\nmatrices. On the SPD manifold, we develop five families of SPD MLRs under five\ntypes of power-deformed metrics. On rotation matrices we propose Lie MLR based\non the popular bi-invariant metric. Extensive experiments on different\nRiemannian backbone networks validate the effectiveness of our framework.\n","authors":["Ziheng Chen","Yue Song","Rui Wang","Xiaojun Wu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2409.19433v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01380v1","updated":"2024-10-02T09:49:45Z","published":"2024-10-02T09:49:45Z","title":"Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition","summary":"  In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.\n","authors":["Jiyeon Kim","Hyunji Lee","Hyowon Cho","Joel Jang","Hyeonbin Hwang","Seungpil Won","Youbin Ahn","Dohaeng Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.01380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12001v3","updated":"2024-10-02T09:40:06Z","published":"2024-05-20T13:14:26Z","title":"Scrutinize What We Ignore: Reining In Task Representation Shift Of\n  Context-Based Offline Meta Reinforcement Learning","summary":"  Offline meta reinforcement learning (OMRL) has emerged as a promising\napproach for interaction avoidance and strong generalization performance by\nleveraging pre-collected data and meta-learning techniques. Previous\ncontext-based approaches predominantly rely on the intuition that alternating\noptimization between the context encoder and the policy can lead to performance\nimprovements, as long as the context encoder follows the principle of\nmaximizing the mutual information between the task variable $M$ and its latent\nrepresentation $Z$ ($I(Z;M)$) while the policy adopts the standard offline\nreinforcement learning (RL) algorithms conditioning on the learned task\nrepresentation.Despite promising results, the theoretical justification of\nperformance improvements for such intuition remains underexplored.Inspired by\nthe return discrepancy scheme in the model-based RL field, we find that the\nprevious optimization framework can be linked with the general RL objective of\nmaximizing the expected return, thereby explaining performance improvements.\nFurthermore, after scrutinizing this optimization framework, we find it ignores\nthe variation of the task representation in the alternating optimization\nprocess, which weakens the condition necessary for monotonic performance\nimprovements, and may therefore violate the monotonicity.We name this issue\n\\underline{task representation shift} and theoretically prove that the\nmonotonic performance improvements can be guaranteed with appropriate context\nencoder updates.We use different settings to rein in the task representation\nshift on three widely adopted training objectives concerning maximizing\n$I(Z;M)$ across different data qualities.Empirical results show that reining in\nthe task representation shift can indeed improve performance.\n","authors":["Hai Zhang","Boyuan Zheng","Tianying Ji","Jinhang Liu","Anqi Guo","Junqiao Zhao","Lanqing Li"],"pdf_url":"https://arxiv.org/pdf/2405.12001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01368v1","updated":"2024-10-02T09:30:01Z","published":"2024-10-02T09:30:01Z","title":"Theoretical Lower Bounds for the Oven Scheduling Problem","summary":"  The Oven Scheduling Problem (OSP) is an NP-hard real-world parallel batch\nscheduling problem arising in the semiconductor industry. The objective of the\nproblem is to schedule a set of jobs on ovens while minimizing several factors,\nnamely total oven runtime, job tardiness, and setup costs. At the same time, it\nmust adhere to various constraints such as oven eligibility and availability,\njob release dates, setup times between batches, and oven capacity limitations.\nThe key to obtaining efficient schedules is to process compatible jobs\nsimultaneously in batches. In this paper, we develop theoretical,\nproblem-specific lower bounds for the OSP that can be computed very quickly. We\nthoroughly examine these lower bounds, evaluating their quality and exploring\ntheir integration into existing solution methods. Specifically, we investigate\ntheir contribution to exact methods and a metaheuristic local search approach\nusing simulated annealing. Moreover, these problem-specific lower bounds enable\nus to assess the solution quality for large instances for which exact methods\noften fail to provide tight lower bounds.\n","authors":["Francesca Da Ros","Marie-Louise Lackner","Nysret Musliu"],"pdf_url":"https://arxiv.org/pdf/2410.01368v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2203.12517"},{"id":"http://arxiv.org/abs/2410.01363v1","updated":"2024-10-02T09:23:07Z","published":"2024-10-02T09:23:07Z","title":"PCQPR: Proactive Conversational Question Planning with Reflection","summary":"  Conversational Question Generation (CQG) enhances the interactivity of\nconversational question-answering systems in fields such as education, customer\nservice, and entertainment. However, traditional CQG, focusing primarily on the\nimmediate context, lacks the conversational foresight necessary to guide\nconversations toward specified conclusions. This limitation significantly\nrestricts their ability to achieve conclusion-oriented conversational outcomes.\nIn this work, we redefine the CQG task as Conclusion-driven Conversational\nQuestion Generation (CCQG) by focusing on proactivity, not merely reacting to\nthe unfolding conversation but actively steering it towards a\nconclusion-oriented question-answer pair. To address this, we propose a novel\napproach, called Proactive Conversational Question Planning with self-Refining\n(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte\nCarlo Tree Search (MCTS) with the analytical capabilities of large language\nmodels (LLMs), PCQPR predicts future conversation turns and continuously\nrefines its questioning strategies. This iterative self-refining mechanism\nensures the generation of contextually relevant questions strategically devised\nto reach a specified outcome. Our extensive evaluations demonstrate that PCQPR\nsignificantly surpasses existing CQG methods, marking a paradigm shift towards\nconclusion-oriented conversational question-answering systems.\n","authors":["Shasha Guo","Lizi Liao","Jing Zhang","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01363v1.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2402.10770v4","updated":"2024-10-02T09:18:47Z","published":"2024-02-16T15:48:33Z","title":"How Reliable Are Automatic Evaluation Methods for Instruction-Tuned\n  LLMs?","summary":"  Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.\n","authors":["Ehsan Doostmohammadi","Oskar Holmström","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2402.10770v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Défossez","Laurent Mazaré","Manu Orsini","Amélie Royer","Patrick Pérez","Hervé Jégou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01353v1","updated":"2024-10-02T09:11:10Z","published":"2024-10-02T09:11:10Z","title":"Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?","summary":"  Code completion, a key downstream task in code generation, is one of the most\nfrequent and impactful methods for enhancing developer productivity in software\ndevelopment. As intelligent completion tools evolve, we need a robust\nevaluation benchmark that enables meaningful comparisons between products and\nguides future advancements. However, existing benchmarks focus more on\ncoarse-grained tasks without industrial analysis resembling general code\ngeneration rather than the real-world scenarios developers encounter. Moreover,\nthese benchmarks often rely on costly and time-consuming human annotation, and\nthe standalone test cases fail to leverage minimal tests for maximum\nrepository-level understanding and code coverage. To address these limitations,\nwe first analyze business data from an industrial code completion tool and\nredefine the evaluation criteria to better align with the developer's intent\nand desired completion behavior throughout the coding process. Based on these\ninsights, we introduce Codev-Agent, an agent-based system that automates\nrepository crawling, constructs execution environments, extracts dynamic\ncalling chains from existing unit tests, and generates new test samples to\navoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent,\nwe present the Code-Development Benchmark (Codev-Bench), a fine-grained,\nreal-world, repository-level, and developer-centric evaluation framework.\nCodev-Bench assesses whether a code completion tool can capture a developer's\nimmediate intent and suggest appropriate code across diverse contexts,\nproviding a more realistic benchmark for code completion in modern software\ndevelopment.\n","authors":["Zhenyu Pan","Rongyu Cao","Yongchang Cao","Yingwei Ma","Binhua Li","Fei Huang","Han Liu","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16040v2","updated":"2024-10-02T09:08:21Z","published":"2024-09-24T12:42:18Z","title":"Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of\n  Experts","summary":"  Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility.\n","authors":["Xiaoming Shi","Shiyu Wang","Yuqi Nie","Dianqi Li","Zhou Ye","Qingsong Wen","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2409.16040v2.pdf","comment":"30 pages, 10 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.01350v1","updated":"2024-10-02T09:07:33Z","published":"2024-10-02T09:07:33Z","title":"Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and\n  Memory-Augmented Context-Aware Timbre Modeling","summary":"  Zero-shot voice conversion (VC) aims to transform the source speaker timbre\ninto an arbitrary unseen one without altering the original speech content.While\nrecent advancements in zero-shot VC methods have shown remarkable progress,\nthere still remains considerable potential for improvement in terms of\nimproving speaker similarity and speech naturalness.In this paper, we propose\nTakin-VC, a novel zero-shot VC framework based on jointly hybrid content and\nmemory-augmented context-aware timbre modeling to tackle this challenge.\nSpecifically, an effective hybrid content encoder, guided by neural codec\ntraining, that leverages quantized features from pre-trained WavLM and\nHybridFormer is first presented to extract the linguistic content of the source\nspeech. Subsequently, we introduce an advanced cross-attention-based\ncontext-aware timbre modeling approach that learns the fine-grained,\nsemantically associated target timbre features. To further enhance both speaker\nsimilarity and real-time performance, we utilize a conditional flow matching\nmodel to reconstruct the Mel-spectrogram of the source speech. Additionally, we\nadvocate an efficient memory-augmented module designed to generate high-quality\nconditional target inputs for the flow matching process, thereby improving the\noverall performance of the proposed system. Experimental results demonstrate\nthat the proposed Takin-VC method surpasses state-of-the-art zero-shot VC\nsystems, delivering superior performance in terms of both speech naturalness\nand speaker similarity.\n","authors":["Yuguang Yang","Yu Pan","Jixun Yao","Xiang Zhang","Jianhao Ye","Hongbin Zhou","Lei Xie","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.01350v1.pdf","comment":"Work in Progress; Under Review"},{"id":"http://arxiv.org/abs/2410.01349v1","updated":"2024-10-02T09:06:54Z","published":"2024-10-02T09:06:54Z","title":"Life, uh, Finds a Way: Systematic Neural Search","summary":"  We tackle the challenge of rapidly adapting an agent's behavior to solve\nspatiotemporally continuous problems in novel settings. Animals exhibit\nextraordinary abilities to adapt to new contexts, a capacity unmatched by\nartificial systems. Instead of focusing on generalization through deep\nreinforcement learning, we propose viewing behavior as the physical\nmanifestation of a search procedure, where robust problem-solving emerges from\nan exhaustive search across all possible behaviors. Surprisingly, this can be\ndone efficiently using online modification of a cognitive graph that guides\naction, challenging the predominant view that exhaustive search in continuous\nspaces is impractical. We describe an algorithm that implicitly enumerates\nbehaviors by regulating the tight feedback loop between execution of behaviors\nand mutation of the graph, and provide a neural implementation based on Hebbian\nlearning and a novel high-dimensional harmonic representation inspired by\nentorhinal cortex. By framing behavior as search, we provide a mathematically\nsimple and biologically plausible model for real-time behavioral adaptation,\nsuccessfully solving a variety of continuous state-space navigation problems.\nThis framework not only offers a flexible neural substrate for other\napplications but also presents a powerful paradigm for understanding adaptive\nbehavior. Our results suggest potential advancements in developmental learning\nand unsupervised skill acquisition, paving the way for autonomous robots to\nmaster complex skills in data-sparse environments demanding flexibility.\n","authors":["Alex Baranski","Jun Tani"],"pdf_url":"https://arxiv.org/pdf/2410.01349v1.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.12478v2","updated":"2024-10-02T09:02:19Z","published":"2024-01-23T04:16:58Z","title":"Mini-batch Submodular Maximization","summary":"  We present the first mini-batch algorithm for maximizing a non-negative\nmonotone decomposable submodular function, $F=\\sum_{i=1}^N f^i$, under a set of\nconstraints. We consider two sampling approaches: uniform and weighted. We\nfirst show that mini-batch with weighted sampling improves over the state of\nthe art sparsifier based approach both in theory and in practice.\n  Surprisingly, our experimental results show that uniform sampling is superior\nto weighted sampling. However, it is impossible to explain this using\nworst-case analysis. Our main contribution is using smoothed analysis to\nprovide a theoretical foundation for our experimental results. We show that,\nunder very mild assumptions, uniform sampling is superior for both the\nmini-batch and the sparsifier approaches. We empirically verify that these\nassumptions hold for our datasets. Uniform sampling is simple to implement and\nhas complexity independent of $N$, making it the perfect candidate to tackle\nmassive real-world datasets.\n","authors":["Gregory Schwartzman"],"pdf_url":"https://arxiv.org/pdf/2401.12478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01337v1","updated":"2024-10-02T08:54:18Z","published":"2024-10-02T08:54:18Z","title":"PhyMPGN: Physics-encoded Message Passing Graph Network for\n  spatiotemporal PDE systems","summary":"  Solving partial differential equations (PDEs) serves as a cornerstone for\nmodeling complex dynamical systems. Recent progresses have demonstrated grand\nbenefits of data-driven neural-based models for predicting spatiotemporal\ndynamics (e.g., tremendous speedup gain compared with classical numerical\nmethods). However, most existing neural models rely on rich training data, have\nlimited extrapolation and generalization abilities, and suffer to produce\nprecise or reliable physical prediction under intricate conditions (e.g.,\nirregular mesh or geometry, complex boundary conditions, diverse PDE\nparameters, etc.). To this end, we propose a new graph learning approach,\nnamely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model\nspatiotemporal PDE systems on irregular meshes given small training datasets.\nSpecifically, we incorporate a GNN into a numerical integrator to approximate\nthe temporal marching of spatiotemporal dynamics for a given PDE system.\nConsidering that many physical phenomena are governed by diffusion processes,\nwe further design a learnable Laplace block, which encodes the discrete\nLaplace-Beltrami operator, to aid and guide the GNN learning in a physically\nfeasible solution space. A boundary condition padding strategy is also designed\nto improve the model convergence and accuracy. Extensive experiments\ndemonstrate that PhyMPGN is capable of accurately predicting various types of\nspatiotemporal dynamics on coarse unstructured meshes, consistently achieves\nthe state-of-the-art results, and outperforms other baselines with considerable\ngains.\n","authors":["Bocheng Zeng","Qi Wang","Mengtao Yan","Yang Liu","Ruizhi Chengze","Yi Zhang","Hongsheng Liu","Zidong Wang","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2410.01337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01335v1","updated":"2024-10-02T08:53:07Z","published":"2024-10-02T08:53:07Z","title":"Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models","summary":"  Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.\n","authors":["Lucas Bandarkar","Benjamin Muller","Pritish Yuvraj","Rui Hou","Nayan Singhal","Hongjiang Lv","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01335v1.pdf","comment":"11 main pages, 23 pages total, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.01334v1","updated":"2024-10-02T08:52:58Z","published":"2024-10-02T08:52:58Z","title":"Unveiling Language Skills under Circuits","summary":"  The exploration of language skills in language models (LMs) has always been\none of the central goals in mechanistic interpretability. However, existing\ncircuit analyses often fall short in representing the full functional scope of\nthese models, primarily due to the exclusion of Feed-Forward layers.\nAdditionally, isolating the effect of a single language skill from a text,\nwhich inherently involves multiple entangled skills, poses a significant\nchallenge. To address these gaps, we introduce a novel concept, Memory Circuit,\na minimum unit that fully and independently manipulates the memory-reading\nfunctionality of a language model, and disentangle the transformer model\nprecisely into a circuit graph which is an ensemble of paths connecting\ndifferent memory circuits. Based on this disentanglement, we identify salient\ncircuit paths, named as skill paths, responsible for three crucial language\nskills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning\n(ICL) Skill, leveraging causal effect estimation through interventions and\ncounterfactuals. Our experiments on various datasets confirm the correspondence\nbetween our identified skill paths and language skills, and validate three\nlongstanding hypotheses: 1) Language skills are identifiable through circuit\ndissection; 2) Simple language skills reside in shallow layers, whereas complex\nlanguage skills are found in deeper layers; 3) Complex language skills are\nformed on top of simpler language skills. Our codes are available at:\nhttps://github.com/Zodiark-ch/Language-Skill-of-LLMs.\n","authors":["Hang Chen","Jiaying Zhu","Xinyu Yang","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13979v3","updated":"2024-10-02T08:51:45Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03199v2","updated":"2024-10-02T08:45:32Z","published":"2024-05-24T13:33:11Z","title":"Bayesian WeakS-to-Strong from Text Classification to Generation","summary":"  Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.\n","authors":["Ziyun Cui","Ziyang Zhang","Wen Wu","Guangzhi Sun","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13550v2","updated":"2024-10-02T08:32:31Z","published":"2024-02-21T06:11:03Z","title":"Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues","summary":"  A successful negotiation requires a range of capabilities, including\ncomprehension of the conversation context, Theory-of-Mind (ToM) skills to infer\nthe partner's motives, strategic reasoning, and effective communication, making\nit challenging for automated systems. Despite the remarkable performance of\nLLMs in various NLP tasks, there is no systematic evaluation of their\ncapabilities in negotiation. Such an evaluation is critical for advancing AI\nnegotiation agents and negotiation research, ranging from designing dialogue\nsystems to providing pedagogical feedback and scaling up data collection\npractices. This work aims to systematically analyze the multifaceted\ncapabilities of LLMs across diverse dialogue scenarios throughout the stages of\na typical negotiation interaction. Our analysis highlights GPT-4's superior\nperformance in many tasks while identifying specific challenges, such as making\nsubjective assessments and generating contextually appropriate, strategically\nadvantageous responses.\n","authors":["Deuksin Kwon","Emily Weiss","Tara Kulshrestha","Kushal Chawla","Gale M. Lucas","Jonathan Gratch"],"pdf_url":"https://arxiv.org/pdf/2402.13550v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01324v1","updated":"2024-10-02T08:32:21Z","published":"2024-10-02T08:32:21Z","title":"Fair Class-Incremental Learning using Sample Weighting","summary":"  Model fairness is becoming important in class-incremental learning for\nTrustworthy AI. While accuracy has been a central focus in class-incremental\nlearning, fairness has been relatively understudied. However, naively using all\nthe samples of the current task for training results in unfair catastrophic\nforgetting for certain sensitive groups including classes. We theoretically\nanalyze that forgetting occurs if the average gradient vector of the current\ntask data is in an \"opposite direction\" compared to the average gradient vector\nof a sensitive group, which means their inner products are negative. We then\npropose a fair class-incremental learning framework that adjusts the training\nweights of current task samples to change the direction of the average gradient\nvector and thus reduce the forgetting of underperforming groups and achieve\nfairness. For various group fairness measures, we formulate optimization\nproblems to minimize the overall losses of sensitive groups while minimizing\nthe disparities among them. We also show the problems can be solved with linear\nprogramming and propose an efficient Fairness-aware Sample Weighting (FSW)\nalgorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff\nresults than state-of-the-art approaches on real datasets.\n","authors":["Jaeyoung Park","Minsu Kim","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2410.01324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01322v1","updated":"2024-10-02T08:26:37Z","published":"2024-10-02T08:26:37Z","title":"Forte : Finding Outliers with Representation Typicality Estimation","summary":"  Generative models can now produce photorealistic synthetic data which is\nvirtually indistinguishable from the real data used to train it. This is a\nsignificant evolution over previous models which could produce reasonable\nfacsimiles of the training data, but ones which could be visually distinguished\nfrom the training data by human evaluation. Recent work on OOD detection has\nraised doubts that generative model likelihoods are optimal OOD detectors due\nto issues involving likelihood misestimation, entropy in the generative\nprocess, and typicality. We speculate that generative OOD detectors also failed\nbecause their models focused on the pixels rather than the semantic content of\nthe data, leading to failures in near-OOD cases where the pixels may be similar\nbut the information content is significantly different. We hypothesize that\nestimating typical sets using self-supervised learners leads to better OOD\ndetectors. We introduce a novel approach that leverages representation\nlearning, and informative summary statistics based on manifold estimation, to\naddress all of the aforementioned issues. Our method outperforms other\nunsupervised approaches and achieves state-of-the art performance on\nwell-established challenging benchmarks, and new synthetic data detection\ntasks.\n","authors":["Debargha Ganguly","Warren Morningstar","Andrew Yu","Vipin Chaudhary"],"pdf_url":"https://arxiv.org/pdf/2410.01322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16223v2","updated":"2024-10-02T08:23:07Z","published":"2024-09-24T16:35:16Z","title":"Fine-Tuning is Fine, if Calibrated","summary":"  Fine-tuning is arguably the most straightforward way to tailor a pre-trained\nmodel (e.g., a foundation model) to downstream applications, but it also comes\nwith the risk of losing valuable knowledge the model had learned in\npre-training. For example, fine-tuning a pre-trained classifier capable of\nrecognizing a large number of classes to master a subset of classes at hand is\nshown to drastically degrade the model's accuracy in the other classes it had\npreviously learned. As such, it is hard to further use the fine-tuned model\nwhen it encounters classes beyond the fine-tuning data. In this paper, we\nsystematically dissect the issue, aiming to answer the fundamental question,\n\"What has been damaged in the fine-tuned model?\" To our surprise, we find that\nthe fine-tuned model neither forgets the relationship among the other classes\nnor degrades the features to recognize these classes. Instead, the fine-tuned\nmodel often produces more discriminative features for these other classes, even\nif they were missing during fine-tuning! {What really hurts the accuracy is the\ndiscrepant logit scales between the fine-tuning classes and the other classes},\nimplying that a simple post-processing calibration would bring back the\npre-trained model's capability and at the same time unveil the feature\nimprovement over all classes. We conduct an extensive empirical study to\ndemonstrate the robustness of our findings and provide preliminary explanations\nunderlying them, suggesting new directions for future theoretical analysis. Our\ncode is available at\nhttps://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.\n","authors":["Zheda Mai","Arpita Chowdhury","Ping Zhang","Cheng-Hao Tu","Hong-You Chen","Vardaan Pahuja","Tanya Berger-Wolf","Song Gao","Charles Stewart","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2409.16223v2.pdf","comment":"The first three authors contribute equally. The paper has been\n  accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01319v1","updated":"2024-10-02T08:22:42Z","published":"2024-10-02T08:22:42Z","title":"Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object\n  Detection by Bridging Domain Gaps","summary":"  LiDAR-based 3D object detectors have been largely utilized in various\napplications, including autonomous vehicles or mobile robots. However,\nLiDAR-based detectors often fail to adapt well to target domains with different\nsensor configurations (e.g., types of sensors, spatial resolution, or FOVs) and\nlocation shifts. Collecting and annotating datasets in a new setup is commonly\nrequired to reduce such gaps, but it is often expensive and time-consuming.\nRecent studies suggest that pre-trained backbones can be learned in a\nself-supervised manner with large-scale unlabeled LiDAR frames. However,\ndespite their expressive representations, they remain challenging to generalize\nwell without substantial amounts of data from the target domain. Thus, we\npropose a novel method, called Domain Adaptive Distill-Tuning (DADT), to adapt\na pre-trained model with limited target data (approximately 100 LiDAR frames),\nretaining its representation power and preventing it from overfitting.\nSpecifically, we use regularizers to align object-level and context-level\nrepresentations between the pre-trained and finetuned models in a\nteacher-student architecture. Our experiments with driving benchmarks, i.e.,\nWaymo Open dataset and KITTI, confirm that our method effectively finetunes a\npre-trained model, achieving significant gains in accuracy.\n","authors":["Jiyun Jang","Mincheol Chang","Jongwon Park","Jinkyu Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01319v1.pdf","comment":"Accepted in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2406.04657v2","updated":"2024-10-02T08:10:29Z","published":"2024-06-07T05:51:57Z","title":"Crafting Heavy-Tails in Weight Matrix Spectrum without Gradient Noise","summary":"  Training strategies for modern deep neural networks (NNs) tend to induce a\nheavy-tailed (HT) empirical spectral density (ESD) in the layer weights. While\nprevious efforts have shown that the HT phenomenon correlates with good\ngeneralization in large NNs, a theoretical explanation of its occurrence is\nstill lacking. Especially, understanding the conditions which lead to this\nphenomenon can shed light on the interplay between generalization and weight\nspectra. Our work aims to bridge this gap by presenting a simple, rich setting\nto model the emergence of HT ESD. In particular, we present a theory-informed\nanalysis for 'crafting' heavy tails in the ESD of two-layer NNs without any\ngradient noise. This is the first work to analyze a noise-free setting and\nincorporate optimizer (GD/Adam) dependent (large) learning rates into the HT\nESD analysis. Our results highlight the role of learning rates on the\nBulk+Spike and HT shape of the ESDs in the early phase of training, which can\nfacilitate generalization in the two-layer NN. These observations shed light on\nthe behavior of large-scale NNs, albeit in a much simpler setting. Last but not\nleast, we present a novel perspective on the ESD evolution dynamics by\nanalyzing the singular vectors of weight matrices and optimizer updates.\n","authors":["Vignesh Kothapalli","Tianyu Pang","Shenyang Deng","Zongmin Liu","Yaoqing Yang"],"pdf_url":"https://arxiv.org/pdf/2406.04657v2.pdf","comment":"34 pages, 32 figures, 4 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.01802v1","updated":"2024-10-02T17:57:38Z","published":"2024-10-02T17:57:38Z","title":"PROXI: Challenging the GNNs for Link Prediction","summary":"  Over the past decade, Graph Neural Networks (GNNs) have transformed graph\nrepresentation learning. In the widely adopted message-passing GNN framework,\nnodes refine their representations by aggregating information from neighboring\nnodes iteratively. While GNNs excel in various domains, recent theoretical\nstudies have raised concerns about their capabilities. GNNs aim to address\nvarious graph-related tasks by utilizing such node representations, however,\nthis one-size-fits-all approach proves suboptimal for diverse tasks.\n  Motivated by these observations, we conduct empirical tests to compare the\nperformance of current GNN models with more conventional and direct methods in\nlink prediction tasks. Introducing our model, PROXI, which leverages proximity\ninformation of node pairs in both graph and attribute spaces, we find that\nstandard machine learning (ML) models perform competitively, even outperforming\ncutting-edge GNN models when applied to these proximity metrics derived from\nnode neighborhoods and attributes. This holds true across both homophilic and\nheterophilic networks, as well as small and large benchmark datasets, including\nthose from the Open Graph Benchmark (OGB). Moreover, we show that augmenting\ntraditional GNNs with PROXI significantly boosts their link prediction\nperformance. Our empirical findings corroborate the previously mentioned\ntheoretical observations and imply that there exists ample room for enhancement\nin current GNN models to reach their potential.\n","authors":["Astrit Tola","Jack Myrick","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.01802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01803v1","updated":"2024-10-02T17:57:38Z","published":"2024-10-02T17:57:38Z","title":"On the expressiveness and spectral bias of KANs","summary":"  Kolmogorov-Arnold Networks (KAN) \\cite{liu2024kan} were very recently\nproposed as a potential alternative to the prevalent architectural backbone of\nmany deep learning models, the multi-layer perceptron (MLP). KANs have seen\nsuccess in various tasks of AI for science, with their empirical efficiency and\naccuracy demostrated in function regression, PDE solving, and many more\nscientific problems.\n  In this article, we revisit the comparison of KANs and MLPs, with emphasis on\na theoretical perspective. On the one hand, we compare the representation and\napproximation capabilities of KANs and MLPs. We establish that MLPs can be\nrepresented using KANs of a comparable size. This shows that the approximation\nand representation capabilities of KANs are at least as good as MLPs.\nConversely, we show that KANs can be represented using MLPs, but that in this\nrepresentation the number of parameters increases by a factor of the KAN grid\nsize. This suggests that KANs with a large grid size may be more efficient than\nMLPs at approximating certain functions. On the other hand, from the\nperspective of learning and optimization, we study the spectral bias of KANs\ncompared with MLPs. We demonstrate that KANs are less biased toward low\nfrequencies than MLPs. We highlight that the multi-level learning feature\nspecific to KANs, i.e. grid extension of splines, improves the learning process\nfor high-frequency components. Detailed comparisons with different choices of\ndepth, width, and grid sizes of KANs are made, shedding some light on how to\nchoose the hyperparameters in practice.\n","authors":["Yixuan Wang","Jonathan W. Siegel","Ziming Liu","Thomas Y. Hou"],"pdf_url":"https://arxiv.org/pdf/2410.01803v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.01799v1","updated":"2024-10-02T17:56:32Z","published":"2024-10-02T17:56:32Z","title":"Efficient $1$-bit tensor approximations","summary":"  We present a spatially efficient decomposition of matrices and\narbitrary-order tensors as linear combinations of tensor products of $\\{-1,\n1\\}$-valued vectors. For any matrix $A \\in \\mathbb{R}^{m \\times n}$, $$A - R_w\n= S_w C_w T_w^\\top = \\sum_{j=1}^w c_j \\cdot \\mathbf{s}_j \\mathbf{t}_j^\\top$$ is\na {\\it $w$-width signed cut decomposition of $A$}. Here $C_w =\n\"diag\"(\\mathbf{c}_w)$ for some $\\mathbf{c}_w \\in \\mathbb{R}^w,$ and $S_w, T_w$,\nand the vectors $\\mathbf{s}_j, \\mathbf{t}_j$ are $\\{-1, 1\\}$-valued. To store\n$(S_w, T_w, C_w)$, we may pack $w \\cdot (m + n)$ bits, and require only $w$\nfloating point numbers. As a function of $w$, $\\|R_w\\|_F$ exhibits exponential\ndecay when applied to #f32 matrices with i.i.d. $\\mathcal N (0, 1)$ entries.\nChoosing $w$ so that $(S_w, T_w, C_w)$ has the same memory footprint as a\n\\textit{f16} or \\textit{bf16} matrix, the relative error is comparable. Our\nalgorithm yields efficient signed cut decompositions in $20$ lines of\npseudocode. It reflects a simple modification from a celebrated 1999 paper [1]\nof Frieze and Kannan. As a first application, we approximate the weight\nmatrices in the open \\textit{Mistral-7B-v0.1} Large Language Model to a $50\\%$\nspatial compression. Remarkably, all $226$ remainder matrices have a relative\nerror $<6\\%$ and the expanded model closely matches \\textit{Mistral-7B-v0.1} on\nthe {\\it huggingface} leaderboard [2]. Benchmark performance degrades slowly as\nwe reduce the spatial compression from $50\\%$ to $25\\%$. We optimize our open\nsource \\textit{rust} implementation [3] with \\textit{simd} instructions on\n\\textit{avx2} and \\textit{avx512} architectures. We also extend our algorithm\nfrom matrices to tensors of arbitrary order and use it to compress a picture of\nthe first author's cat Angus.\n","authors":["Alex W. Neal Riasanovsky","Sarah El Kazdadi"],"pdf_url":"https://arxiv.org/pdf/2410.01799v1.pdf","comment":"16 pages, one cat picture reused a lot"},{"id":"http://arxiv.org/abs/2410.01796v1","updated":"2024-10-02T17:53:23Z","published":"2024-10-02T17:53:23Z","title":"Bellman Diffusion: Generative Modeling as Learning a Linear Operator in\n  the Distribution Space","summary":"  Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and\nScore-based Generative Models (SGMs), have advanced high-fidelity data\ngeneration and complex continuous distribution approximation. However, their\napplication in Markov Decision Processes (MDPs), particularly in distributional\nReinforcement Learning (RL), remains underexplored, with conventional\nhistogram-based methods dominating the field. This paper rigorously highlights\nthat this application gap is caused by the nonlinearity of modern DGMs, which\nconflicts with the linearity required by the Bellman equation in MDPs. For\ninstance, EBMs involve nonlinear operations such as exponentiating energy\nfunctions and normalizing constants. To address this, we introduce Bellman\nDiffusion, a novel DGM framework that maintains linearity in MDPs through\ngradient and scalar field modeling. With divergence-based training techniques\nto optimize neural network proxies and a new type of stochastic differential\nequation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the\ntarget distribution. Our empirical results show that Bellman Diffusion achieves\naccurate field estimations and is a capable image generator, converging 1.5x\nfaster than the traditional histogram-based baseline in distributional RL\ntasks. This work enables the effective integration of DGMs into MDP\napplications, unlocking new avenues for advanced decision-making frameworks.\n","authors":["Yangming Li","Chieh-Hsin Lai","Carola-Bibiane Schönlieb","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.01796v1.pdf","comment":"Paper under review"},{"id":"http://arxiv.org/abs/2410.01795v1","updated":"2024-10-02T17:53:08Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01793v1","updated":"2024-10-02T17:51:58Z","published":"2024-10-02T17:51:58Z","title":"Thermodynamic Bayesian Inference","summary":"  A fully Bayesian treatment of complicated predictive models (such as deep\nneural networks) would enable rigorous uncertainty quantification and the\nautomation of higher-level tasks including model selection. However, the\nintractability of sampling Bayesian posteriors over many parameters inhibits\nthe use of Bayesian methods where they are most needed. Thermodynamic computing\nhas emerged as a paradigm for accelerating operations used in machine learning,\nsuch as matrix inversion, and is based on the mapping of Langevin equations to\nthe dynamics of noisy physical systems. Hence, it is natural to consider the\nimplementation of Langevin sampling algorithms on thermodynamic devices. In\nthis work we propose electronic analog devices that sample from Bayesian\nposteriors by realizing Langevin dynamics physically. Circuit designs are given\nfor sampling the posterior of a Gaussian-Gaussian model and for Bayesian\nlogistic regression, and are validated by simulations. It is shown, under\nreasonable assumptions, that the Bayesian posteriors for these models can be\nsampled in time scaling with $\\ln(d)$, where $d$ is dimension. For the\nGaussian-Gaussian model, the energy cost is shown to scale with $ d \\ln(d)$.\nThese results highlight the potential for fast, energy-efficient Bayesian\ninference using thermodynamic computing.\n","authors":["Maxwell Aifer","Samuel Duffield","Kaelan Donatella","Denis Melanson","Phoebe Klett","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2410.01793v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.01789v1","updated":"2024-10-02T17:46:22Z","published":"2024-10-02T17:46:22Z","title":"Investigating on RLHF methodology","summary":"  In this article, we investigate the alignment of Large Language Models\naccording to human preferences. We discuss the features of training a\nPreference Model, which simulates human preferences, and the methods and\ndetails we found essential for achieving the best results. We also discuss\nusing Reinforcement Learning to fine-tune Large Language Models and describe\nthe challenges we faced and the ways to overcome them. Additionally, we present\nour experience with the Direct Preference Optimization method, which enables us\nto align a Large Language Model with human preferences without creating a\nseparate Preference Model. As our contribution, we introduce the approach for\ncollecting a preference dataset through perplexity filtering, which makes the\nprocess of creating such a dataset for a specific Language Model much easier\nand more cost-effective.\n","authors":["Alexey Kutalev","Sergei Markoff"],"pdf_url":"https://arxiv.org/pdf/2410.01789v1.pdf","comment":"23 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01786v1","updated":"2024-10-02T17:42:16Z","published":"2024-10-02T17:42:16Z","title":"Learning To Solve Differential Equation Constrained Optimization\n  Problems","summary":"  Differential equations (DE) constrained optimization plays a critical role in\nnumerous scientific and engineering fields, including energy systems, aerospace\nengineering, ecology, and finance, where optimal configurations or control\nstrategies must be determined for systems governed by ordinary or stochastic\ndifferential equations. Despite its significance, the computational challenges\nassociated with these problems have limited their practical use. To address\nthese limitations, this paper introduces a learning-based approach to\nDE-constrained optimization that combines techniques from proxy optimization\nand neural differential equations. The proposed approach uses a dual-network\narchitecture, with one approximating the control strategies, focusing on\nsteady-state constraints, and another solving the associated DEs. This\ncombination enables the approximation of optimal strategies while accounting\nfor dynamic constraints in near real-time. Experiments across problems in\nenergy optimization and finance modeling show that this method provides full\ncompliance with dynamic constraints and it produces results up to 25 times more\nprecise than other methods which do not explicitly model the system's dynamic\nequations.\n","authors":["Vincenzo Di Vito","Mostafa Mohammadian","Kyri Baker","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2410.01786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.01445v3","updated":"2024-10-02T17:34:06Z","published":"2024-07-01T16:37:18Z","title":"FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training\n  with Limited Resources","summary":"  Existing studies of training state-of-the-art Contrastive Language-Image\nPretraining (CLIP) models on large-scale data involve hundreds of or even\nthousands of GPUs due to the requirement of a large batch size. However, such a\nlarge amount of resources is not accessible to most people. While advanced\ncompositional optimization techniques for optimizing global contrastive losses\nhave been demonstrated effective for removing the requirement of large batch\nsize, their performance on large-scale data remains underexplored and not\noptimized. To bridge the gap, this paper explores several aspects of CLIP\ntraining with limited resources (e.g., up to tens of GPUs). First, we introduce\nFastCLIP, a general CLIP training framework built on advanced compositional\noptimization techniques while designed and optimized for the distributed\nsetting. Our framework is equipped with an efficient gradient reduction\nstrategy to reduce communication overhead. Second, to further boost training\nefficiency, we investigate three components of the framework from an\noptimization perspective: the schedule of the inner learning rate, the update\nrules of the temperature parameter and the model parameters, respectively.\nExperiments on different strategies for each component shed light on how to\nconduct CLIP training more efficiently. Finally, we benchmark the performance\nof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different\ncompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7\nmillion, 9.1 million to 315 million image-text pairs to demonstrate the\nsignificant improvement of FastCLIP in the resource-limited setting. We release\nthe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .\n","authors":["Xiyuan Wei","Fanjiang Ye","Ori Yonay","Xingyu Chen","Baixi Sun","Dingwen Tao","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01445v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01778v1","updated":"2024-10-02T17:31:33Z","published":"2024-10-02T17:31:33Z","title":"TopER: Topological Embeddings in Graph Representation Learning","summary":"  Graph embeddings play a critical role in graph representation learning,\nallowing machine learning models to explore and interpret graph-structured\ndata. However, existing methods often rely on opaque, high-dimensional\nembeddings, limiting interpretability and practical visualization.\n  In this work, we introduce Topological Evolution Rate (TopER), a novel,\nlow-dimensional embedding approach grounded in topological data analysis. TopER\nsimplifies a key topological approach, Persistent Homology, by calculating the\nevolution rate of graph substructures, resulting in intuitive and interpretable\nvisualizations of graph data. This approach not only enhances the exploration\nof graph datasets but also delivers competitive performance in graph clustering\nand classification tasks. Our TopER-based models achieve or surpass\nstate-of-the-art results across molecular, biological, and social network\ndatasets in tasks such as classification, clustering, and visualization.\n","authors":["Astrit Tola","Funmilola Mary Taiwom","Cuneyt Gurcan Akcora","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.01778v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.01776v1","updated":"2024-10-02T17:31:01Z","published":"2024-10-02T17:31:01Z","title":"Dynamical-generative downscaling of climate model ensembles","summary":"  Regional high-resolution climate projections are crucial for many\napplications, such as agriculture, hydrology, and natural hazard risk\nassessment. Dynamical downscaling, the state-of-the-art method to produce\nlocalized future climate information, involves running a regional climate model\n(RCM) driven by an Earth System Model (ESM), but it is too computationally\nexpensive to apply to large climate projection ensembles. We propose a novel\napproach combining dynamical downscaling with generative artificial\nintelligence to reduce the cost and improve the uncertainty estimates of\ndownscaled climate projections. In our framework, an RCM dynamically downscales\nESM output to an intermediate resolution, followed by a generative diffusion\nmodel that further refines the resolution to the target scale. This approach\nleverages the generalizability of physics-based models and the sampling\nefficiency of diffusion models, enabling the downscaling of large multi-model\nensembles. We evaluate our method against dynamically-downscaled climate\nprojections from the CMIP6 ensemble. Our results demonstrate its ability to\nprovide more accurate uncertainty bounds on future regional climate than\nalternatives such as dynamical downscaling of smaller ensembles, or traditional\nempirical statistical downscaling methods. We also show that\ndynamical-generative downscaling results in significantly lower errors than\nbias correction and spatial disaggregation (BCSD), and captures more accurately\nthe spectra and multivariate correlations of meteorological fields. These\ncharacteristics make the dynamical-generative framework a flexible, accurate,\nand efficient way to downscale large ensembles of climate projections,\ncurrently out of reach for pure dynamical downscaling.\n","authors":["Ignacio Lopez-Gomez","Zhong Yi Wan","Leonardo Zepeda-Núñez","Tapio Schneider","John Anderson","Fei Sha"],"pdf_url":"https://arxiv.org/pdf/2410.01776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01774v1","updated":"2024-10-02T17:30:21Z","published":"2024-10-02T17:30:21Z","title":"Trained Transformer Classifiers Generalize and Exhibit Benign\n  Overfitting In-Context","summary":"  Transformers have the capacity to act as supervised learning algorithms: by\nproperly encoding a set of labeled training (\"in-context\") examples and an\nunlabeled test example into an input sequence of vectors of the same dimension,\nthe forward pass of the transformer can produce predictions for that unlabeled\ntest example. A line of recent work has shown that when linear transformers are\npre-trained on random instances for linear regression tasks, these trained\ntransformers make predictions using an algorithm similar to that of ordinary\nleast squares. In this work, we investigate the behavior of linear transformers\ntrained on random linear classification tasks. Via an analysis of the implicit\nregularization of gradient descent, we characterize how many pre-training tasks\nand in-context examples are needed for the trained transformer to generalize\nwell at test-time. We further show that in some settings, these trained\ntransformers can exhibit \"benign overfitting in-context\": when in-context\nexamples are corrupted by label flipping noise, the transformer memorizes all\nof its in-context examples (including those with noisy labels) yet still\ngeneralizes near-optimally for clean test examples.\n","authors":["Spencer Frei","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.01774v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2407.12492v2","updated":"2024-10-02T17:29:54Z","published":"2024-07-17T11:18:49Z","title":"Temporal Test-Time Adaptation with State-Space Models","summary":"  Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.\n","authors":["Mona Schirmer","Dan Zhang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2407.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01771v1","updated":"2024-10-02T17:28:22Z","published":"2024-10-02T17:28:22Z","title":"Bayesian Binary Search","summary":"  We present Bayesian Binary Search (BBS), a novel probabilistic variant of the\nclassical binary search/bisection algorithm. BBS leverages machine\nlearning/statistical techniques to estimate the probability density of the\nsearch space and modifies the bisection step to split based on probability\ndensity rather than the traditional midpoint, allowing for the learned\ndistribution of the search space to guide the search algorithm. Search space\ndensity estimation can flexibly be performed using supervised probabilistic\nmachine learning techniques (e.g., Gaussian process regression, Bayesian neural\nnetworks, quantile regression) or unsupervised learning algorithms (e.g.,\nGaussian mixture models, kernel density estimation (KDE), maximum likelihood\nestimation (MLE)). We demonstrate significant efficiency gains of using BBS on\nboth simulated data across a variety of distributions and in a real-world\nbinary search use case of probing channel balances in the Bitcoin Lightning\nNetwork, for which we have deployed the BBS algorithm in a production setting.\n","authors":["Vikash Singh","Matthew Khanzadeh","Vincent Davis","Harrison Rush","Emanuele Rossi","Jesse Shrader","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2410.01771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01770v1","updated":"2024-10-02T17:27:13Z","published":"2024-10-02T17:27:13Z","title":"Explainable Earth Surface Forecasting under Extreme Events","summary":"  With climate change-related extreme events on the rise, high dimensional\nEarth observation data presents a unique opportunity for forecasting and\nunderstanding impacts on ecosystems. This is, however, impeded by the\ncomplexity of processing, visualizing, modeling, and explaining this data. To\nshowcase how this challenge can be met, here we train a convolutional long\nshort-term memory-based architecture on the novel DeepExtremeCubes dataset.\nDeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January\n2016-October 2022) worldwide, along with labeled extreme events, meteorological\ndata, vegetation land cover, and topography map, sampled from locations\naffected by extreme climate events and surrounding areas. When predicting\nfuture reflectances and vegetation impacts through kernel normalized difference\nvegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.\nExplainable artificial intelligence was used to analyze the model's predictions\nduring the October 2020 Central South America compound heatwave and drought\nevent. We chose the same area exactly one year before the event as\ncounterfactual, finding that the average temperature and surface pressure are\ngenerally the best predictors under normal conditions. In contrast, minimum\nanomalies of evaporation and surface latent heat flux take the lead during the\nevent. A change of regime is also observed in the attributions before the\nevent, which might help assess how long the event was brewing before happening.\nThe code to replicate all experiments and figures in this paper is publicly\navailable at https://github.com/DeepExtremes/txyXAI\n","authors":["Oscar J. Pellicer-Valero","Miguel-Ángel Fernández-Torres","Chaonan Ji","Miguel D. Mahecha","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2410.01770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01767v1","updated":"2024-10-02T17:22:09Z","published":"2024-10-02T17:22:09Z","title":"Decision-Focused Uncertainty Quantification","summary":"  There is increasing interest in ''decision-focused'' machine learning methods\nwhich train models to account for how their predictions are used in downstream\noptimization problems. Doing so can often improve performance on subsequent\ndecision problems. However, current methods for uncertainty quantification do\nnot incorporate any information at all about downstream decisions. We develop a\nframework based on conformal prediction to produce prediction sets that account\nfor a downstream decision loss function, making them more appropriate to inform\nhigh-stakes decision-making. Our approach harnesses the strengths of conformal\nmethods--modularity, model-agnosticism, and statistical coverage\nguarantees--while incorporating downstream decisions and user-specified utility\nfunctions. We prove that our methods retain standard coverage guarantees.\nEmpirical evaluation across a range of datasets and utility metrics\ndemonstrates that our methods achieve significantly lower decision loss\ncompared to standard conformal methods. Additionally, we present a real-world\nuse case in healthcare diagnosis, where our method effectively incorporates the\nhierarchical structure of dermatological diseases. It successfully generates\nsets with coherent diagnostic meaning, aiding the triage process during\ndermatology diagnosis and illustrating how our method can ground high-stakes\ndecision-making on external domain knowledge.\n","authors":["Santiago Cortes-Gomez","Carlos Patiño","Yewon Byun","Steven Wu","Eric Horvitz","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.01767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17253v2","updated":"2024-10-02T17:21:47Z","published":"2024-08-30T12:51:55Z","title":"VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters","summary":"  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time-series domain, the proposed VisionTS could achieve\nsuperior zero-shot forecasting performance compared to existing TSF foundation\nmodels. With fine-tuning for one epoch, VisionTS could further improve the\nforecasting and achieve state-of-the-art performance in most cases. Extensive\nexperiments reveal intrinsic similarities between images and real-world time\nseries, suggesting visual models may offer a ``free lunch'' for TSF and\nhighlight the potential for future cross-modality research. Our code is\npublicly available at https://github.com/Keytoyze/VisionTS.\n","authors":["Mouxiang Chen","Lefei Shen","Zhuo Li","Xiaoyun Joy Wang","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17253v2.pdf","comment":"v2: add more experiments"},{"id":"http://arxiv.org/abs/2410.01766v1","updated":"2024-10-02T17:21:43Z","published":"2024-10-02T17:21:43Z","title":"SegHeD: Segmentation of Heterogeneous Data for Multiple Sclerosis\n  Lesions with Anatomical Constraints","summary":"  Assessment of lesions and their longitudinal progression from brain magnetic\nresonance (MR) images plays a crucial role in diagnosing and monitoring\nmultiple sclerosis (MS). Machine learning models have demonstrated a great\npotential for automated MS lesion segmentation. Training such models typically\nrequires large-scale high-quality datasets that are consistently annotated.\nHowever, MS imaging datasets are often small, segregated across multiple sites,\nwith different formats (cross-sectional or longitudinal), and diverse\nannotation styles. This poses a significant challenge to train a unified MS\nlesion segmentation model. To tackle this challenge, we present SegHeD, a novel\nmulti-dataset multi-task segmentation model that can incorporate heterogeneous\ndata as input and perform all-lesion, new-lesion, as well as vanishing-lesion\nsegmentation. Furthermore, we account for domain knowledge about MS lesions,\nincorporating longitudinal, spatial, and volumetric constraints into the\nsegmentation model. SegHeD is assessed on five MS datasets and achieves a high\nperformance in all, new, and vanishing-lesion segmentation, outperforming\nseveral state-of-the-art methods in this field.\n","authors":["Berke Doga Basaran","Xinru Zhang","Paul M. Matthews","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2410.01766v1.pdf","comment":"13 pages, 4 figures, MICCAI, LDTM Workshop"},{"id":"http://arxiv.org/abs/2406.10995v2","updated":"2024-10-02T17:20:28Z","published":"2024-06-16T16:15:20Z","title":"Concept-skill Transferability-based Data Selection for Large\n  Vision-Language Models","summary":"  Instruction tuning, or supervised finetuning on extensive task-specific data,\nis necessary for Large Vision-Language Models (LVLMs) to generalize well across\na broad range of vision-language (VL) tasks. However, training on large VL\ndatasets can become prohibitively expensive. In this work, we introduce\nCOINCIDE, an effective and scalable data selection technique that uses a small\nmodel as a reference model to select visual instruction tuning data for\nefficient finetuning of a target LVLM, focusing on diversity and\ntransferability. Specifically, we cluster the training data using internal\nactivations from a small model, which identifies VL concept-skill compositions\nneeded by a target LVLM. We then sample data from these diverse clusters by\nconsidering their density and transferability, or the ability to transfer well\nto other concept-skill compositions. This approach ensures the diversity of\nthese compositions, which is vital for LVLM generalization. Extensive\nexperiments demonstrate that COINCIDE achieves superior performance and data\nselection efficiency against 8 strong baselines on two distinct datasets:\nLLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE\nachieves performance comparable to the LVLM finetuned on the whole dataset,\nwith 70% reduction of the wall-clock running time. On the Vision-Flan dataset,\nour method achieves superior results with only 16.7% of the training data.\n","authors":["Jaewoo Lee","Boyang Li","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.10995v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.01100v2","updated":"2024-10-02T17:09:53Z","published":"2024-07-01T09:06:57Z","title":"Eliminating Position Bias of Language Models: A Mechanistic Approach","summary":"  Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.\n","authors":["Ziqi Wang","Hanlin Zhang","Xiner Li","Kuan-Hao Huang","Chi Han","Shuiwang Ji","Sham M. Kakade","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.01100v2.pdf","comment":"26 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2410.01755v1","updated":"2024-10-02T17:05:48Z","published":"2024-10-02T17:05:48Z","title":"Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes","summary":"  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model designed for protein sequences, we\ngenerated embeddings that capture the functional and structural properties of\nproteins sequence. These embeddings were integrated with protein expression\nlevel to form enriched biological representations, which were analyzed using\nmachine learning methods like ensemble K-means for clustering and XGBoost for\nclassification. Our approach enabled successful clustering of patients into\nbiologically distinct groups and accurately predicted clinical outcomes such as\nsurvival and biomarkers status, achieving high performance metrics, notably an\nF1 score of 0.88 for survival and 0.87 for biomarkers status prediction.\nAnalysis of feature importance highlighted key proteins like KMT2C, GCN1, and\nCLASP2, linked to hormone receptor and Human Epidermal Growth Factor Receptor 2\n(HER2) expression, which play a role in tumor progression and patient outcomes,\nrespectively. Furthermore, protein-protein interaction networks and correlation\nanalyses revealed the interdependence of proteins that may influence breast\ncancer subtype behaviors. These findings suggest that integrating protein\nsequence and expression data provides valuable insights into tumor biology and\nhas significant potential to enhance personalized treatment strategies in\nbreast cancer care.\n","authors":["Hossein Sholehrasa"],"pdf_url":"https://arxiv.org/pdf/2410.01755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01752v1","updated":"2024-10-02T17:02:17Z","published":"2024-10-02T17:02:17Z","title":"TorchSISSO: A PyTorch-Based Implementation of the Sure Independence\n  Screening and Sparsifying Operator for Efficient and Interpretable Model\n  Discovery","summary":"  Symbolic regression (SR) is a powerful machine learning approach that\nsearches for both the structure and parameters of algebraic models, offering\ninterpretable and compact representations of complex data. Unlike traditional\nregression methods, SR explores progressively complex feature spaces, which can\nuncover simple models that generalize well, even from small datasets. Among SR\nalgorithms, the Sure Independence Screening and Sparsifying Operator (SISSO)\nhas proven particularly effective in the natural sciences, helping to\nrediscover fundamental physical laws as well as discover new interpretable\nequations for materials property modeling. However, its widespread adoption has\nbeen limited by performance inefficiencies and the challenges posed by its\nFORTRAN-based implementation, especially in modern computing environments. In\nthis work, we introduce TorchSISSO, a native Python implementation built in the\nPyTorch framework. TorchSISSO leverages GPU acceleration, easy integration, and\nextensibility, offering a significant speed-up and improved accuracy over the\noriginal. We demonstrate that TorchSISSO matches or exceeds the performance of\nthe original SISSO across a range of tasks, while dramatically reducing\ncomputational time and improving accessibility for broader scientific\napplications.\n","authors":["Madhav Muthyala","Farshud Sorourifar","Joel A. Paulson"],"pdf_url":"https://arxiv.org/pdf/2410.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04194v2","updated":"2024-10-02T17:01:58Z","published":"2024-09-06T11:24:25Z","title":"Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models","summary":"  Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.\n","authors":["Malte Luttermann","Ralf Möller","Mattis Hartwig"],"pdf_url":"https://arxiv.org/pdf/2409.04194v2.pdf","comment":"Accepted to the Proceedings of the 47th German Conference on\n  Artificial Intelligence (KI 2024)"},{"id":"http://arxiv.org/abs/2410.01748v1","updated":"2024-10-02T17:01:10Z","published":"2024-10-02T17:01:10Z","title":"Not All LLM Reasoners Are Created Equal","summary":"  We study the depth of grade-school math (GSM) problem-solving capabilities of\nLLMs. To this end, we evaluate their performance on pairs of existing math word\nproblems together so that the answer to the second problem depends on correctly\nanswering the first problem. Our findings reveal a significant reasoning gap in\nmost LLMs, that is performance difference between solving the compositional\npairs and solving each question independently. This gap is more pronounced in\nsmaller, more cost-efficient, and math-specialized models. Moreover,\ninstruction-tuning recipes and code generation have varying effects across LLM\nsizes, while finetuning on GSM can lead to task overfitting. Our analysis\nindicates that large reasoning gaps are not because of test-set leakage, but\ndue to distraction from additional context and poor second-hop reasoning.\nOverall, LLMs exhibit systematic differences in their reasoning abilities,\ndespite what their performance on standard benchmarks indicates.\n","authors":["Arian Hosseini","Alessandro Sordoni","Daniel Toyama","Aaron Courville","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2410.01748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01746v1","updated":"2024-10-02T17:01:01Z","published":"2024-10-02T17:01:01Z","title":"Leray-Schauder Mappings for Operator Learning","summary":"  We present an algorithm for learning operators between Banach spaces, based\non the use of Leray-Schauder mappings to learn a finite-dimensional\napproximation of compact subspaces. We show that the resulting method is a\nuniversal approximator of (possibly nonlinear) operators. We demonstrate the\nefficiency of the approach on two benchmark datasets showing it achieves\nresults comparable to state of the art models.\n","authors":["Emanuele Zappala"],"pdf_url":"https://arxiv.org/pdf/2410.01746v1.pdf","comment":"6 pages, 2 figures, 1 table. Comments are welcome!"},{"id":"http://arxiv.org/abs/2408.12186v2","updated":"2024-10-02T16:58:37Z","published":"2024-08-22T08:02:10Z","title":"Transformers are Minimax Optimal Nonparametric In-Context Learners","summary":"  In-context learning (ICL) of large language models has proven to be a\nsurprisingly effective method of learning a new task from only a few\ndemonstrative examples. In this paper, we study the efficacy of ICL from the\nviewpoint of statistical learning theory. We develop approximation and\ngeneralization error bounds for a transformer composed of a deep neural network\nand one linear attention layer, pretrained on nonparametric regression tasks\nsampled from general function spaces including the Besov space and piecewise\n$\\gamma$-smooth class. We show that sufficiently trained transformers can\nachieve -- and even improve upon -- the minimax optimal estimation risk in\ncontext by encoding the most relevant basis representations during pretraining.\nOur analysis extends to high-dimensional or sequential data and distinguishes\nthe \\emph{pretraining} and \\emph{in-context} generalization gaps. Furthermore,\nwe establish information-theoretic lower bounds for meta-learners w.r.t. both\nthe number of tasks and in-context examples. These findings shed light on the\nroles of task diversity and representation learning for ICL.\n","authors":["Juno Kim","Tai Nakamaki","Taiji Suzuki"],"pdf_url":"https://arxiv.org/pdf/2408.12186v2.pdf","comment":"NeurIPS 2024; 40 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.13975v2","updated":"2024-10-02T16:56:09Z","published":"2024-05-22T20:20:14Z","title":"HOPE for a Robust Parameterization of Long-memory State Space Models","summary":"  State-space models (SSMs) that utilize linear, time-invariant (LTI) systems\nare known for their effectiveness in learning long sequences. To achieve\nstate-of-the-art performance, an SSM often needs a specifically designed\ninitialization, and the training of state matrices is on a logarithmic scale\nwith a very small learning rate. To understand these choices from a unified\nperspective, we view SSMs through the lens of Hankel operator theory. Building\nupon it, we develop a new parameterization scheme, called HOPE, for LTI systems\nthat utilizes Markov parameters within Hankel operators. Our approach helps\nimprove the initialization and training stability, leading to a more robust\nparameterization. We efficiently implement these innovations by nonuniformly\nsampling the transfer functions of LTI systems, and they require fewer\nparameters compared to canonical SSMs. When benchmarked against\nHiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel\noperators demonstrates improved performance on Long-Range Arena (LRA) tasks.\nMoreover, our new parameterization endows the SSM with non-decaying memory\nwithin a fixed time window, which is empirically corroborated by a sequential\nCIFAR-10 task with padded noise.\n","authors":["Annan Yu","Michael W. Mahoney","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2405.13975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01745v1","updated":"2024-10-02T16:56:03Z","published":"2024-10-02T16:56:03Z","title":"PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through\n  Pre-trained Network Distillation","summary":"  Intrinsic motivation, inspired by the psychology of developmental learning in\ninfants, stimulates exploration in agents without relying solely on sparse\nexternal rewards. Existing methods in reinforcement learning like Random\nNetwork Distillation (RND) face significant limitations, including (1) relying\non raw visual inputs, leading to a lack of meaningful representations, (2) the\ninability to build a robust latent space, (3) poor target network\ninitialization and (4) rapid degradation of intrinsic rewards. In this paper,\nwe introduce Pre-trained Network Distillation (PreND), a novel approach to\nenhance intrinsic motivation in reinforcement learning (RL) by improving upon\nthe widely used prediction-based method, RND. PreND addresses these challenges\nby incorporating pre-trained representation models into both the target and\npredictor networks, resulting in more meaningful and stable intrinsic rewards,\nwhile enhancing the representation learned by the model. We also tried simple\nbut effective variants of the predictor network optimization by controlling the\nlearning rate. Through experiments on the Atari domain, we demonstrate that\nPreND significantly outperforms RND, offering a more robust intrinsic\nmotivation signal that leads to better exploration, improving overall\nperformance and sample efficiency. This research highlights the importance of\ntarget and predictor networks representation in prediction-based intrinsic\nmotivation, setting a new direction for improving RL agents' learning\nefficiency in sparse reward environments.\n","authors":["Mohammadamin Davoodabadi","Negin Hashemi Dijujin","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2410.01745v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.01739v1","updated":"2024-10-02T16:50:29Z","published":"2024-10-02T16:50:29Z","title":"Mimicking Human Intuition: Cognitive Belief-Driven Q-Learning","summary":"  Reinforcement learning encounters challenges in various environments related\nto robustness and explainability. Traditional Q-learning algorithms cannot\neffectively make decisions and utilize the historical learning experience. To\novercome these limitations, we propose Cognitive Belief-Driven Q-Learning\n(CBDQ), which integrates subjective belief modeling into the Q-learning\nframework, enhancing decision-making accuracy by endowing agents with\nhuman-like learning and reasoning capabilities. Drawing inspiration from\ncognitive science, our method maintains a subjective belief distribution over\nthe expectation of actions, leveraging a cluster-based subjective belief model\nthat enables agents to reason about the potential probability associated with\neach decision. CBDQ effectively mitigates overestimated phenomena and optimizes\ndecision-making policies by integrating historical experiences with current\ncontextual information, mimicking the dynamics of human decision-making. We\nevaluate the proposed method on discrete control benchmark tasks in various\ncomplicate environments. The results demonstrate that CBDQ exhibits stronger\nadaptability, robustness, and human-like characteristics in handling these\nenvironments, outperforming other baselines. We hope this work will give\nresearchers a fresh perspective on understanding and explaining Q-learning.\n","authors":["Xingrui Gu","Guanren Qiao","Chuyi Jiang","Tianqing Xia","Hangyu Mao"],"pdf_url":"https://arxiv.org/pdf/2410.01739v1.pdf","comment":"Under review by ICLR 25"},{"id":"http://arxiv.org/abs/2410.01736v1","updated":"2024-10-02T16:47:35Z","published":"2024-10-02T16:47:35Z","title":"Recursive Abstractive Processing for Retrieval in Dynamic Datasets","summary":"  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n","authors":["Charbel Chucri","Rami Azouz","Joachim Ott"],"pdf_url":"https://arxiv.org/pdf/2410.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01735v1","updated":"2024-10-02T16:46:38Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v1.pdf","comment":"20 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2409.09828v2","updated":"2024-10-02T16:42:46Z","published":"2024-09-15T19:04:50Z","title":"Latent Diffusion Models for Controllable RNA Sequence Generation","summary":"  This work presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences of variable lengths. RNA is a key\nintermediary between DNA and protein, exhibiting high sequence diversity and\ncomplex three-dimensional structures to support a wide range of functions. We\nutilize pretrained BERT-type models to encode raw RNA sequences into\ntoken-level, biologically meaningful representations. A Query Transformer is\nemployed to compress such representations into a set of fixed-length latent\nvectors, with an autoregressive decoder trained to reconstruct RNA sequences\nfrom these latent variables. We then develop a continuous diffusion model\nwithin this latent space. To enable optimization, we integrate the gradients of\nreward models--surrogates for RNA functional properties--into the backward\ndiffusion process, thereby generating RNAs with high reward scores. Empirical\nresults confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological metrics. Further, we fine-tune\nthe diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize\nsequences for high translation efficiencies. Our guided diffusion model\neffectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and\nTranslation Efficiency (TE), outperforming baselines in balancing rewards and\nstructural stability trade-off. Our findings hold potential for advancing RNA\nsequence-function research and therapeutic RNA design.\n","authors":["Kaixuan Huang","Yukang Yang","Kaidi Fu","Yanyi Chu","Le Cong","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15939v2","updated":"2024-10-02T16:42:35Z","published":"2024-02-24T23:56:15Z","title":"Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI","summary":"  Dynamic magnetic resonance imaging (MRI) plays an indispensable role in\ncardiac diagnosis. To enable fast imaging, the k-space data can be undersampled\nbut the image reconstruction poses a great challenge of high-dimensional\nprocessing. This challenge necessitates extensive training data in deep\nlearning reconstruction methods. In this work, we propose a novel and efficient\napproach, leveraging a dimension-reduced separable learning scheme that can\nperform exceptionally well even with highly limited training data. We design\nthis new approach by incorporating spatiotemporal priors into the development\nof a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an\niteration process of a 2D spatiotemporal reconstruction model with both\ntemporal low-rankness and spatial sparsity. Intermediate outputs can also be\nvisualized to provide insights into the network behavior and enhance\ninterpretability. Extensive results on cardiac cine datasets demonstrate that\nthe proposed DeepSSL surpasses state-of-the-art methods both visually and\nquantitatively, while reducing the demand for training cases by up to 75%.\nAdditionally, its preliminary adaptability to unseen cardiac patients has been\nverified through a blind reader study conducted by experienced radiologists and\ncardiologists. Furthermore, DeepSSL enhances the accuracy of the downstream\ntask of cardiac segmentation and exhibits robustness in prospectively\nundersampled real-time cardiac MRI.\n","authors":["Zi Wang","Min Xiao","Yirong Zhou","Chengyan Wang","Naiming Wu","Yi Li","Yiwen Gong","Shufu Chang","Yinyin Chen","Liuhong Zhu","Jianjun Zhou","Congbo Cai","He Wang","Di Guo","Guang Yang","Xiaobo Qu"],"pdf_url":"https://arxiv.org/pdf/2402.15939v2.pdf","comment":"12 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.14012v2","updated":"2024-10-02T16:40:10Z","published":"2024-09-21T04:40:08Z","title":"Test Time Learning for Time Series Forecasting","summary":"  Time-series forecasting has seen significant advancements with the\nintroduction of token prediction mechanisms such as multi-head attention.\nHowever, these methods often struggle to achieve the same performance as in\nlanguage modeling, primarily due to the quadratic computational cost and the\ncomplexity of capturing long-range dependencies in time-series data.\nState-space models (SSMs), such as Mamba, have shown promise in addressing\nthese challenges by offering efficient solutions with linear RNNs capable of\nmodeling long sequences with larger context windows. However, there remains\nroom for improvement in accuracy and scalability.\n  We propose the use of Test-Time Training (TTT) modules in a parallel\narchitecture to enhance performance in long-term time series forecasting.\nThrough extensive experiments on standard benchmark datasets, we demonstrate\nthat TTT modules consistently outperform state-of-the-art models, including the\nMamba-based TimeMachine, particularly in scenarios involving extended sequence\nand prediction lengths. Our results show significant improvements in Mean\nSquared Error (MSE) and Mean Absolute Error (MAE), especially on larger\ndatasets such as Electricity, Traffic, and Weather, underscoring the\neffectiveness of TTT in capturing long-range dependencies. Additionally, we\nexplore various convolutional architectures within the TTT framework, showing\nthat even simple configurations like 1D convolution with small filters can\nachieve competitive results. This work sets a new benchmark for time-series\nforecasting and lays the groundwork for future research in scalable,\nhigh-performance forecasting models.\n","authors":["Panayiotis Christou","Shichu Chen","Xupeng Chen","Parijat Dube"],"pdf_url":"https://arxiv.org/pdf/2409.14012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01727v1","updated":"2024-10-02T16:37:19Z","published":"2024-10-02T16:37:19Z","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.01727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08473v2","updated":"2024-10-02T16:37:16Z","published":"2024-06-12T17:56:46Z","title":"Strategies for Pretraining Neural Operators","summary":"  Pretraining for partial differential equation (PDE) modeling has recently\nshown promise in scaling neural operators across datasets to improve\ngeneralizability and performance. Despite these advances, our understanding of\nhow pretraining affects neural operators is still limited; studies generally\npropose tailored architectures and datasets that make it challenging to compare\nor examine different pretraining frameworks. To address this, we compare\nvarious pretraining methods without optimizing architecture choices to\ncharacterize pretraining dynamics on different models and datasets as well as\nto understand its scaling and generalization behavior. We find that pretraining\nis highly dependent on model and dataset choices, but in general transfer\nlearning or physics-based pretraining strategies work best. In addition,\npretraining performance can be further improved by using data augmentations.\nLastly, pretraining can be additionally beneficial when fine-tuning in scarce\ndata regimes or when generalizing to downstream data similar to the pretraining\ndistribution. Through providing insights into pretraining neural operators for\nphysics prediction, we hope to motivate future work in developing and\nevaluating pretraining methods for PDEs.\n","authors":["Anthony Zhou","Cooper Lorsung","AmirPouya Hemmasian","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2406.08473v2.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19653v2","updated":"2024-10-02T16:23:12Z","published":"2024-05-30T03:12:04Z","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","summary":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call \"system captions\" or\nSysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","authors":["Patrick Emami","Zhaonan Li","Saumya Sinha","Truc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.19653v2.pdf","comment":"21 pages. Under review"},{"id":"http://arxiv.org/abs/2410.01709v1","updated":"2024-10-02T16:16:05Z","published":"2024-10-02T16:16:05Z","title":"Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training","summary":"  Test-time domain adaptation is a challenging task that aims to adapt a\npre-trained model to limited, unlabeled target data during inference. Current\nmethods that rely on self-supervision and entropy minimization underperform\nwhen the self-supervised learning (SSL) task does not align well with the\nprimary objective. Additionally, minimizing entropy can lead to suboptimal\nsolutions when there is limited diversity within minibatches. This paper\nintroduces a meta-learning minimax framework for test-time training on batch\nnormalization (BN) layers, ensuring that the SSL task aligns with the primary\ntask while addressing minibatch overfitting. We adopt a mixed-BN approach that\ninterpolates current test batch statistics with the statistics from source\ndomains and propose a stochastic domain synthesizing method to improve model\ngeneralization and robustness to domain shifts. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art techniques across\nvarious domain adaptation and generalization benchmarks, significantly\nenhancing the pre-trained model's robustness on unseen domains.\n","authors":["Chen Tao","Li Shen","Soumik Mondal"],"pdf_url":"https://arxiv.org/pdf/2410.01709v1.pdf","comment":"10 pages, 7 tables, 1 figure"},{"id":"http://arxiv.org/abs/2410.01706v1","updated":"2024-10-02T16:15:26Z","published":"2024-10-02T16:15:26Z","title":"Performant, Memory Efficient and Scalable Multi-Agent Reinforcement\n  Learning","summary":"  As the field of multi-agent reinforcement learning (MARL) progresses towards\nlarger and more complex environments, achieving strong performance while\nmaintaining memory efficiency and scalability to many agents becomes\nincreasingly important. Although recent research has led to several advanced\nalgorithms, to date, none fully address all of these key properties\nsimultaneously. In this work, we introduce Sable, a novel and theoretically\nsound algorithm that adapts the retention mechanism from Retentive Networks to\nMARL. Sable's retention-based sequence modelling architecture allows for\ncomputationally efficient scaling to a large number of agents, as well as\nmaintaining a long temporal context, making it well-suited for large-scale\npartially observable environments. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in the majority of tasks (34 out\nof 45, roughly 75\\%). Furthermore, Sable demonstrates stable performance as we\nscale the number of agents, handling environments with more than a thousand\nagents while exhibiting a linear increase in memory usage. Finally, we conduct\nablation studies to isolate the source of Sable's performance gains and confirm\nits efficient computational memory usage. Our results highlight Sable's\nperformance and efficiency, positioning it as a leading approach to MARL at\nscale.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09722v2","updated":"2024-10-02T16:14:09Z","published":"2024-07-12T23:29:54Z","title":"Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.\n","authors":["Zongyue Qin","Ziniu Hu","Zifan He","Neha Prakriya","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01697v1","updated":"2024-10-02T16:05:03Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v3","updated":"2024-10-02T16:00:29Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:020. Expansion of\n  \"Dimensionality Reduction for Improving Out-of-Distribution Detection in\n  Medical Image Segmentation\" arXiv:2308.03723. Code available at\n  https://github.com/mckellwoodland/dimen_reduce_mahal\n  (https://zenodo.org/records/13881989)"},{"id":"http://arxiv.org/abs/2409.19546v3","updated":"2024-10-02T15:57:57Z","published":"2024-09-29T04:16:24Z","title":"Almost Sure Convergence of Average Reward Temporal Difference Learning","summary":"  Tabular average reward Temporal Difference (TD) learning is perhaps the\nsimplest and the most fundamental policy evaluation algorithm in average reward\nreinforcement learning. After at least 25 years since its discovery, we are\nfinally able to provide a long-awaited almost sure convergence analysis.\nNamely, we are the first to prove that, under very mild conditions, tabular\naverage reward TD converges almost surely to a sample path dependent fixed\npoint. Key to this success is a new general stochastic approximation result\nconcerning nonexpansive mappings with Markovian and additive noise, built on\nrecent advances in stochastic Krasnoselskii-Mann iterations.\n","authors":["Ethan Blaser","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19546v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01687v1","updated":"2024-10-02T15:57:18Z","published":"2024-10-02T15:57:18Z","title":"Uncertainty Quantification with Bayesian Higher Order ReLU KANs","summary":"  We introduce the first method of uncertainty quantification in the domain of\nKolmogorov-Arnold Networks, specifically focusing on (Higher Order) ReLUKANs to\nenhance computational efficiency given the computational demands of Bayesian\nmethods. The method we propose is general in nature, providing access to both\nepistemic and aleatoric uncertainties. It is also capable of generalization to\nother various basis functions. We validate our method through a series of\nclosure tests, including simple one-dimensional functions and application to\nthe domain of (Stochastic) Partial Differential Equations. Referring to the\nlatter, we demonstrate the method's ability to correctly identify functional\ndependencies introduced through the inclusion of a stochastic term. The code\nsupporting this work can be found at\nhttps://github.com/wmdataphys/Bayesian-HR-KAN\n","authors":["James Giroux","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2410.01687v1.pdf","comment":"13 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2410.01686v1","updated":"2024-10-02T15:55:08Z","published":"2024-10-02T15:55:08Z","title":"Positional Attention: Out-of-Distribution Generalization and\n  Expressivity for Neural Algorithmic Reasoning","summary":"  There has been a growing interest in the ability of neural networks to solve\nalgorithmic tasks, such as arithmetic, summary statistics, and sorting. While\nstate-of-the-art models like Transformers have demonstrated good generalization\nperformance on in-distribution tasks, their out-of-distribution (OOD)\nperformance is poor when trained end-to-end. In this paper, we focus on value\ngeneralization, a common instance of OOD generalization where the test\ndistribution has the same input sequence length as the training distribution,\nbut the value ranges in the training and test distributions do not necessarily\noverlap. To address this issue, we propose that using fixed positional\nencodings to determine attention weights-referred to as positional\nattention-enhances empirical OOD performance while maintaining expressivity. We\nsupport our claim about expressivity by proving that Transformers with\npositional attention can effectively simulate parallel algorithms.\n","authors":["Artur Back de Luca","George Giapitzakis","Shenghao Yang","Petar Veličković","Kimon Fountoulakis"],"pdf_url":"https://arxiv.org/pdf/2410.01686v1.pdf","comment":"37 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.01680v1","updated":"2024-10-02T15:50:35Z","published":"2024-10-02T15:50:35Z","title":"PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation","summary":"  Various visual foundation models have distinct strengths and weaknesses, both\nof which can be improved through heterogeneous multi-teacher knowledge\ndistillation without labels, termed \"agglomerative models.\" We build upon this\nbody of work by studying the effect of the teachers' activation statistics,\nparticularly the impact of the loss function on the resulting student model\nquality. We explore a standard toolkit of statistical normalization techniques\nto better align the different distributions and assess their effects. Further,\nwe examine the impact on downstream teacher-matching metrics, which motivates\nthe use of Hadamard matrices. With these matrices, we demonstrate useful\nproperties, showing how they can be used for isotropic standardization, where\neach dimension of a multivariate distribution is standardized using the same\nscale. We call this technique \"PHI Standardization\" (PHI-S) and empirically\ndemonstrate that it produces the best student model across the suite of methods\nstudied.\n","authors":["Mike Ranzinger","Jon Barker","Greg Heinrich","Pavlo Molchanov","Bryan Catanzaro","Andrew Tao"],"pdf_url":"https://arxiv.org/pdf/2410.01680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01679v1","updated":"2024-10-02T15:49:30Z","published":"2024-10-02T15:49:30Z","title":"VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment","summary":"  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n","authors":["Amirhossein Kazemnejad","Milad Aghajohari","Eva Portelance","Alessandro Sordoni","Siva Reddy","Aaron Courville","Nicolas Le Roux"],"pdf_url":"https://arxiv.org/pdf/2410.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06140v3","updated":"2024-10-02T15:43:43Z","published":"2022-10-12T12:48:25Z","title":"Differentially Private Bootstrap: New Privacy Analysis and Inference\n  Strategies","summary":"  Differentially private (DP) mechanisms protect individual-level information\nby introducing randomness into the statistical analysis procedure. Despite the\navailability of numerous DP tools, there remains a lack of general techniques\nfor conducting statistical inference under DP. We examine a DP bootstrap\nprocedure that releases multiple private bootstrap estimates to infer the\nsampling distribution and construct confidence intervals (CIs). Our privacy\nanalysis presents new results on the privacy cost of a single DP bootstrap\nestimate, applicable to any DP mechanism, and identifies some misapplications\nof the bootstrap in the existing literature. For the composition of the DP\nbootstrap, we present a numerical method to compute the exact privacy cost of\nreleasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP)\nframework (Dong et al., 2022), we show that the release of $B$ DP bootstrap\nestimates from mechanisms satisfying $(\\mu/\\sqrt{(2-2/\\mathrm{e})B})$-GDP\nasymptotically satisfies $\\mu$-GDP as $B$ goes to infinity. Then, we perform\nprivate statistical inference by post-processing the DP bootstrap estimates. We\nprove that our point estimates are consistent, our standard CIs are\nasymptotically valid, and both enjoy optimal convergence rates. To further\nimprove the finite performance, we use deconvolution with DP bootstrap\nestimates to accurately infer the sampling distribution. We derive CIs for\ntasks such as population mean estimation, logistic regression, and quantile\nregression, and we compare them to existing methods using simulations and\nreal-world experiments on 2016 Canada Census data. Our private CIs achieve the\nnominal coverage level and offer the first approach to private inference for\nquantile regression.\n","authors":["Zhanyu Wang","Guang Cheng","Jordan Awan"],"pdf_url":"https://arxiv.org/pdf/2210.06140v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01669v1","updated":"2024-10-02T15:37:12Z","published":"2024-10-02T15:37:12Z","title":"Sparse Covariance Neural Networks","summary":"  Covariance Neural Networks (VNNs) perform graph convolutions on the\ncovariance matrix of tabular data and achieve success in a variety of\napplications. However, the empirical covariance matrix on which the VNNs\noperate may contain many spurious correlations, making VNNs' performance\ninconsistent due to these noisy estimates and decreasing their computational\nefficiency. To tackle this issue, we put forth Sparse coVariance Neural\nNetworks (S-VNNs), a framework that applies sparsification techniques on the\nsample covariance matrix before convolution. When the true covariance matrix is\nsparse, we propose hard and soft thresholding to improve covariance estimation\nand reduce computational cost. Instead, when the true covariance is dense, we\npropose stochastic sparsification where data correlations are dropped in\nprobability according to principled strategies. We show that S-VNNs are more\nstable than nominal VNNs as well as sparse principal component analysis. By\nanalyzing the impact of sparsification on their behavior, we provide novel\nconnections between S-VNN stability and data distribution. We support our\ntheoretical findings with experimental results on various application\nscenarios, ranging from brain data to human action recognition, and show an\nimproved task performance, stability, and computational efficiency of S-VNNs\ncompared with nominal VNNs.\n","authors":["Andrea Cavallo","Zhan Gao","Elvin Isufi"],"pdf_url":"https://arxiv.org/pdf/2410.01669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01660v1","updated":"2024-10-02T15:26:52Z","published":"2024-10-02T15:26:52Z","title":"Conformal Generative Modeling with Improved Sample Efficiency through\n  Sequential Greedy Filtering","summary":"  Generative models lack rigorous statistical guarantees for their outputs and\nare therefore unreliable in safety-critical applications. In this work, we\npropose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a\nsequential conformal prediction method producing prediction sets that satisfy a\nrigorous statistical guarantee called conformal admissibility control. This\nguarantee states that with high probability, the prediction sets contain at\nleast one admissible (or valid) example. To this end, our method first samples\nan initial set of i.i.d. examples from a black box generative model. Then, this\nset is iteratively pruned via so-called greedy filters. As a consequence of the\niterative generation procedure, admissibility of the final prediction set\nfactorizes as a Markov chain. This factorization is crucial, because it allows\nto control each factor separately, using conformal prediction. In comparison to\nprior work, our method demonstrates a large reduction in the number of\nadmissibility evaluations during calibration. This reduction is important in\nsafety-critical applications, where these evaluations must be conducted\nmanually by domain experts and are therefore costly and time consuming. We\nhighlight the advantages of our method in terms of admissibility evaluations\nand cardinality of the prediction sets through experiments in natural language\ngeneration and molecular graph extension tasks.\n","authors":["Klaus-Rudolf Kladny","Bernhard Schölkopf","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2410.01660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01658v1","updated":"2024-10-02T15:25:26Z","published":"2024-10-02T15:25:26Z","title":"Smaller Confidence Intervals From IPW Estimators via Data-Dependent\n  Coarsening","summary":"  Inverse propensity-score weighted (IPW) estimators are prevalent in causal\ninference for estimating average treatment effects in observational studies.\nUnder unconfoundedness, given accurate propensity scores and $n$ samples, the\nsize of confidence intervals of IPW estimators scales down with $n$, and,\nseveral of their variants improve the rate of scaling. However, neither IPW\nestimators nor their variants are robust to inaccuracies: even if a single\ncovariate has an $\\varepsilon>0$ additive error in the propensity score, the\nsize of confidence intervals of these estimators can increase arbitrarily.\nMoreover, even without errors, the rate with which the confidence intervals of\nthese estimators go to zero with $n$ can be arbitrarily slow in the presence of\nextreme propensity scores (those close to 0 or 1).\n  We introduce a family of Coarse IPW (CIPW) estimators that captures existing\nIPW estimators and their variants. Each CIPW estimator is an IPW estimator on a\ncoarsened covariate space, where certain covariates are merged. Under mild\nassumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme\npropensity scores, we give an efficient algorithm to find a robust estimator:\ngiven $\\varepsilon$-inaccurate propensity scores and $n$ samples, its\nconfidence interval size scales with $\\varepsilon+1/\\sqrt{n}$. In contrast,\nunder the same assumptions, existing estimators' confidence interval sizes are\n$\\Omega(1)$ irrespective of $\\varepsilon$ and $n$. Crucially, our estimator is\ndata-dependent and we show that no data-independent CIPW estimator can be\nrobust to inaccuracies.\n","authors":["Alkis Kalavasis","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2410.01658v1.pdf","comment":"Accepted for presentation at the 37th Conference on Learning Theory\n  (COLT) 2024"},{"id":"http://arxiv.org/abs/2410.01657v1","updated":"2024-10-02T15:22:27Z","published":"2024-10-02T15:22:27Z","title":"Scalable and Consistent Graph Neural Networks for Distributed Mesh-based\n  Data-driven Modeling","summary":"  This work develops a distributed graph neural network (GNN) methodology for\nmesh-based modeling applications using a consistent neural message passing\nlayer. As the name implies, the focus is on enabling scalable operations that\nsatisfy physical consistency via halo nodes at sub-graph boundaries. Here,\nconsistency refers to the fact that a GNN trained and evaluated on one rank\n(one large graph) is arithmetically equivalent to evaluations on multiple ranks\n(a partitioned graph). This concept is demonstrated by interfacing GNNs with\nNekRS, a GPU-capable exascale CFD solver developed at Argonne National\nLaboratory. It is shown how the NekRS mesh partitioning can be linked to the\ndistributed GNN training and inference routines, resulting in a scalable\nmesh-based data-driven modeling workflow. We study the impact of consistency on\nthe scalability of mesh-based GNNs, demonstrating efficient scaling in\nconsistent GNNs for up to O(1B) graph nodes on the Frontier exascale\nsupercomputer.\n","authors":["Shivam Barwey","Riccardo Balin","Bethany Lusch","Saumil Patel","Ramesh Balakrishnan","Pinaki Pal","Romit Maulik","Venkatram Vishwanath"],"pdf_url":"https://arxiv.org/pdf/2410.01657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01656v1","updated":"2024-10-02T15:21:07Z","published":"2024-10-02T15:21:07Z","title":"Efficient Statistics With Unknown Truncation, Polynomial Time\n  Algorithms, Beyond Gaussians","summary":"  We study the estimation of distributional parameters when samples are shown\nonly if they fall in some unknown set $S \\subseteq \\mathbb{R}^d$. Kontonis,\nTzamos, and Zampetakis (FOCS'19) gave a $d^{\\mathrm{poly}(1/\\varepsilon)}$ time\nalgorithm for finding $\\varepsilon$-accurate parameters for the special case of\nGaussian distributions with diagonal covariance matrix. Recently, Diakonikolas,\nKane, Pittas, and Zarifis (COLT'24) showed that this exponential dependence on\n$1/\\varepsilon$ is necessary even when $S$ belongs to some well-behaved\nclasses. These works leave the following open problems which we address in this\nwork: Can we estimate the parameters of any Gaussian or even extend beyond\nGaussians? Can we design $\\mathrm{poly}(d/\\varepsilon)$ time algorithms when\n$S$ is a simple set such as a halfspace?\n  We make progress on both of these questions by providing the following\nresults:\n  1. Toward the first question, we give a $d^{\\mathrm{poly}(\\ell/\\varepsilon)}$\ntime algorithm for any exponential family that satisfies some structural\nassumptions and any unknown set $S$ that is $\\varepsilon$-approximable by\ndegree-$\\ell$ polynomials. This result has two important applications:\n  1a) The first algorithm for estimating arbitrary Gaussian distributions from\nsamples truncated to an unknown $S$; and\n  1b) The first algorithm for linear regression with unknown truncation and\nGaussian features.\n  2. To address the second question, we provide an algorithm with runtime\n$\\mathrm{poly}(d/\\varepsilon)$ that works for a set of exponential families\n(containing all Gaussians) when $S$ is a halfspace or an axis-aligned\nrectangle.\n  Along the way, we develop tools that may be of independent interest,\nincluding, a reduction from PAC learning with positive and unlabeled samples to\nPAC learning with positive and negative samples that is robust to certain\ncovariate shifts.\n","authors":["Jane H. Lee","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2410.01656v1.pdf","comment":"Accepted for presentation at the 65th IEEE Symposium on Foundations\n  of Computer Science (FOCS), 2024; abstract shortened for arXiv"},{"id":"http://arxiv.org/abs/2410.01655v1","updated":"2024-10-02T15:19:35Z","published":"2024-10-02T15:19:35Z","title":"Extending Contextual Self-Modulation: Meta-Learning Across Modalities,\n  Task Dimensionalities, and Data Regimes","summary":"  Contextual Self-Modulation (CSM) is a potent regularization mechanism for the\nNeural Context Flow (NCF) framework which demonstrates powerful meta-learning\nof physical systems. However, CSM has limitations in its applicability across\ndifferent modalities and in high-data regimes. In this work, we introduce two\nextensions: $i$CSM, which expands CSM to infinite-dimensional tasks, and\nStochasticNCF, which improves scalability. These extensions are demonstrated\nthrough comprehensive experimentation on a range of tasks, including dynamical\nsystems with parameter variations, computer vision challenges, and curve\nfitting problems. $i$CSM embeds the contexts into an infinite-dimensional\nfunction space, as opposed to CSM which uses finite-dimensional context\nvectors. StochasticNCF enables the application of both CSM and $i$CSM to\nhigh-data scenarios by providing an unbiased approximation of meta-gradient\nupdates through a sampled set of nearest environments. Additionally, we\nincorporate higher-order Taylor expansions via Taylor-Mode automatic\ndifferentiation, revealing that higher-order approximations do not necessarily\nenhance generalization. Finally, we demonstrate how CSM can be integrated into\nother meta-learning frameworks with FlashCAVIA, a computationally efficient\nextension of the CAVIA meta-learning framework (Zintgraf et al. 2019).\nFlashCAVIA outperforms its predecessor across various benchmarks and reinforces\nthe utility of bi-level optimization techniques. Together, these contributions\nestablish a robust framework for tackling an expanded spectrum of meta-learning\ntasks, offering practical insights for out-of-distribution generalization. Our\nopen-sourced library, designed for flexible integration of self-modulation into\ncontextual meta-learning workflows, is available at\n\\url{github.com/ddrous/self-mod}.\n","authors":["Roussel Desmond Nzoyem","David A. W. Barton","Tom Deakin"],"pdf_url":"https://arxiv.org/pdf/2410.01655v1.pdf","comment":"23 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2405.02154v3","updated":"2024-10-02T15:18:44Z","published":"2024-05-03T15:02:21Z","title":"Neural Context Flows for Meta-Learning of Dynamical Systems","summary":"  Neural Ordinary Differential Equations (NODEs) often struggle to adapt to new\ndynamic behaviors caused by parameter changes in the underlying system, even\nwhen these dynamics are similar to previously observed behaviors. This problem\nbecomes more challenging when the changing parameters are unobserved, meaning\ntheir value or influence cannot be directly measured when collecting data. To\naddress this issue, we introduce Neural Context Flow (NCF), a robust and\ninterpretable Meta-Learning framework that includes uncertainty estimation. NCF\nuses higher-order Taylor expansion to enable contextual self-modulation,\nallowing context vectors to influence dynamics from other domains while also\nmodulating themselves. After establishing convergence guarantees, we\nempirically test NCF and compare it to related adaptation methods. Our results\nshow that NCF achieves state-of-the-art Out-of-Distribution performance on 5\nout of 6 linear and non-linear benchmark problems. Through extensive\nexperiments, we explore the flexible model architecture of NCF and the encoded\nrepresentations within the learned context vectors. Our findings highlight the\npotential implications of NCF for foundational models in the physical sciences,\noffering a promising approach to improving the adaptability and generalization\nof NODEs in various scientific applications. Our code is openly available at\n\\url{https://github.com/ddrous/ncflow}.\n","authors":["Roussel Desmond Nzoyem","David A. W. Barton","Tom Deakin"],"pdf_url":"https://arxiv.org/pdf/2405.02154v3.pdf","comment":"31 pages, 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2112.00600v3","updated":"2024-10-02T15:17:56Z","published":"2021-12-01T16:14:49Z","title":"Towards Futuristic Autonomous Experimentation--A Surprise-Reacting\n  Sequential Experiment Policy","summary":"  An autonomous experimentation platform in manufacturing is supposedly capable\nof conducting a sequential search for finding suitable manufacturing conditions\nby itself or even for discovering new materials with minimal human\nintervention. The core of the intelligent control of such platforms is a policy\nto decide where to conduct the next experiment based on what has been done thus\nfar. Such policy inevitably trades off between exploitation and exploration.\nCurrently, the prevailing approach is to use various acquisition functions in\nthe Bayesian optimization framework. We discuss whether it is beneficial to\ntrade off exploitation versus exploration by measuring the element and degree\nof surprise associated with the immediate past observation. We devise a\nsurprise-reacting policy using two existing surprise metrics, known as the\nShannon surprise and Bayesian surprise. Our analysis shows that the\nsurprise-reacting policy appears to be better suited for quickly characterizing\nthe overall landscape of a response surface under resource constraints. We do\nnot claim that we have a fully autonomous experimentation system but believe\nthat the surprise-reacting capability benefits the automation of sequential\ndecisions in autonomous experimentation.\n","authors":["Imtiaz Ahmed","Satish Bukkapatnam","Bhaskar Botcha","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2112.00600v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01649v1","updated":"2024-10-02T15:16:53Z","published":"2024-10-02T15:16:53Z","title":"shapiq: Shapley Interactions for Machine Learning","summary":"  Originally rooted in game theory, the Shapley Value (SV) has recently become\nan important tool in machine learning research. Perhaps most notably, it is\nused for feature attribution and data valuation in explainable artificial\nintelligence. Shapley Interactions (SIs) naturally extend the SV and address\nits limitations by assigning joint contributions to groups of entities, which\nenhance understanding of black box machine learning models. Due to the\nexponential complexity of computing SVs and SIs, various methods have been\nproposed that exploit structural assumptions or yield probabilistic estimates\ngiven limited resources. In this work, we introduce shapiq, an open-source\nPython package that unifies state-of-the-art algorithms to efficiently compute\nSVs and any-order SIs in an application-agnostic framework. Moreover, it\nincludes a benchmarking suite containing 11 machine learning applications of\nSIs with pre-computed games and ground-truth values to systematically assess\ncomputational performance across domains. For practitioners, shapiq is able to\nexplain and visualize any-order feature interactions in predictions of models,\nincluding vision transformers, language models, as well as XGBoost and LightGBM\nwith TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and\nconsolidate the application of SVs and SIs in machine learning that facilitates\nfuture research. The source code and documentation are available at\nhttps://github.com/mmschlk/shapiq.\n","authors":["Maximilian Muschalik","Hubert Baniecki","Fabian Fumagalli","Patrick Kolpaczki","Barbara Hammer","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2410.01649v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2307.01181v2","updated":"2024-10-02T15:13:40Z","published":"2023-07-03T17:46:23Z","title":"Fitting an ellipsoid to a quadratic number of random points","summary":"  We consider the problem $(\\mathrm{P})$ of fitting $n$ standard Gaussian\nrandom vectors in $\\mathbb{R}^d$ to the boundary of a centered ellipsoid, as\n$n, d \\to \\infty$. This problem is conjectured to have a sharp feasibility\ntransition: for any $\\varepsilon > 0$, if $n \\leq (1 - \\varepsilon) d^2 / 4$\nthen $(\\mathrm{P})$ has a solution with high probability, while $(\\mathrm{P})$\nhas no solutions with high probability if $n \\geq (1 + \\varepsilon) d^2 /4$. So\nfar, only a trivial bound $n \\geq d^2 / 2$ is known on the negative side, while\nthe best results on the positive side assume $n \\leq d^2 /\n\\mathrm{polylog}(d)$. In this work, we improve over previous approaches using a\nkey result of Bartl & Mendelson (2022) on the concentration of Gram matrices of\nrandom vectors under mild assumptions on their tail behavior. This allows us to\ngive a simple proof that $(\\mathrm{P})$ is feasible with high probability when\n$n \\leq d^2 / C$, for a (possibly large) constant $C > 0$.\n","authors":["Afonso S. Bandeira","Antoine Maillard","Shahar Mendelson","Elliot Paquette"],"pdf_url":"https://arxiv.org/pdf/2307.01181v2.pdf","comment":"17 pages; Update (v2) to match the published version"},{"id":"http://arxiv.org/abs/2410.01644v1","updated":"2024-10-02T15:13:26Z","published":"2024-10-02T15:13:26Z","title":"A Novel Framework of Horizontal-Vertical Hybrid Federated Learning for\n  EdgeIoT","summary":"  This letter puts forth a new hybrid horizontal-vertical federated learning\n(HoVeFL) for mobile edge computing-enabled Internet of Things (EdgeIoT). In\nthis framework, certain EdgeIoT devices train local models using the same data\nsamples but analyze disparate data features, while the others focus on the same\nfeatures using non-independent and identically distributed (non-IID) data\nsamples. Thus, even though the data features are consistent, the data samples\nvary across devices. The proposed HoVeFL formulates the training of local and\nglobal models to minimize the global loss function. Performance evaluations on\nCIFAR-10 and SVHN datasets reveal that the testing loss of HoVeFL with 12\nhorizontal FL devices and six vertical FL devices is 5.5% and 25.2% higher,\nrespectively, compared to a setup with six horizontal FL devices and 12\nvertical FL devices.\n","authors":["Kai Li","Yilei Liang","Xin Yuan","Wei Ni","Jon Crowcroft","Chau Yuen","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2410.01644v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.01643v1","updated":"2024-10-02T15:13:25Z","published":"2024-10-02T15:13:25Z","title":"Stable Offline Value Function Learning with Bisimulation-based\n  Representations","summary":"  In reinforcement learning, offline value function learning is the procedure\nof using an offline dataset to estimate the expected discounted return from\neach state when taking actions according to a fixed target policy. The\nstability of this procedure, i.e., whether it converges to its fixed-point,\ncritically depends on the representations of the state-action pairs. Poorly\nlearned representations can make value function learning unstable, or even\ndivergent. Therefore, it is critical to stabilize value function learning by\nexplicitly shaping the state-action representations. Recently, the class of\nbisimulation-based algorithms have shown promise in shaping representations for\ncontrol. However, it is still unclear if this class of methods can stabilize\nvalue function learning. In this work, we investigate this question and answer\nit affirmatively. We introduce a bisimulation-based algorithm called kernel\nrepresentations for offline policy evaluation (KROPE). KROPE uses a kernel to\nshape state-action representations such that state-action pairs that have\nsimilar immediate rewards and lead to similar next state-action pairs under the\ntarget policy also have similar representations. We show that KROPE: 1) learns\nstable representations and 2) leads to lower value error than baselines. Our\nanalysis provides new theoretical insight into the stability properties of\nbisimulation-based methods and suggests that practitioners can use these\nmethods for stable and accurate evaluation of offline reinforcement learning\nagents.\n","authors":["Brahma S. Pavse","Yudong Chen","Qiaomin Xie","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2410.01643v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.01639v1","updated":"2024-10-02T15:09:36Z","published":"2024-10-02T15:09:36Z","title":"Moral Alignment for LLM Agents","summary":"  Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2410.01639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01635v1","updated":"2024-10-02T15:07:13Z","published":"2024-10-02T15:07:13Z","title":"Does Graph Prompt Work? A Data Operation Perspective with Theoretical\n  Analysis","summary":"  In recent years, graph prompting has emerged as a promising research\ndirection, enabling the learning of additional tokens or subgraphs appended to\nthe original graphs without requiring retraining of pre-trained graph models\nacross various applications. This novel paradigm, shifting from the traditional\npretraining and finetuning to pretraining and prompting has shown significant\nempirical success in simulating graph data operations, with applications\nranging from recommendation systems to biological networks and graph\ntransferring. However, despite its potential, the theoretical underpinnings of\ngraph prompting remain underexplored, raising critical questions about its\nfundamental effectiveness. The lack of rigorous theoretical proof of why and\nhow much it works is more like a dark cloud over the graph prompt area to go\nfurther. To fill this gap, this paper introduces a theoretical framework that\nrigorously analyzes graph prompting from a data operation perspective. Our\ncontributions are threefold: First, we provide a formal guarantee theorem,\ndemonstrating graph prompts capacity to approximate graph transformation\noperators, effectively linking upstream and downstream tasks. Second, we derive\nupper bounds on the error of these data operations by graph prompts for a\nsingle graph and extend this discussion to batches of graphs, which are common\nin graph model training. Third, we analyze the distribution of data operation\nerrors, extending our theoretical findings from linear graph models (e.g., GCN)\nto non-linear graph models (e.g., GAT). Extensive experiments support our\ntheoretical results and confirm the practical implications of these guarantees.\n","authors":["Qunzhong Wang","Xiangguo Sun","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07480v2","updated":"2024-10-02T15:02:13Z","published":"2024-09-02T10:03:03Z","title":"EEG-Language Modeling for Pathology Detection","summary":"  Multimodal language modeling constitutes a recent breakthrough which\nleverages advances in large language models to pretrain capable multimodal\nmodels. The integration of natural language during pretraining has been shown\nto significantly improve learned representations, particularly in computer\nvision. However, the efficacy of multimodal language modeling in the realm of\nfunctional brain data, specifically for advancing pathology detection, remains\nunexplored. This study pioneers EEG-language models trained on clinical reports\nand 15000 EEGs. We extend methods for multimodal alignment to this novel domain\nand investigate which textual information in reports is useful for training\nEEG-language models. Our results indicate that models learn richer\nrepresentations from being exposed to a variety of report segments, including\nthe patient's clinical history, description of the EEG, and the physician's\ninterpretation. Compared to models exposed to narrower clinical text\ninformation, we find such models to retrieve EEGs based on clinical reports\n(and vice versa) with substantially higher accuracy. Yet, this is only observed\nwhen using a contrastive learning approach. Particularly in regimes with few\nannotations, we observe that representations of EEG-language models can\nsignificantly improve pathology detection compared to those of EEG-only models,\nas demonstrated by both zero-shot classification and linear probes. In sum,\nthese results highlight the potential of integrating brain activity data with\nclinical text, suggesting that EEG-language models represent significant\nprogress for clinical applications.\n","authors":["Sam Gijsen","Kerstin Ritter"],"pdf_url":"https://arxiv.org/pdf/2409.07480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01623v1","updated":"2024-10-02T14:58:27Z","published":"2024-10-02T14:58:27Z","title":"Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank\n  Constraint?","summary":"  Low-rank training has emerged as a promising approach for reducing memory\nusage in training Large Language Models (LLMs). Previous methods either rely on\ndecomposing weight matrices (e.g., LoRA), or seek to decompose gradient\nmatrices (e.g., GaLore) to ensure reduced memory consumption. However, both of\nthem constrain the training in a low-rank subspace, thus inevitably leading to\nsub-optimal performance. This raises a question: whether it is possible to\nconsistently preserve the low-rank constraint for memory efficiency, while\nachieving full-rank training (i.e., training with full-rank gradients of\nfull-rank weights) to avoid inferior outcomes? In this paper, we propose a new\nplug-and-play training framework for LLMs called Fira, as the first attempt to\nachieve this goal. First, we observe an interesting phenomenon during LLM\ntraining: the scaling impact of adaptive optimizers (e.g., Adam) on the\ngradient norm remains similar from low-rank to full-rank training. Based on\nthis observation, we propose a norm-based scaling method, which utilizes the\nscaling impact of low-rank optimizers as substitutes for that of original\nfull-rank optimizers to enable full-rank training. In this way, we can preserve\nthe low-rank constraint in the optimizer while achieving full-rank training for\nbetter performance. Moreover, we find that there are sudden gradient rises\nduring the optimization process, potentially causing loss spikes. To address\nthis, we further put forward a norm-growth limiter to smooth the gradient via\nregulating the relative increase of gradient norms. Extensive experiments on\nthe pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA\nand GaLore, achieving performance that is comparable to or even better than\nfull-rank training.\n","authors":["Xi Chen","Kaituo Feng","Changsheng Li","Xunhao Lai","Xiangyu Yue","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01623v1.pdf","comment":"Code is available at: https://github.com/xichen-fy/Fira"},{"id":"http://arxiv.org/abs/2405.14953v3","updated":"2024-10-02T14:56:33Z","published":"2024-05-23T18:01:11Z","title":"MallowsPO: Fine-Tune Your LLM with Preference Dispersions","summary":"  Direct Preference Optimization (DPO) has recently emerged as a popular\napproach to improve reinforcement learning with human feedback (RLHF), leading\nto better techniques to fine-tune large language models (LLM). A weakness of\nDPO, however, lies in its lack of capability to characterize the diversity of\nhuman preferences. Inspired by Mallows' theory of preference ranking, we\ndevelop in this paper a new approach, the MallowsPO. A distinct feature of this\napproach is a dispersion index, which reflects the dispersion of human\npreference to prompts. We show that existing DPO models can be reduced to\nspecial cases of this dispersion index, thus unified with MallowsPO. More\nimportantly, we demonstrate (empirically) how to use this dispersion index to\nenhance the performance of DPO in a broad array of benchmark tasks, from\nsynthetic bandit selection to controllable generations and dialogues, while\nmaintaining great generalization capabilities. MallowsPO is also compatible\nwith other SOTA offline preference optimization methods, boosting nearly 2\\%\nextra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.\n","authors":["Haoxian Chen","Hanyang Zhao","Henry Lam","David Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2405.14953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01617v1","updated":"2024-10-02T14:56:21Z","published":"2024-10-02T14:56:21Z","title":"On Using Certified Training towards Empirical Robustness","summary":"  Adversarial training is arguably the most popular way to provide empirical\nrobustness against specific adversarial examples. While variants based on\nmulti-step attacks incur significant computational overhead, single-step\nvariants are vulnerable to a failure mode known as catastrophic overfitting,\nwhich hinders their practical utility for large perturbations. A parallel line\nof work, certified training, has focused on producing networks amenable to\nformal guarantees of robustness against any possible attack. However, the wide\ngap between the best-performing empirical and certified defenses has severely\nlimited the applicability of the latter. Inspired by recent developments in\ncertified training, which rely on a combination of adversarial attacks with\nnetwork over-approximations, and by the connections between local linearity and\ncatastrophic overfitting, we present experimental evidence on the practical\nutility and limitations of using certified training towards empirical\nrobustness. We show that, when tuned for the purpose, a recent certified\ntraining algorithm can prevent catastrophic overfitting on single-step attacks,\nand that it can bridge the gap to multi-step baselines under appropriate\nexperimental settings. Finally, we present a novel regularizer for network\nover-approximations that can achieve similar effects while markedly reducing\nruntime.\n","authors":["Alessandro De Palma","Serge Durand","Zakaria Chihani","François Terrier","Caterina Urban"],"pdf_url":"https://arxiv.org/pdf/2410.01617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03869v2","updated":"2024-10-02T14:52:13Z","published":"2024-04-05T03:02:57Z","title":"Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable\n  Collaboration","summary":"  The emergence of multi-agent reinforcement learning (MARL) is significantly\ntransforming various fields like autonomous vehicle networks. However,\nreal-world multi-agent systems typically contain multiple roles, and the scale\nof these systems dynamically fluctuates. Consequently, in order to achieve\nzero-shot scalable collaboration, it is essential that strategies for different\nroles can be updated flexibly according to the scales, which is still a\nchallenge for current MARL frameworks. To address this, we propose a novel MARL\nframework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. We first leverage a latent network to learn strategy patterns for\neach agent adaptively. Second, we introduce a heterogeneous layer to be\ninserted into decision-making networks, whose parameters are specifically\ngenerated by the learned latent variables. Our approach is scalable as all the\nparameters are shared except for the heterogeneous layer, and gains both\ninter-individual and temporal heterogeneity, allowing SHPPO to adapt\neffectively to varying scales. SHPPO exhibits superior performance in classic\nMARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google\nResearch Football (GRF), showcasing enhanced zero-shot scalability, and\noffering insights into the learned latent variables' impact on team performance\nby visualization.\n","authors":["Xudong Guo","Daming Shi","Junjie Yu","Wenhui Fan"],"pdf_url":"https://arxiv.org/pdf/2404.03869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01611v1","updated":"2024-10-02T14:49:05Z","published":"2024-10-02T14:49:05Z","title":"DRUPI: Dataset Reduction Using Privileged Information","summary":"  Dataset reduction (DR) seeks to select or distill samples from large datasets\ninto smaller subsets while preserving performance on target tasks. Existing\nmethods primarily focus on pruning or synthesizing data in the same format as\nthe original dataset, typically the input data and corresponding labels.\nHowever, in DR settings, we find it is possible to synthesize more information\nbeyond the data-label pair as an additional learning target to facilitate model\ntraining. In this paper, we introduce Dataset Reduction Using Privileged\nInformation (DRUPI), which enriches DR by synthesizing privileged information\nalongside the reduced dataset. This privileged information can take the form of\nfeature labels or attention labels, providing auxiliary supervision to improve\nmodel learning. Our findings reveal that effective feature labels must balance\nbetween being overly discriminative and excessively diverse, with a moderate\nlevel proving optimal for improving the reduced dataset's efficacy. Extensive\nexperiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI\nintegrates seamlessly with existing dataset reduction methods, offering\nsignificant performance gains.\n","authors":["Shaobo Wang","Yantai Yang","Shuaiyu Zhang","Chenghao Sun","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01606v1","updated":"2024-10-02T14:47:05Z","published":"2024-10-02T14:47:05Z","title":"Automated Red Teaming with GOAT: the Generative Offensive Agent Tester","summary":"  Red teaming assesses how large language models (LLMs) can produce content\nthat violates norms, policies, and rules set during their safety training.\nHowever, most existing automated methods in the literature are not\nrepresentative of the way humans tend to interact with AI models. Common users\nof AI models may not have advanced knowledge of adversarial machine learning\nmethods or access to model internals, and they do not spend a lot of time\ncrafting a single highly effective adversarial prompt. Instead, they are likely\nto make use of techniques commonly shared online and exploit the multiturn\nconversational nature of LLMs. While manual testing addresses this gap, it is\nan inefficient and often expensive process. To address these limitations, we\nintroduce the Generative Offensive Agent Tester (GOAT), an automated agentic\nred teaming system that simulates plain language adversarial conversations\nwhile leveraging multiple adversarial prompting techniques to identify\nvulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by\nprompting a general-purpose model in a way that encourages reasoning through\nthe choices of methods available, the current target model's response, and the\nnext steps. Our approach is designed to be extensible and efficient, allowing\nhuman testers to focus on exploring new areas of risk while automation covers\nthe scaled adversarial stress-testing of known risk territory. We present the\ndesign and evaluation of GOAT, demonstrating its effectiveness in identifying\nvulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama\n3.1 and 88% against GPT-4 on the JailbreakBench dataset.\n","authors":["Maya Pavlova","Erik Brinkman","Krithika Iyer","Vitor Albiero","Joanna Bitton","Hailey Nguyen","Joe Li","Cristian Canton Ferrer","Ivan Evtimov","Aaron Grattafiori"],"pdf_url":"https://arxiv.org/pdf/2410.01606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01600v1","updated":"2024-10-02T14:39:13Z","published":"2024-10-02T14:39:13Z","title":"ENTP: Encoder-only Next Token Prediction","summary":"  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n","authors":["Ethan Ewer","Daewon Chae","Thomas Zeng","Jinkyu Kim","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01599v1","updated":"2024-10-02T14:38:37Z","published":"2024-10-02T14:38:37Z","title":"Towards Model Discovery Using Domain Decomposition and PINNs","summary":"  We enhance machine learning algorithms for learning model parameters in\ncomplex systems represented by ordinary differential equations (ODEs) with\ndomain decomposition methods. The study evaluates the performance of two\napproaches, namely (vanilla) Physics-Informed Neural Networks (PINNs) and\nFinite Basis Physics-Informed Neural Networks (FBPINNs), in learning the\ndynamics of test models with a quasi-stationary longtime behavior. We test the\napproaches for data sets in different dynamical regions and with varying noise\nlevel. As results, we find a better performance for the FBPINN approach\ncompared to the vanilla PINN approach, even in cases with data from only a\nquasi-stationary time domain with few dynamics.\n","authors":["Tirtho S. Saha","Alexander Heinlein","Cordula Reisch"],"pdf_url":"https://arxiv.org/pdf/2410.01599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17943v2","updated":"2024-10-02T14:37:12Z","published":"2024-02-27T23:52:58Z","title":"Sequential transport maps using SoS density estimation and\n  $α$-divergences","summary":"  Transport-based density estimation methods are receiving growing interest\nbecause of their ability to efficiently generate samples from the approximated\ndensity. We further invertigate the sequential transport maps framework\nproposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of\ncomposed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first\nestimating an intermediate density of moderate complexity, and then by\ncomputing the exact KR map from a reference density to the precomputed\napproximate density. In our work, we explore the use of Sum-of-Squares (SoS)\ndensities and $\\alpha$-divergences for approximating the intermediate\ndensities. Combining SoS densities with $\\alpha$-divergence interestingly\nyields convex optimization problems which can be efficiently solved using\nsemidefinite programming. The main advantage of $\\alpha$-divergences is to\nenable working with unnormalized densities, which provides benefits both\nnumerically and theoretically. In particular, we provide a new convergence\nanalyses of the sequential transport maps based on information geometric\nproperties of $\\alpha$-divergences. The choice of intermediate densities is\nalso crucial for the efficiency of the method. While tempered (or annealed)\ndensities are the state-of-the-art, we introduce diffusion-based intermediate\ndensities which permits to approximate densities known from samples only. Such\nintermediate densities are well-established in machine learning for generative\nmodeling. Finally we propose low-dimensional maps (or lazy maps) for dealing\nwith high-dimensional problems and numerically demonstrate our methods on\nBayesian inference problems and unsupervised learning tasks.\n","authors":["Benjamin Zanger","Olivier Zahm","Tiangang Cui","Martin Schreiber"],"pdf_url":"https://arxiv.org/pdf/2402.17943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01597v1","updated":"2024-10-02T14:34:45Z","published":"2024-10-02T14:34:45Z","title":"SAFE: Semantic Adaptive Feature Extraction with Rate Control for 6G\n  Wireless Communications","summary":"  Most current Deep Learning-based Semantic Communication (DeepSC) systems are\ndesigned and trained exclusively for particular single-channel conditions,\nwhich restricts their adaptability and overall bandwidth utilization. To\naddress this, we propose an innovative Semantic Adaptive Feature Extraction\n(SAFE) framework, which significantly improves bandwidth efficiency by allowing\nusers to select different sub-semantic combinations based on their channel\nconditions. This paper also introduces three advanced learning algorithms to\noptimize the performance of SAFE framework as a whole. Through a series of\nsimulation experiments, we demonstrate that the SAFE framework can effectively\nand adaptively extract and transmit semantics under different channel bandwidth\nconditions, of which effectiveness is verified through objective and subjective\nquality evaluations.\n","authors":["Yuna Yan","Lixin Li","Xin Zhang","Wensheng Lin","Wenchi Cheng","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2410.01597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15317v3","updated":"2024-10-02T14:34:08Z","published":"2024-05-24T07:59:02Z","title":"NuwaTS: a Foundation Model Mending Every Incomplete Time Series","summary":"  Time series imputation is critical for many real-world applications and has\nbeen widely studied. However, existing models often require specialized designs\ntailored to specific missing patterns, variables, or domains which limits their\ngeneralizability. In addition, current evaluation frameworks primarily focus on\ndomain-specific tasks and often rely on time-wise train/validation/test data\nsplits, which fail to rigorously assess a model's ability to generalize across\nunseen variables or domains. In this paper, we present \\textbf{NuwaTS}, a novel\nframework that repurposes Pre-trained Language Models (PLMs) for general time\nseries imputation. Once trained, NuwaTS can be applied to impute missing data\nacross any domain. We introduce specialized embeddings for each sub-series\npatch, capturing information about the patch, its missing data patterns, and\nits statistical characteristics. By combining contrastive learning with the\nimputation task, we train PLMs to create a versatile, one-for-all imputation\nmodel. Additionally, we employ a plug-and-play fine-tuning approach, enabling\nefficient adaptation to domain-specific tasks with minimal adjustments. To\nevaluate cross-variable and cross-domain generalization, we propose a new\nbenchmarking protocol that partitions the datasets along the variable\ndimension. Experimental results on over seventeen million time series samples\nfrom diverse domains demonstrate that NuwaTS outperforms state-of-the-art\ndomain-specific models across various datasets under the proposed benchmarking\nprotocol. Furthermore, we show that NuwaTS generalizes to other time series\ntasks, such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n","authors":["Jinguo Cheng","Chunwei Yang","Wanlin Cai","Yuxuan Liang","Qingsong Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2405.15317v3.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2407.14207v5","updated":"2024-10-02T14:32:59Z","published":"2024-07-19T11:12:08Z","title":"Longhorn: State Space Models are Amortized Online Learners","summary":"  Modern large language models are built on sequence modeling via next-token\nprediction. While the Transformer remains the dominant architecture for\nsequence modeling, its quadratic decoding complexity in sequence length poses a\nmajor limitation. State-space models (SSMs) present a competitive alternative,\noffering linear decoding efficiency while maintaining parallelism during\ntraining. However, most existing SSMs rely on linear recurrence designs that\nappear somewhat ad hoc. In this work, we explore SSM design through the lens of\nonline learning, conceptualizing SSMs as meta-modules for specific online\nlearning problems. This approach links SSM design to formulating precise online\nlearning objectives, with state transition rules derived from solving these\nobjectives. Based on this insight, we introduce a novel deep SSM architecture,\nLonghorn, whose update resembles the closed-form solution for solving the\nonline associative recall problem. Our experimental results show that Longhorn\noutperforms state-of-the-art SSMs, including the Mamba model, on standard\nsequence modeling benchmarks, language modeling, and vision tasks.\nSpecifically, Longhorn achieves a 1.8x improvement in sample efficiency\ncompared to Mamba, and can extrapolate over contexts that are up to 16x longer\nduring inference.\n","authors":["Bo Liu","Rui Wang","Lemeng Wu","Yihao Feng","Peter Stone","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14207v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03541v2","updated":"2024-10-02T14:30:15Z","published":"2024-02-05T21:55:24Z","title":"HAMLET: Graph Transformer Neural Operator for Partial Differential\n  Equations","summary":"  We present a novel graph transformer framework, HAMLET, designed to address\nthe challenges in solving partial differential equations (PDEs) using neural\nnetworks. The framework uses graph transformers with modular input encoders to\ndirectly incorporate differential equation information into the solution\nprocess. This modularity enhances parameter correspondence control, making\nHAMLET adaptable to PDEs of arbitrary geometries and varied input formats.\nNotably, HAMLET scales effectively with increasing data complexity and noise,\nshowcasing its robustness. HAMLET is not just tailored to a single type of\nphysical simulation, but can be applied across various domains. Moreover, it\nboosts model resilience and performance, especially in scenarios with limited\ndata. We demonstrate, through extensive experiments, that our framework is\ncapable of outperforming current techniques for PDEs.\n","authors":["Andrey Bryutkin","Jiahao Huang","Zhongying Deng","Guang Yang","Carola-Bibiane Schönlieb","Angelica Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2402.03541v2.pdf","comment":"18 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.09031v3","updated":"2024-10-02T14:24:50Z","published":"2024-06-13T12:04:40Z","title":"A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and\n  Generalizability","summary":"  Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 17 graph\npooling methods and 28 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis, backbone analysis, parameter analysis and visualization to\nprovide more evidence. Extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can\nprovide valuable insights and guidance for deep geometric learning research.\nThe source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.\n","authors":["Pengyun Wang","Junyu Luo","Yanxin Shen","Ming Zhang","Siyu Heng","Xiao Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01588v1","updated":"2024-10-02T14:20:30Z","published":"2024-10-02T14:20:30Z","title":"DynFrs: An Efficient Framework for Machine Unlearning in Random Forest","summary":"  Random Forests are widely recognized for establishing efficacy in\nclassification and regression tasks, standing out in various domains such as\nmedical diagnosis, finance, and personalized recommendations. These domains,\nhowever, are inherently sensitive to privacy concerns, as personal and\nconfidential data are involved. With increasing demand for the right to be\nforgotten, particularly under regulations such as GDPR and CCPA, the ability to\nperform machine unlearning has become crucial for Random Forests. However,\ninsufficient attention was paid to this topic, and existing approaches face\ndifficulties in being applied to real-world scenarios. Addressing this gap, we\npropose the DynFrs framework designed to enable efficient machine unlearning in\nRandom Forests while preserving predictive accuracy. Dynfrs leverages\nsubsampling method Occ(q) and a lazy tag strategy Lzy, and is still adaptable\nto any Random Forest variant. In essence, Occ(q) ensures that each sample in\nthe training set occurs only in a proportion of trees so that the impact of\ndeleting samples is limited, and Lzy delays the reconstruction of a tree node\nuntil necessary, thereby avoiding unnecessary modifications on tree structures.\nIn experiments, applying Dynfrs on Extremely Randomized Trees yields\nsubstantial improvements, achieving orders of magnitude faster unlearning\nperformance and better predictive accuracy than existing machine unlearning\nmethods for Random Forests.\n","authors":["Shurong Wang","Zhuoyang Shen","Xinbao Qiao","Tongning Zhang","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01580v1","updated":"2024-10-02T14:15:32Z","published":"2024-10-02T14:15:32Z","title":"Learning-Augmented Robust Algorithmic Recourse","summary":"  The widespread use of machine learning models in high-stakes domains can have\na major negative impact, especially on individuals who receive undesirable\noutcomes. Algorithmic recourse provides such individuals with suggestions of\nminimum-cost improvements they can make to achieve a desirable outcome in the\nfuture. However, machine learning models often get updated over time and this\ncan cause a recourse to become invalid (i.e., not lead to the desirable\noutcome). The robust recourse literature aims to choose recourses that are less\nsensitive, even against adversarial model changes, but this comes at a higher\ncost. To overcome this obstacle, we initiate the study of algorithmic recourse\nthrough the learning-augmented framework and evaluate the extent to which a\ndesigner equipped with a prediction regarding future model changes can reduce\nthe cost of recourse when the prediction is accurate (consistency) while also\nlimiting the cost even when the prediction is inaccurate (robustness). We\npropose a novel algorithm for this problem, study the robustness-consistency\ntrade-off, and analyze how prediction accuracy affects performance.\n","authors":["Kshitij Kayastha","Vasilis Gkatzelis","Shahin Jabbari"],"pdf_url":"https://arxiv.org/pdf/2410.01580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01577v1","updated":"2024-10-02T14:13:06Z","published":"2024-10-02T14:13:06Z","title":"Coordinate-Based Neural Representation Enabling Zero-Shot Learning for\n  3D Multiparametric Quantitative MRI","summary":"  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific\nphysical parameters with significant potential for neuroscience research and\nclinical practice. However, lengthy scan times for 3D multiparametric qMRI\nacquisition limit its clinical utility. Here, we propose SUMMIT, an innovative\nimaging methodology that includes data acquisition and an unsupervised\nreconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes\nmultiple important quantitative properties into highly undersampled k-space. It\nfurther leverages implicit neural representation incorporated with a dedicated\nphysics model to reconstruct the desired multiparametric maps without needing\nexternal training datasets. SUMMIT delivers co-registered T1, T2, T2*, and\nquantitative susceptibility mapping. Extensive simulations and phantom imaging\ndemonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised\napproach for qMRI reconstruction also introduces a novel zero-shot learning\nparadigm for multiparametric imaging applicable to various medical imaging\nmodalities.\n","authors":["Guoyan Lao","Ruimin Feng","Haikun Qi","Zhenfeng Lv","Qiangqiang Liu","Chunlei Liu","Yuyao Zhang","Hongjiang Wei"],"pdf_url":"https://arxiv.org/pdf/2410.01577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01574v1","updated":"2024-10-02T14:11:29Z","published":"2024-10-02T14:11:29Z","title":"Fake It Until You Break It: On the Adversarial Robustness of\n  AI-generated Image Detectors","summary":"  While generative AI (GenAI) offers countless possibilities for creative and\nproductive tasks, artificially generated media can be misused for fraud,\nmanipulation, scams, misinformation campaigns, and more. To mitigate the risks\nassociated with maliciously generated media, forensic classifiers are employed\nto identify AI-generated content. However, current forensic classifiers are\noften not evaluated in practically relevant scenarios, such as the presence of\nan attacker or when real-world artifacts like social media degradations affect\nimages. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)\ndetectors under different attack scenarios. We demonstrate that forensic\nclassifiers can be effectively attacked in realistic settings, even when the\nattacker does not have access to the target model and post-processing occurs\nafter the adversarial examples are created, which is standard on social media\nplatforms. These attacks can significantly reduce detection accuracy to the\nextent that the risks of relying on detectors outweigh their benefits. Finally,\nwe propose a simple defense mechanism to make CLIP-based detectors, which are\ncurrently the best-performing detectors, robust against these attacks.\n","authors":["Sina Mavali","Jonas Ricker","David Pape","Yash Sharma","Asja Fischer","Lea Schoenherr"],"pdf_url":"https://arxiv.org/pdf/2410.01574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01570v1","updated":"2024-10-02T14:09:51Z","published":"2024-10-02T14:09:51Z","title":"Truncated Kernel Stochastic Gradient Descent on Spheres","summary":"  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD employs a \"truncation\"\noperation, enabling the application of a series-based kernel function in\nstochastic gradient descent, thereby avoiding the difficulties of finding\nsuitable closed-form kernel functions in high-dimensional spaces. In contrast\nto traditional kernel SGD, T-kernel SGD is more effective in balancing bias and\nvariance by dynamically adjusting the hypothesis space during iterations. The\nmost significant advantage of the proposed algorithm is that it can achieve\ntheoretically optimal convergence rates using a constant step size (independent\nof the sample size) while overcoming the inherent saturation problem of kernel\nSGD. Additionally, we leverage the structure of spherical polynomials to derive\nan equivalent T-kernel SGD, significantly reducing storage and computational\ncosts compared to kernel SGD. Typically, T-kernel SGD requires only\n$\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$ computational complexity and\n$\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage to achieve optimal rates for\nthe d-dimensional sphere, where $0<\\epsilon<\\frac{1}{2}$ can be arbitrarily\nsmall if the optimal fitting or the underlying space possesses sufficient\nregularity. This regularity is determined by the smoothness parameter of the\nobjective function and the decaying rate of the eigenvalues of the integral\noperator associated with the kernel function, both of which reflect the\ndifficulty of the estimation problem. Our main results quantitatively\ncharacterize how this prior information influences the convergence of T-kernel\nSGD. The numerical experiments further validate the theoretical findings\npresented in this paper.\n","authors":["JinHui Bai","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.01570v1.pdf","comment":"57 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.13977v2","updated":"2024-10-02T14:01:49Z","published":"2024-05-22T20:24:41Z","title":"Improving Fairness and Mitigating MADness in Generative Models","summary":"  Generative models unfairly penalize data belonging to minority classes,\nsuffer from model autophagy disorder (MADness), and learn biased estimates of\nthe underlying distribution parameters. Our theoretical and empirical results\nshow that training generative models with intentionally designed hypernetworks\nleads to models that 1) are more fair when generating datapoints belonging to\nminority classes 2) are more stable in a self-consumed (i.e., MAD) setting, and\n3) learn parameters that are less statistically biased. To further mitigate\nunfairness, MADness, and bias, we introduce a regularization term that\npenalizes discrepancies between a generative model's estimated weights when\ntrained on real data versus its own synthetic data. To facilitate training\nexisting deep generative models within our framework, we offer a scalable\nimplementation of hypernetworks that automatically generates a hypernetwork\narchitecture for any given generative model.\n","authors":["Paul Mayer","Lorenzo Luzi","Ali Siahkoohi","Don H. Johnson","Richard G. Baraniuk"],"pdf_url":"https://arxiv.org/pdf/2405.13977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01565v1","updated":"2024-10-02T14:01:34Z","published":"2024-10-02T14:01:34Z","title":"Bayes' Power for Explaining In-Context Learning Generalizations","summary":"  Traditionally, neural network training has been primarily viewed as an\napproximation of maximum likelihood estimation (MLE). This interpretation\noriginated in a time when training for multiple epochs on small datasets was\ncommon and performance was data bound; but it falls short in the era of\nlarge-scale single-epoch trainings ushered in by large self-supervised setups,\nlike language models. In this new setup, performance is compute-bound, but data\nis readily available. As models became more powerful, in-context learning\n(ICL), i.e., learning in a single forward-pass based on the context, emerged as\none of the dominant paradigms. In this paper, we argue that a more useful\ninterpretation of neural network behavior in this era is as an approximation of\nthe true posterior, as defined by the data-generating process. We demonstrate\nthis interpretations' power for ICL and its usefulness to predict\ngeneralizations to previously unseen tasks. We show how models become robust\nin-context learners by effectively composing knowledge from their training\ndata. We illustrate this with experiments that reveal surprising\ngeneralizations, all explicable through the exact posterior. Finally, we show\nthe inherent constraints of the generalization capabilities of posteriors and\nthe limitations of neural networks in approximating these posteriors.\n","authors":["Samuel Müller","Noah Hollmann","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2410.01565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01562v1","updated":"2024-10-02T14:00:41Z","published":"2024-10-02T14:00:41Z","title":"HRTF Estimation using a Score-based Prior","summary":"  We present a head-related transfer function (HRTF) estimation method which\nrelies on a data-driven prior given by a score-based diffusion model. The HRTF\nis estimated in reverberant environments using natural excitation signals, e.g.\nhuman speech. The impulse response of the room is estimated along with the HRTF\nby optimizing a parametric model of reverberation based on the statistical\nbehaviour of room acoustics. The posterior distribution of HRTF given the\nreverberant measurement and excitation signal is modelled using the score-based\nHRTF prior and a log-likelihood approximation. We show that the resulting\nmethod outperforms several baselines, including an oracle recommender system\nthat assigns the optimal HRTF in our training set based on the smallest\ndistance to the true HRTF at the given direction of arrival. In particular, we\nshow that the diffusion prior can account for the large variability of\nhigh-frequency content in HRTFs.\n","authors":["Etienne Thuillier","Jean-Marie Lemercier","Eloi Moliner","Timo Gerkmann","Vesa Välimäki"],"pdf_url":"https://arxiv.org/pdf/2410.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01556v1","updated":"2024-10-02T13:52:55Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07191v2","updated":"2024-10-02T13:45:53Z","published":"2024-08-13T20:16:11Z","title":"Joint Graph Rewiring and Feature Denoising via Spectral Resonance","summary":"  In graph learning the graph and the node features both contain noisy\ninformation about the node labels. In this paper we propose joint denoising and\nrewiring (JDR)--an algorithm to jointly rewire the graph and denoise the\nfeatures, which improves the performance of downstream node classification\ngraph neural nets (GNNs). JDR improves the alignment between the leading\neigenspaces of graph and feature matrices. To approximately solve the\nassociated non-convex optimization problem we propose a heuristic that\nefficiently handles real-world graph datasets with multiple classes and\ndifferent levels of homophily or heterophily. We theoretically justify JDR in a\nstylized setting and verify the effectiveness of our approach through extensive\nexperiments on synthetic and real-world graph datasets. The results show that\nJDR consistently outperforms existing rewiring methods on node classification\nusing GNNs as downstream models.\n","authors":["Jonas Linkerhägner","Cheng Shi","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2408.07191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03124v2","updated":"2024-10-02T13:45:11Z","published":"2024-07-31T14:54:29Z","title":"Closed-loop Diffusion Control of Complex Physical Systems","summary":"  The control problems of complex physical systems have broad applications in\nscience and engineering. Previous studies have shown that generative control\nmethods based on diffusion models offer significant advantages for solving\nthese problems. However, existing generative control approaches face challenges\nin both performance and efficiency when extended to the closed-loop setting,\nwhich is essential for effective control. In this paper, we propose an\nefficient Closed-Loop Diffusion method for Physical systems Control\n(CL-DiffPhyCon). By employing an asynchronous denoising framework for different\nphysical time steps, CL-DiffPhyCon generates control signals conditioned on\nreal-time feedback from the environment with significantly reduced\ncomputational cost during sampling. Additionally, the control process could be\nfurther accelerated by incorporating fast sampling techniques, such as DDIM. We\nevaluate CL-DiffPhyCon on two tasks: 1D Burgers' equation control and 2D\nincompressible fluid control. The results demonstrate that CL-DiffPhyCon\nachieves superior control performance with significant improvements in sampling\nefficiency.\n","authors":["Long Wei","Haodong Feng","Yuchen Yang","Ruiqi Feng","Peiyan Hu","Xiang Zheng","Tao Zhang","Dixia Fan","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2408.03124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2312.09610v2","updated":"2024-10-02T13:42:53Z","published":"2023-12-15T08:53:45Z","title":"A Synthesis of Green Architectural Tactics for ML-Enabled Systems","summary":"  The rapid adoption of artificial intelligence (AI) and machine learning (ML)\nhas generated growing interest in understanding their environmental impact and\nthe challenges associated with designing environmentally friendly ML-enabled\nsystems. While Green AI research, i.e., research that tries to minimize the\nenergy footprint of AI, is receiving increasing attention, very few concrete\nguidelines are available on how ML-enabled systems can be designed to be more\nenvironmentally sustainable. In this paper, we provide a catalog of 30 green\narchitectural tactics for ML-enabled systems to fill this gap. An architectural\ntactic is a high-level design technique to improve software quality, in our\ncase environmental sustainability. We derived the tactics from the analysis of\n51 peer-reviewed publications that primarily explore Green AI, and validated\nthem using a focus group approach with three experts. The 30 tactics we\nidentified are aimed to serve as an initial reference guide for further\nexploration into Green AI from a software engineering perspective, and assist\nin designing sustainable ML-enabled systems. To enhance transparency and\nfacilitate their widespread use and extension, we make the tactics available\nonline in easily consumable formats. Wide-spread adoption of these tactics has\nthe potential to substantially reduce the societal impact of ML-enabled systems\nregarding their energy and carbon footprint.\n","authors":["Heli Järvenpää","Patricia Lago","Justus Bogner","Grace Lewis","Henry Muccini","Ipek Ozkaya"],"pdf_url":"https://arxiv.org/pdf/2312.09610v2.pdf","comment":"Accepted for publication at the 2024 International Conference on\n  Software Engineering - Software Engineering in Society (ICSE-SEIS'2024)"},{"id":"http://arxiv.org/abs/2407.14129v2","updated":"2024-10-02T13:42:29Z","published":"2024-07-19T08:59:00Z","title":"Comparing and Contrasting Deep Learning Weather Prediction Backbones on\n  Navier-Stokes and Atmospheric Dynamics","summary":"  Remarkable progress in the development of Deep Learning Weather Prediction\n(DLWP) models positions them to become competitive with traditional numerical\nweather prediction (NWP) models. Indeed, a wide number of DLWP architectures --\nbased on various backbones, including U-Net, Transformer, Graph Neural Network\n(GNN), and Fourier Neural Operator (FNO) -- have demonstrated their potential\nat forecasting atmospheric states. However, due to differences in training\nprotocols, forecast horizons, and data choices, it remains unclear which (if\nany) of these methods and architectures are most suitable for weather\nforecasting and for future model development. Here, we step back and provide a\ndetailed empirical analysis, under controlled conditions, comparing and\ncontrasting the most prominent DLWP models, along with their backbones. We\naccomplish this by predicting synthetic two-dimensional incompressible\nNavier-Stokes and real-world global weather dynamics. In terms of accuracy,\nmemory consumption, and runtime, our results illustrate various tradeoffs. For\nexample, on synthetic data, we observe favorable performance of FNO; and on the\nreal-world WeatherBench dataset, our results demonstrate the suitability of\nConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged\nweather rollouts of up to 365 days, we observe superior stability and physical\nsoundness in architectures that formulate a spherical data representation,\ni.e., GraphCast and Spherical FNO. In addition, we observe that all of these\nmodel backbones \"saturate,\" i.e., none of them exhibit so-called neural\nscaling, which highlights an important direction for future work on these and\nrelated models. The code is available at\nhttps://github.com/amazon-science/dlwp-benchmark.\n","authors":["Matthias Karlbauer","Danielle C. Maddix","Abdul Fatir Ansari","Boran Han","Gaurav Gupta","Yuyang Wang","Andrew Stuart","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2407.14129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10079v3","updated":"2024-10-02T13:40:53Z","published":"2023-04-20T04:12:50Z","title":"Dynamic Graph Representation Learning via Edge Temporal States Modeling\n  and Structure-reinforced Transformer","summary":"  Dynamic graph representation learning has emerged as a crucial research area,\ndriven by the growing need for analyzing time-evolving graph data in real-world\napplications. While recent approaches leveraging recurrent neural networks\n(RNNs) and graph neural networks (GNNs) have shown promise, they often fail to\nadequately capture the impact of temporal edge states on inter-node\nrelationships, consequently overlooking the dynamic changes in node features\ninduced by these evolving relationships. Furthermore, these methods suffer from\nGNNs' inherent over-smoothing problem, which hinders the extraction of global\nstructural features. To address these challenges, we introduce the Recurrent\nStructure-reinforced Graph Transformer (RSGT), a novel framework for dynamic\ngraph representation learning. It first designs a heuristic method to\nexplicitly model edge temporal states by employing different edge types and\nweights based on the differences between consecutive snapshots, thereby\nintegrating varying edge temporal states into the graph's topological\nstructure. We then propose a structure-reinforced graph transformer that\ncaptures temporal node representations encoding both graph topology and\nevolving dynamics through a recurrent learning paradigm, enabling the\nextraction of both local and global structural features. Comprehensive\nexperiments on four real-world datasets demonstrate RSGT's superior performance\nin discrete dynamic graph representation learning, consistently outperforming\nexisting methods in dynamic link prediction tasks.\n","authors":["Shengxiang Hu","Guobing Zou","Song Yang","Shiyi Lin","Yanglan Gan","Bofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.10079v3.pdf","comment":"This work has been submitted to the Elsevier for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2406.07107v3","updated":"2024-10-02T13:38:28Z","published":"2024-06-11T09:49:00Z","title":"Agnostic Sharpness-Aware Minimization","summary":"  Sharpness-aware minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both the training loss and the sharpness\nof the loss landscape, leading the model into flatter minima that are\nassociated with better generalization properties. In another aspect,\nModel-Agnostic Meta-Learning (MAML) is a framework designed to improve the\nadaptability of models. MAML optimizes a set of meta-models that are\nspecifically tailored for quick adaptation to multiple tasks with minimal\nfine-tuning steps and can generalize well with limited data. In this work, we\nexplore the connection between SAM and MAML in enhancing model generalization.\nWe introduce Agnostic-SAM, a novel approach that combines the principles of\nboth SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the\nmodel toward wider local minima using training data, while concurrently\nmaintaining low loss values on validation data. By doing so, it seeks flatter\nminima that are not only robust to small perturbations but also less vulnerable\nto data distributional shift problems. Our experimental results demonstrate\nthat Agnostic-SAM significantly improves generalization over baselines across a\nrange of datasets and under challenging conditions such as noisy labels or data\nlimitation.\n","authors":["Van-Anh Nguyen","Quyen Tran","Tuan Truong","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2406.07107v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.03179v2","updated":"2024-10-02T13:32:56Z","published":"2024-07-03T14:59:46Z","title":"Motion meets Attention: Video Motion Prompts","summary":"  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.\n","authors":["Qixiang Chen","Lei Wang","Piotr Koniusz","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2407.03179v2.pdf","comment":"Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)"},{"id":"http://arxiv.org/abs/2410.01545v1","updated":"2024-10-02T13:31:06Z","published":"2024-10-02T13:31:06Z","title":"Lines of Thought in Large Language Models","summary":"  Large Language Models achieve next-token prediction by transporting a\nvectorized piece of text (prompt) across an accompanying embedding space under\nthe action of successive transformer layers. The resulting high-dimensional\ntrajectories realize different contextualization, or 'thinking', steps, and\nfully determine the output probability distribution. We aim to characterize the\nstatistical properties of ensembles of these 'lines of thought.' We observe\nthat independent trajectories cluster along a low-dimensional, non-Euclidean\nmanifold, and that their path can be well approximated by a stochastic equation\nwith few parameters extracted from data. We find it remarkable that the vast\ncomplexity of such large models can be reduced to a much simpler form, and we\nreflect on implications.\n","authors":["Raphaël Sarfati","Toni J. B. Liu","Nicolas Boullé","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.01545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01540v1","updated":"2024-10-02T13:29:52Z","published":"2024-10-02T13:29:52Z","title":"Edge-preserving noise for diffusion models","summary":"  Classical generative diffusion models learn an isotropic Gaussian denoising\nprocess, treating all spatial regions uniformly, thus neglecting potentially\nvaluable structural information in the data. Inspired by the long-established\nwork on anisotropic diffusion in image processing, we present a novel\nedge-preserving diffusion model that is a generalization of denoising diffusion\nprobablistic models (DDPM). In particular, we introduce an edge-aware noise\nscheduler that varies between edge-preserving and isotropic Gaussian noise. We\nshow that our model's generative process converges faster to results that more\nclosely match the target distribution. We demonstrate its capability to better\nlearn the low-to-mid frequencies within the dataset, which plays a crucial role\nin representing shapes and structural information. Our edge-preserving\ndiffusion process consistently outperforms state-of-the-art baselines in\nunconditional image generation. It is also more robust for generative tasks\nguided by a shape-based prior, such as stroke-to-image generation. We present\nqualitative and quantitative results showing consistent improvements (FID\nscore) of up to 30% for both tasks.\n","authors":["Jente Vandersanden","Sascha Holl","Xingchang Huang","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2410.01540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01537v1","updated":"2024-10-02T13:28:02Z","published":"2024-10-02T13:28:02Z","title":"Attention layers provably solve single-location regression","summary":"  Attention-based models, such as Transformer, excel across various tasks but\nlack a comprehensive theoretical understanding, especially regarding token-wise\nsparsity and internal linear representations. To address this gap, we introduce\nthe single-location regression task, where only one token in a sequence\ndetermines the output, and its position is a latent random variable,\nretrievable via a linear projection of the input. To solve this task, we\npropose a dedicated predictor, which turns out to be a simplified version of a\nnon-linear self-attention layer. We study its theoretical properties, by\nshowing its asymptotic Bayes optimality and analyzing its training dynamics. In\nparticular, despite the non-convex nature of the problem, the predictor\neffectively learns the underlying structure. This work highlights the capacity\nof attention mechanisms to handle sparse token information and internal linear\nstructures.\n","authors":["Pierre Marion","Raphaël Berthier","Gérard Biau","Claire Boyer"],"pdf_url":"https://arxiv.org/pdf/2410.01537v1.pdf","comment":"41 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.14341v2","updated":"2024-10-02T13:24:42Z","published":"2024-06-20T14:09:00Z","title":"HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?","summary":"  Accurately forecasting multiple future events within a given time horizon is\ncrucial for finance, retail, social networks, and healthcare applications.\nEvent timing and labels are typically modeled using Marked Temporal Point\nProcesses (MTPP), with evaluations often focused on next-event prediction\nquality. While some studies have extended evaluations to a fixed number of\nfuture events, we demonstrate that this approach leads to inaccuracies in\nhandling false positives and false negatives. To address these issues, we\npropose a novel evaluation method inspired by object detection techniques from\ncomputer vision. Specifically, we introduce Temporal mean Average Precision\n(T-mAP), a temporal variant of mAP, which overcomes the limitations of existing\nlong-horizon evaluation metrics. Our extensive experiments demonstrate that\nmodels with strong next-event prediction accuracy can yield poor long-horizon\nforecasts and vice versa, indicating that specialized methods are needed for\neach task. To support further research, we release HoTPP, the first benchmark\ndesigned explicitly for evaluating long-horizon MTPP predictions. HoTPP\nincludes large-scale datasets with up to 43 million events and provides\noptimized procedures for both autoregressive and parallel inference, paving the\nway for future advancements in the field.\n","authors":["Ivan Karpukhin","Foma Shipilov","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2406.14341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01531v1","updated":"2024-10-02T13:24:24Z","published":"2024-10-02T13:24:24Z","title":"TiVaT: Joint-Axis Attention for Time Series Forecasting with Lead-Lag\n  Dynamics","summary":"  Multivariate time series (MTS) forecasting plays a crucial role in various\nreal-world applications, yet simultaneously capturing both temporal and\ninter-variable dependencies remains a challenge. Conventional Channel-Dependent\n(CD) models handle these dependencies separately, limiting their ability to\nmodel complex interactions such as lead-lag dynamics. To address these\nlimitations, we propose TiVaT (Time-Variable Transformer), a novel architecture\nthat integrates temporal and variate dependencies through its Joint-Axis (JA)\nattention mechanism. TiVaT's ability to capture intricate variate-temporal\ndependencies, including asynchronous interactions, is further enhanced by the\nincorporation of Distance-aware Time-Variable (DTV) Sampling, which reduces\nnoise and improves accuracy through a learned 2D map that focuses on key\ninteractions. TiVaT effectively models both temporal and variate dependencies,\nconsistently delivering strong performance across diverse datasets. Notably, it\nexcels in capturing complex patterns within multivariate time series, enabling\nit to surpass or remain competitive with state-of-the-art methods. This\npositions TiVaT as a new benchmark in MTS forecasting, particularly in handling\ndatasets characterized by intricate and challenging dependencies.\n","authors":["Junwoo Ha","Hyukjae Kwon","Sungsoo Kim","Kisu Lee","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01531v1.pdf","comment":"15pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.16587v2","updated":"2024-10-02T13:22:27Z","published":"2024-05-26T14:38:24Z","title":"Cost-Effective Online Multi-LLM Selection with Versatile Reward Models","summary":"  With the rapid advancement of large language models (LLMs), the diversity of\nmulti-LLM tasks and the variability in their pricing structures have become\nincreasingly important, as costs can vary greatly between different LLMs. To\ntackle these challenges, we introduce the \\textit{C2MAB-V}, a\n\\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed\n\\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM\nselection and usage. This online model differs from traditional static\napproaches or those reliant on a single LLM without cost consideration. With\nmultiple LLMs deployed on a scheduling cloud and a local server dedicated to\nhandling user queries, \\textit{C2MAB-V} facilitates the selection of multiple\nLLMs over a combinatorial search space, specifically tailored for various\ncollaborative task types with different reward models. Based on our designed\nonline feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can\neffectively address the multi-LLM selection challenge by managing the\nexploration-exploitation trade-off across different models, while also\nbalancing cost and reward for diverse tasks. The NP-hard integer linear\nprogramming problem for selecting multiple LLMs with trade-off dilemmas is\naddressed by: i) decomposing the integer problem into a relaxed form by the\nlocal server, ii) utilizing a discretization rounding scheme that provides\noptimal LLM combinations by the scheduling cloud, and iii) continual online\nupdates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers\nstrict guarantees over versatile reward models, matching state-of-the-art\nresults for regret and violations in some degenerate cases. Empirically, we\nshow that \\textit{C2MAB-V} effectively balances performance and cost-efficiency\nwith nine LLMs for three application scenarios.\n","authors":["Xiangxiang Dai","Jin Li","Xutong Liu","Anqi Yu","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2405.16587v2.pdf","comment":"32 pages, 14 figures, conference"},{"id":"http://arxiv.org/abs/2408.13131v2","updated":"2024-10-02T13:21:50Z","published":"2024-08-23T14:57:46Z","title":"DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction","summary":"  Long-horizon event forecasting is critical across various domains, including\nretail, finance, healthcare, and social networks. Traditional methods, such as\nMarked Temporal Point Processes (MTPP), often rely on autoregressive models to\npredict multiple future events. However, these models frequently suffer from\nissues like converging to constant or repetitive outputs, which limits their\neffectiveness and general applicability. To address these challenges, we\nintroduce DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection techniques from computer vision. DeTPP employs a\nunique matching-based loss function that selectively prioritizes reliably\npredictable events, improving the accuracy and diversity of predictions during\ninference. Our method establishes a new state-of-the-art in long-horizon event\nforecasting, achieving up to a 77% relative improvement over existing MTPP and\nnext-K methods. The proposed hybrid approach enhances the accuracy of next\nevent prediction by up to 2.7% on a large transactional dataset. Notably, DeTPP\nis also among the fastest methods for inference. The implementation of DeTPP is\npublicly available on GitHub.\n","authors":["Ivan Karpukhin","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.13131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09335v2","updated":"2024-10-02T13:13:06Z","published":"2024-08-18T02:31:55Z","title":"Exploratory Optimal Stopping: A Singular Control Formulation","summary":"  This paper explores continuous-time and state-space optimal stopping problems\nfrom a reinforcement learning perspective. We begin by formulating the stopping\nproblem using randomized stopping times, where the decision maker's control is\nrepresented by the probability of stopping within a given time--specifically, a\nbounded, non-decreasing, c\\`adl\\`ag control process. To encourage exploration\nand facilitate learning, we introduce a regularized version of the problem by\npenalizing it with the cumulative residual entropy of the randomized stopping\ntime. The regularized problem takes the form of an (n+1)-dimensional degenerate\nsingular stochastic control with finite-fuel. We address this through the\ndynamic programming principle, which enables us to identify the unique optimal\nexploratory strategy. For the specific case of a real option problem, we derive\na semi-explicit solution to the regularized problem, allowing us to assess the\nimpact of entropy regularization and analyze the vanishing entropy limit.\nFinally, we propose a reinforcement learning algorithm based on policy\niteration. We show both policy improvement and policy convergence results for\nour proposed algorithm.\n","authors":["Jodi Dianetti","Giorgio Ferrari","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.09335v2.pdf","comment":"49 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.02135v2","updated":"2024-10-02T13:11:08Z","published":"2024-09-02T12:55:27Z","title":"Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based\n  Sampling","summary":"  Learning-based methods have gained attention as general-purpose solvers due\nto their ability to automatically learn problem-specific heuristics, reducing\nthe need for manually crafted heuristics. However, these methods often face\nscalability challenges. To address these issues, the improved Sampling\nalgorithm for Combinatorial Optimization (iSCO), using discrete Langevin\ndynamics, has been proposed, demonstrating better performance than several\nlearning-based solvers. This study proposes a different approach that\nintegrates gradient-based update through continuous relaxation, combined with\nQuasi-Quantum Annealing (QQA). QQA smoothly transitions the objective function,\nstarting from a simple convex function, minimized at half-integral values, to\nthe original objective function, where the relaxed variables are minimized only\nin the discrete space. Furthermore, we incorporate parallel run communication\nleveraging GPUs to enhance exploration capabilities and accelerate convergence.\nNumerical experiments demonstrate that our method is a competitive\ngeneral-purpose solver, achieving performance comparable to iSCO and\nlearning-based solvers across various benchmark problems. Notably, our method\nexhibits superior speed-quality trade-offs for large-scale instances compared\nto iSCO, learning-based solvers, commercial solvers, and specialized\nalgorithms.\n","authors":["Yuma Ichikawa","Yamato Arai"],"pdf_url":"https://arxiv.org/pdf/2409.02135v2.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.01516v1","updated":"2024-10-02T13:05:09Z","published":"2024-10-02T13:05:09Z","title":"Bounds on $L_p$ Errors in Density Ratio Estimation via $f$-Divergence\n  Loss Functions","summary":"  Density ratio estimation (DRE) is a fundamental machine learning technique\nfor identifying relationships between two probability distributions.\n$f$-divergence loss functions, derived from variational representations of\n$f$-divergence, are commonly employed in DRE to achieve state-of-the-art\nresults. This study presents a novel perspective on DRE using $f$-divergence\nloss functions by deriving the upper and lower bounds on $L_p$ errors. These\nbounds apply to any estimator within a class of Lipschitz continuous\nestimators, irrespective of the specific $f$-divergence loss functions\nutilized. The bounds are formulated as a product of terms that include the data\ndimension and the expected value of the density ratio raised to the power of\n$p$. Notably, the lower bound incorporates an exponential term dependent on the\nKullback--Leibler divergence, indicating that the $L_p$ error significantly\nincreases with the Kullback--Leibler divergence for $p > 1$, and this increase\nbecomes more pronounced as $p$ increases. Furthermore, these theoretical\nfindings are substantiated through numerical experiments.\n","authors":["Yoshiaki Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2410.01516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00535v2","updated":"2024-10-02T13:02:06Z","published":"2024-10-01T09:21:29Z","title":"Optimal Causal Representations and the Causal Information Bottleneck","summary":"  To effectively study complex causal systems, it is often useful to construct\nrepresentations that simplify parts of the system by discarding irrelevant\ndetails while preserving key features. The Information Bottleneck (IB) method\nis a widely used approach in representation learning that compresses random\nvariables while retaining information about a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces representations which are causally interpretable, and which can\nbe used when reasoning about interventions. We present experimental results\ndemonstrating that the learned representations accurately capture causality as\nintended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v2.pdf","comment":"Submitted to ICLR 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2410.01506v1","updated":"2024-10-02T12:58:55Z","published":"2024-10-02T12:58:55Z","title":"LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature\n  Fusion","summary":"  In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.\n","authors":["Dexuan Ding","Lei Wang","Liyun Zhu","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2410.01506v1.pdf","comment":"Research paper"},{"id":"http://arxiv.org/abs/2402.02041v3","updated":"2024-10-02T12:57:41Z","published":"2024-02-03T05:33:01Z","title":"$α$-Divergence Loss Function for Neural Density Ratio Estimation","summary":"  Density ratio estimation (DRE) is a fundamental machine learning technique\nfor capturing relationships between two probability distributions.\nState-of-the-art DRE methods estimate the density ratio using neural networks\ntrained with loss functions derived from variational representations of\n$f$-divergence. However, existing methods face optimization challenges, such as\noverfitting due to lower-unbounded loss functions, biased mini-batch gradients,\nvanishing training loss gradients, and high sample requirements for\nKullback-Leibler (KL) divergence loss functions. To address these issues, we\nfocus on $\\alpha$-divergence, which provides a suitable variational\nrepresentation of $f$-divergence. Subsequently, a novel loss function for DRE,\nthe $\\alpha$-divergence loss function ($\\alpha$-Div), is derived. $\\alpha$-Div\nis concise but offers stable and effective optimization for DRE. The\nboundedness of $\\alpha$-divergence provides the potential for successful DRE\nwith data exhibiting high KL-divergence. Our numerical experiments demonstrate\nthe effectiveness in optimization using $\\alpha$-Div. However, the experiments\nalso show that the proposed loss function offers no significant advantage over\nthe KL-divergence loss function in terms of RMSE for DRE. This indicates that\nthe accuracy of DRE is primarily determined by the amount of KL-divergence in\nthe data and is less dependent on $\\alpha$-divergence.\n","authors":["Yoshiaki Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2402.02041v3.pdf","comment":"$\\mathcal{T}_{\\text{Lip}}$ in Theorem 7.1 (Theorem B.15.) was changed\n  to the set of all locally Lipschitz continuous functions. In the previous\n  version, $\\mathcal{T}_{\\text{Lip}}$ was defined as the set of all Lipschitz\n  continuous functions, which is unsuitable for the statement of case (ii) in\n  the theorem"},{"id":"http://arxiv.org/abs/2402.05569v4","updated":"2024-10-02T12:57:32Z","published":"2024-02-08T11:10:39Z","title":"Training-Free Message Passing for Learning on Hypergraphs","summary":"  Hypergraphs are crucial for modelling higher-order interactions in real-world\ndata. Hypergraph neural networks (HNNs) effectively utilise these structures by\nmessage passing to generate informative node features for various downstream\ntasks like node classification. However, the message passing module in existing\nHNNs typically requires a computationally intensive training process, which\nlimits their practical use. To tackle this challenge, we propose an alternative\napproach by decoupling the usage of hypergraph structural information from the\nmodel learning stage. This leads to a novel training-free message passing\nmodule, named TF-MP-Module, which can be precomputed in the data preprocessing\nstage, thereby reducing the computational burden. We refer to the hypergraph\nneural network equipped with our TF-MP-Module as TF-HNN. We theoretically\nsupport the efficiency and effectiveness of TF-HNN by showing that: 1) It is\nmore training-efficient compared to existing HNNs; 2) It utilises as much\ninformation as existing HNNs for node feature generation; and 3) It is robust\nagainst the oversmoothing issue while using long-range interactions.\nExperiments based on seven real-world hypergraph benchmarks in node\nclassification and hyperlink prediction show that, compared to state-of-the-art\nHNNs, TF-HNN exhibits both competitive performance and superior training\nefficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN\noutperforms the node classification accuracy of the best baseline by 10% with\njust 1% of the training time of that baseline.\n","authors":["Bohan Tang","Zexi Liu","Keyue Jiang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2402.05569v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00488v2","updated":"2024-10-02T12:55:53Z","published":"2024-08-31T15:47:31Z","title":"Rapid Gyroscope Calibration: A Deep Learning Approach","summary":"  Low-cost gyroscope calibration is essential for ensuring the accuracy and\nreliability of gyroscope measurements. Stationary calibration estimates the\ndeterministic parts of measurement errors. To this end, a common practice is to\naverage the gyroscope readings during a predefined period and estimate the\ngyroscope bias. Calibration duration plays a crucial role in performance,\ntherefore, longer periods are preferred. However, some applications require\nquick startup times and calibration is therefore allowed only for a short time.\nIn this work, we focus on reducing low-cost gyroscope calibration time using\ndeep learning methods. We propose a deep-learning framework and explore the\npossibilities of using multiple real and virtual gyroscopes to improve the\ncalibration performance of single gyroscopes. To train and validate our\napproach, we recorded a dataset consisting of 169 hours of gyroscope readings,\nusing 24 gyroscopes of two different brands. We also created a virtual dataset\nconsisting of simulated gyroscope readings. The two datasets were used to\nevaluate our proposed approach. One of our key achievements in this work is\nreducing gyroscope calibration time by up to 89% using three low-cost\ngyroscopes.\n","authors":["Yair Stolero","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2409.00488v2.pdf","comment":"10 Pages, 14 Figures"},{"id":"http://arxiv.org/abs/2410.01500v1","updated":"2024-10-02T12:51:25Z","published":"2024-10-02T12:51:25Z","title":"Discrete Diffusion Schrödinger Bridge Matching for Graph\n  Transformation","summary":"  Transporting between arbitrary distributions is a fundamental goal in\ngenerative modeling. Recently proposed diffusion bridge models provide a\npotential solution, but they rely on a joint distribution that is difficult to\nobtain in practice. Furthermore, formulations based on continuous domains limit\ntheir applicability to discrete domains such as graphs. To overcome these\nlimitations, we propose Discrete Diffusion Schr\\\"odinger Bridge Matching\n(DDSBM), a novel framework that utilizes continuous-time Markov chains to solve\nthe SB problem in a high-dimensional discrete state space. Our approach extends\nIterative Markovian Fitting to discrete domains, and we have proved its\nconvergence to the SB. Furthermore, we adapt our framework for the graph\ntransformation and show that our design choice of underlying dynamics\ncharacterized by independent modifications of nodes and edges can be\ninterpreted as the entropy-regularized version of optimal transport with a cost\nfunction described by the graph edit distance. To demonstrate the effectiveness\nof our framework, we have applied DDSBM to molecular optimization in the field\nof chemistry. Experimental results demonstrate that DDSBM effectively optimizes\nmolecules' property-of-interest with minimal graph transformation, successfully\nretaining other features.\n","authors":["Jun Hyeong Kim","Seonghwan Kim","Seokhyun Moon","Hyeongwoo Kim","Jeheon Woo","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.14219v2","updated":"2024-10-02T12:45:50Z","published":"2024-05-23T06:28:44Z","title":"Understanding the Training and Generalization of Pretrained Transformer\n  for Sequential Decision Making","summary":"  In this paper, we consider the supervised pre-trained transformer for a class\nof sequential decision-making problems. The class of considered problems is a\nsubset of the general formulation of reinforcement learning in that there is no\ntransition probability matrix; though seemingly restrictive, the subset class\nof problems covers bandits, dynamic pricing, and newsvendor problems as special\ncases. Such a structure enables the use of optimal actions/decisions in the\npre-training phase, and the usage also provides new insights for the training\nand generalization of the pre-trained transformer. We first note the training\nof the transformer model can be viewed as a performative prediction problem,\nand the existing methods and theories largely ignore or cannot resolve an\nout-of-distribution issue. We propose a natural solution that includes the\ntransformer-generated action sequences in the training procedure, and it enjoys\nbetter properties both numerically and theoretically. The availability of the\noptimal actions in the considered tasks also allows us to analyze the\nproperties of the pre-trained transformer as an algorithm and explains why it\nmay lack exploration and how this can be automatically resolved. Numerically,\nwe categorize the advantages of pre-trained transformers over the structured\nalgorithms such as UCB and Thompson sampling into three cases: (i) it better\nutilizes the prior knowledge in the pre-training data; (ii) it can elegantly\nhandle the misspecification issue suffered by the structured algorithms; (iii)\nfor short time horizon such as $T\\le50$, it behaves more greedy and enjoys much\nbetter regret than the structured algorithms designed for asymptotic\noptimality.\n","authors":["Hanzhao Wang","Yu Pan","Fupeng Sun","Shang Liu","Kalyan Talluri","Guanting Chen","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.14219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01879v2","updated":"2024-10-02T12:42:56Z","published":"2024-02-02T20:08:11Z","title":"$σ$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial\n  Examples","summary":"  Evaluating the adversarial robustness of deep networks to gradient-based\nattacks is challenging. While most attacks consider $\\ell_2$- and\n$\\ell_\\infty$-norm constraints to craft input perturbations, only a few\ninvestigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular,\n$\\ell_0$-norm attacks remain the least studied due to the inherent complexity\nof optimizing over a non-convex and non-differentiable constraint. However,\nevaluating adversarial robustness under these attacks could reveal weaknesses\notherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm\nattacks. In this work, we propose a novel $\\ell_0$-norm attack, called\n$\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$\nnorm to facilitate gradient-based optimization, and an adaptive projection\noperator to dynamically adjust the trade-off between loss minimization and\nperturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet\ndatasets, involving robust and non-robust models, show that $\\sigma$-zero finds\nminimum $\\ell_0$-norm adversarial examples without requiring any time-consuming\nhyperparameter tuning, and that it outperforms all competing sparse attacks in\nterms of success rate, perturbation size, and efficiency.\n","authors":["Antonio Emanuele Cinà","Francesco Villani","Maura Pintor","Lea Schönherr","Battista Biggio","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2402.01879v2.pdf","comment":"Code available at\n  https://github.com/Cinofix/sigma-zero-adversarial-attack"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01483v1","updated":"2024-10-02T12:34:32Z","published":"2024-10-02T12:34:32Z","title":"Foldable SuperNets: Scalable Merging of Transformers with Different\n  Initializations and Tasks","summary":"  Many recent methods aim to merge neural networks (NNs) with identical\narchitectures trained on different tasks to obtain a single multi-task model.\nMost existing works tackle the simpler setup of merging NNs initialized from a\ncommon pre-trained network, where simple heuristics like weight averaging work\nwell. This work targets a more challenging goal: merging large transformers\ntrained on different tasks from distinct initializations. First, we demonstrate\nthat traditional merging methods fail catastrophically in this setup. To\novercome this challenge, we propose Foldable SuperNet Merge (FS-Merge), a\nmethod that optimizes a SuperNet to fuse the original models using a feature\nreconstruction loss. FS-Merge is simple, data-efficient, and capable of merging\nmodels of varying widths. We test FS-Merge against existing methods, including\nknowledge distillation, on MLPs and transformers across various settings,\nsizes, tasks, and modalities. FS-Merge consistently outperforms them, achieving\nSOTA results, particularly in limited data scenarios.\n","authors":["Edan Kinderman","Itay Hubara","Haggai Maron","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2410.01483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01482v1","updated":"2024-10-02T12:34:04Z","published":"2024-10-02T12:34:04Z","title":"One Wave to Explain Them All: A Unifying Perspective on Post-hoc\n  Explainability","summary":"  Despite the growing use of deep neural networks in safety-critical\ndecision-making, their inherent black-box nature hinders transparency and\ninterpretability. Explainable AI (XAI) methods have thus emerged to understand\na model's internal workings, and notably attribution methods also called\nsaliency maps. Conventional attribution methods typically identify the\nlocations -- the where -- of significant regions within an input. However,\nbecause they overlook the inherent structure of the input data, these methods\noften fail to interpret what these regions represent in terms of structural\ncomponents (e.g., textures in images or transients in sounds). Furthermore,\nexisting methods are usually tailored to a single data modality, limiting their\ngeneralizability. In this paper, we propose leveraging the wavelet domain as a\nrobust mathematical foundation for attribution. Our approach, the Wavelet\nAttribution Method (WAM) extends the existing gradient-based feature\nattributions into the wavelet domain, providing a unified framework for\nexplaining classifiers across images, audio, and 3D shapes. Empirical\nevaluations demonstrate that WAM matches or surpasses state-of-the-art methods\nacross faithfulness metrics and models in image, audio, and 3D explainability.\nFinally, we show how our method explains not only the where -- the important\nparts of the input -- but also the what -- the relevant patterns in terms of\nstructural components.\n","authors":["Gabriel Kasmi","Amandine Brunetto","Thomas Fel","Jayneel Parekh"],"pdf_url":"https://arxiv.org/pdf/2410.01482v1.pdf","comment":"main: 10 pages, appendix: 14 pages, 5 Tables, 25 Figures"},{"id":"http://arxiv.org/abs/2410.01480v1","updated":"2024-10-02T12:33:16Z","published":"2024-10-02T12:33:16Z","title":"Introducing Flexible Monotone Multiple Choice Item Response Theory\n  Models and Bit Scales","summary":"  Item Response Theory (IRT) is a powerful statistical approach for evaluating\ntest items and determining test taker abilities through response analysis. An\nIRT model that better fits the data leads to more accurate latent trait\nestimates. In this study, we present a new model for multiple choice data, the\nmonotone multiple choice (MMC) model, which we fit using autoencoders. Using\nboth simulated scenarios and real data from the Swedish Scholastic Aptitude\nTest, we demonstrate empirically that the MMC model outperforms the traditional\nnominal response IRT model in terms of fit. Furthermore, we illustrate how the\nlatent trait scale from any fitted IRT model can be transformed into a ratio\nscale, aiding in score interpretation and making it easier to compare different\ntypes of IRT models. We refer to these new scales as bit scales. Bit scales are\nespecially useful for models for which minimal or no assumptions are made for\nthe latent trait scale distributions, such as for the autoencoder fitted models\nin this study.\n","authors":["Joakim Wallmark","Maria Josefsson","Marie Wiberg"],"pdf_url":"https://arxiv.org/pdf/2410.01480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01476v1","updated":"2024-10-02T12:30:05Z","published":"2024-10-02T12:30:05Z","title":"Reducing Variance in Meta-Learning via Laplace Approximation for\n  Regression Tasks","summary":"  Given a finite set of sample points, meta-learning algorithms aim to learn an\noptimal adaptation strategy for new, unseen tasks. Often, this data can be\nambiguous as it might belong to different tasks concurrently. This is\nparticularly the case in meta-regression tasks. In such cases, the estimated\nadaptation strategy is subject to high variance due to the limited amount of\nsupport data for each task, which often leads to sub-optimal generalization\nperformance. In this work, we address the problem of variance reduction in\ngradient-based meta-learning and formalize the class of problems prone to this,\na condition we refer to as \\emph{task overlap}. Specifically, we propose a\nnovel approach that reduces the variance of the gradient estimate by weighing\neach support point individually by the variance of its posterior over the\nparameters. To estimate the posterior, we utilize the Laplace approximation,\nwhich allows us to express the variance in terms of the curvature of the loss\nlandscape of our meta-learner. Experimental results demonstrate the\neffectiveness of the proposed method and highlight the importance of variance\nreduction in meta-learning.\n","authors":["Alfredo Reichlin","Gustaf Tegnér","Miguel Vasco","Hang Yin","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2410.01476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10780v2","updated":"2024-10-02T12:27:13Z","published":"2024-07-15T14:59:43Z","title":"Correlations Are Ruining Your Gradient Descent","summary":"  Herein the topics of (natural) gradient descent, data decorrelation, and\napproximate methods for backpropagation are brought into a common discussion.\nNatural gradient descent illuminates how gradient vectors, pointing at\ndirections of steepest descent, can be improved by considering the local\ncurvature of loss landscapes. We extend this perspective and show that to fully\nsolve the problem illuminated by natural gradients in neural networks, one must\nrecognise that correlations in the data at any linear transformation, including\nnode responses at every layer of a neural network, cause a non-orthonormal\nrelationship between the model's parameters. To solve this requires a method\nfor decorrelating inputs at each individual layer of a neural network. We\ndescribe a range of methods which have been proposed for decorrelation and\nwhitening of node output, and expand on these to provide a novel method\nspecifically useful for distributed computing and computational neuroscience.\nImplementing decorrelation within multi-layer neural networks, we can show that\nnot only is training via backpropagation sped up significantly but also\nexisting approximations of backpropagation, which have failed catastrophically\nin the past, benefit significantly in their accuracy and convergence speed.\nThis has the potential to provide a route forward for approximate gradient\ndescent methods which have previously been discarded, training approaches for\nanalogue and neuromorphic hardware, and potentially insights as to the efficacy\nand utility of decorrelation processes in the brain.\n","authors":["Nasir Ahmad"],"pdf_url":"https://arxiv.org/pdf/2407.10780v2.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19531v2","updated":"2024-10-02T12:22:51Z","published":"2024-06-27T21:12:26Z","title":"Off-policy Evaluation with Deeply-abstracted States","summary":"  Off-policy evaluation (OPE) is crucial for assessing a target policy's impact\noffline before its deployment. However, achieving accurate OPE in large state\nspaces remains challenging. This paper studies state abstractions -- originally\ndesigned for policy learning -- in the context of OPE. Our contributions are\nthree-fold: (i) We define a set of irrelevance conditions central to learning\nstate abstractions for OPE, and derive a backward-model-irrelevance condition\nfor achieving irrelevance in %sequential and (marginalized) importance sampling\nratios by constructing a time-reversed Markov decision process (MDP). (ii) We\npropose a novel iterative procedure that sequentially projects the original\nstate space into a smaller space, resulting in a deeply-abstracted state, which\nsubstantially simplifies the sample complexity of OPE arising from high\ncardinality. (iii) We prove the Fisher consistencies of various OPE estimators\nwhen applied to our proposed abstract state spaces.\n","authors":["Meiling Hao","Pingfan Su","Liyuan Hu","Zoltan Szabo","Qingyuan Zhao","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2406.19531v2.pdf","comment":"56 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.09858v2","updated":"2024-10-02T12:22:10Z","published":"2024-08-19T10:03:14Z","title":"ShortCircuit: AlphaZero-Driven Circuit Design","summary":"  Chip design relies heavily on generating Boolean circuits, such as\nAND-Inverter Graphs (AIGs), from functional descriptions like truth tables.\nThis generation operation is a key process in logic synthesis, a primary chip\ndesign stage. While recent advances in deep learning have aimed to accelerate\ncircuit design, these efforts have mostly focused on tasks other than\nsynthesis, and traditional heuristic methods have plateaued. In this paper, we\nintroduce ShortCircuit, a novel transformer-based architecture that leverages\nthe structural properties of AIGs and performs efficient space exploration.\nContrary to prior approaches attempting end-to-end generation of logic circuits\nusing deep networks, ShortCircuit employs a two-phase process combining\nsupervised with reinforcement learning to enhance generalization to unseen\ntruth tables. We also propose an AlphaZero variant to handle the double\nexponentially large state space and the reward sparsity, enabling the discovery\nof near-optimal designs. To evaluate the generative performance of our model ,\nwe extract 500 truth tables from a set of 20 real-world circuits. ShortCircuit\nsuccessfully generates AIGs for $98\\%$ of the 8-input test truth tables, and\noutperforms the state-of-the-art logic synthesis tool, ABC, by $18.62\\%$ in\nterms of circuits size.\n","authors":["Dimitrios Tsaras","Antoine Grosnit","Lei Chen","Zhiyao Xie","Haitham Bou-Ammar","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01464v1","updated":"2024-10-02T12:16:46Z","published":"2024-10-02T12:16:46Z","title":"Flow Matching for Accelerated Simulation of Atomic Transport in\n  Materials","summary":"  We introduce LiFlow, a generative framework to accelerate molecular dynamics\n(MD) simulations for crystalline materials that formulates the task as\nconditional generation of atomic displacements. The model uses flow matching,\nwith a Propagator submodel to generate atomic displacements and a Corrector to\nlocally correct unphysical geometries, and incorporates an adaptive prior based\non the Maxwell-Boltzmann distribution to account for chemical and thermal\nconditions. We benchmark LiFlow on a dataset comprising 25-ps trajectories of\nlithium diffusion across 4,186 solid-state electrolyte (SSE) candidates at four\ntemperatures. The model obtains a consistent Spearman rank correlation of\n0.7-0.8 for lithium mean squared displacement (MSD) predictions on unseen\ncompositions. Furthermore, LiFlow generalizes from short training trajectories\nto larger supercells and longer simulations while maintaining high accuracy.\nWith speed-ups of up to 600,000$\\times$ compared to first-principles methods,\nLiFlow enables scalable simulations at significantly larger length and time\nscales.\n","authors":["Juno Nam","Sulin Liu","Gavin Winter","KyuJung Jun","Soojung Yang","Rafael Gómez-Bombarelli"],"pdf_url":"https://arxiv.org/pdf/2410.01464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01463v1","updated":"2024-10-02T12:14:36Z","published":"2024-10-02T12:14:36Z","title":"Selective Aggregation for Low-Rank Adaptation in Federated Learning","summary":"  We investigate LoRA in federated learning through the lens of the asymmetry\nanalysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$\nmatrices are responsible for learning general knowledge, while $B$ matrices\nfocus on capturing client-specific knowledge. Based on this finding, we\nintroduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two\nlow-rank trainable matrices $A$ and $B$ to model the weight update, but only\n$A$ matrices are shared with the server for aggregation. Moreover, we delve\ninto the relationship between the learned $A$ and $B$ matrices in other LoRA\nvariants, such as rsLoRA and VeRA, revealing a consistent pattern.\nConsequently, we extend our FedSA-LoRA method to these LoRA variants, resulting\nin FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm\nfor integrating LoRA with FL, offering guidance for future work on subsequent\nLoRA variants combined with FL. Extensive experimental results on natural\nlanguage understanding and generation tasks demonstrate the effectiveness of\nthe proposed method.\n","authors":["Pengxin Guo","Shuang Zeng","Yanran Wang","Huijie Fan","Feifei Wang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2410.01463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07656v4","updated":"2024-10-02T12:12:31Z","published":"2024-01-15T12:52:56Z","title":"Learning Explainable and Better Performing Representations of POMDP\n  Strategies","summary":"  Strategies for partially observable Markov decision processes (POMDP)\ntypically require memory. One way to represent this memory is via automata. We\npresent a method to learn an automaton representation of a strategy using a\nmodification of the L*-algorithm. Compared to the tabular representation of a\nstrategy, the resulting automaton is dramatically smaller and thus also more\nexplainable. Moreover, in the learning process, our heuristics may even improve\nthe strategy's performance. In contrast to approaches that synthesize an\nautomaton directly from the POMDP thereby solving it, our approach is\nincomparably more scalable.\n","authors":["Alexander Bork","Debraj Chakraborty","Kush Grover","Jan Kretinsky","Stefanie Mohr"],"pdf_url":"https://arxiv.org/pdf/2401.07656v4.pdf","comment":"Technical report for the submission to TACAS 24"},{"id":"http://arxiv.org/abs/2410.01458v1","updated":"2024-10-02T12:10:07Z","published":"2024-10-02T12:10:07Z","title":"From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with\n  LLM-Guided Knowledge","summary":"  Q-shaping is an extension of Q-value initialization and serves as an\nalternative to reward shaping for incorporating domain knowledge to accelerate\nagent training, thereby improving sample efficiency by directly shaping\nQ-values. This approach is both general and robust across diverse tasks,\nallowing for immediate impact assessment while guaranteeing optimality. We\nevaluated Q-shaping across 20 different environments using a large language\nmodel (LLM) as the heuristic provider. The results demonstrate that Q-shaping\nsignificantly enhances sample efficiency, achieving a \\textbf{16.87\\%}\nimprovement over the best baseline in each environment and a \\textbf{253.80\\%}\nimprovement compared to LLM-based reward shaping methods. These findings\nestablish Q-shaping as a superior and unbiased alternative to conventional\nreward shaping in reinforcement learning.\n","authors":["Xiefeng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01458v1.pdf","comment":"q-shaping, reinforcement learning, reward shaping"},{"id":"http://arxiv.org/abs/2410.01457v1","updated":"2024-10-02T12:07:47Z","published":"2024-10-02T12:07:47Z","title":"Verbalized Graph Representation Learning: A Fully Interpretable Graph\n  Model Based on Large Language Models Throughout the Entire Process","summary":"  Representation learning on text-attributed graphs (TAGs) has attracted\nsignificant interest due to its wide-ranging real-world applications,\nparticularly through Graph Neural Networks (GNNs). Traditional GNN methods\nfocus on encoding the structural information of graphs, often using shallow\ntext embeddings for node or edge attributes. This limits the model to\nunderstand the rich semantic information in the data and its reasoning ability\nfor complex downstream tasks, while also lacking interpretability. With the\nrise of large language models (LLMs), an increasing number of studies are\ncombining them with GNNs for graph representation learning and downstream\ntasks. While these approaches effectively leverage the rich semantic\ninformation in TAGs datasets, their main drawback is that they are only\npartially interpretable, which limits their application in critical fields. In\nthis paper, we propose a verbalized graph representation learning (VGRL) method\nwhich is fully interpretable. In contrast to traditional graph machine learning\nmodels, which are usually optimized within a continuous parameter space, VGRL\nconstrains this parameter space to be text description which ensures complete\ninterpretability throughout the entire process, making it easier for users to\nunderstand and trust the decisions of the model. We conduct several studies to\nempirically evaluate the effectiveness of VGRL and we believe these method can\nserve as a stepping stone in graph representation learning.\n","authors":["Xingyu Ji","Jiale Liu","Lu Li","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01457v1.pdf","comment":"under review. corresponding author: Zeyu Zhang"},{"id":"http://arxiv.org/abs/2410.01452v1","updated":"2024-10-02T12:02:43Z","published":"2024-10-02T12:02:43Z","title":"Ensembles provably learn equivariance through data augmentation","summary":"  Recently, it was proved that group equivariance emerges in ensembles of\nneural networks as the result of full augmentation in the limit of infinitely\nwide neural networks (neural tangent kernel limit). In this paper, we extend\nthis result significantly. We provide a proof that this emergence does not\ndepend on the neural tangent kernel limit at all. We also consider stochastic\nsettings, and furthermore general architectures. For the latter, we provide a\nsimple sufficient condition on the relation between the architecture and the\naction of the group for our results to hold. We validate our findings through\nsimple numeric experiments.\n","authors":["Oskar Nordenfors","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2410.01452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10115v2","updated":"2024-10-02T11:59:27Z","published":"2024-04-15T20:07:44Z","title":"Multiple-Input Fourier Neural Operator (MIFNO) for source-dependent 3D\n  elastodynamics","summary":"  Numerical simulations are essential tools to evaluate the solution of the\nwave equation in complex settings, such as three-dimensional (3D) domains with\nheterogeneous properties. However, their application is limited by high\ncomputational costs and existing surrogate models lack the flexibility of\nnumerical solvers. This work introduces the Multiple-Input Fourier Neural\nOperator (MIFNO) to deal with structured 3D fields representing material\nproperties as well as vectors describing the source characteristics. The MIFNO\nis applied to the problem of elastic wave propagation in the Earth's crust. It\nis trained on the HEMEW^S-3D database containing 30000 earthquake simulations\nin different heterogeneous domains with random source positions and\norientations. Outputs are time- and space-dependent surface wavefields. The\nMIFNO predictions are assessed as good to excellent based on Goodness-Of-Fit\n(GOF) criteria. Wave arrival times and wave fronts' propagation are very\naccurate since 80% of the predictions have an excellent phase GOF. The\nfluctuations amplitudes are good for 87% of the predictions. The envelope score\nis hindered by the small-scale fluctuations that are challenging to capture due\nto the complex physical phenomena associated with high-frequency features.\nNevertheless, the MIFNO can generalize to sources located outside the training\ndomain and it shows good generalization ability to a real complex overthrust\ngeology. When focusing on a region of interest, transfer learning improves the\naccuracy with limited additional costs, since GOF scores improved by more than\n1 GOF unit with only 500 additional specific samples. The MIFNO is the first\nsurrogate model offering the flexibility of an earthquake simulator with\nvarying sources and material properties. Its good accuracy and massive speed-up\noffer new perspectives to replace numerical simulations in many-query problems.\n","authors":["Fanny Lehmann","Filippo Gatti","Didier Clouteau"],"pdf_url":"https://arxiv.org/pdf/2404.10115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.09549v2","updated":"2024-10-02T11:44:26Z","published":"2024-06-13T19:30:32Z","title":"Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective","summary":"  Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01440v1","updated":"2024-10-02T11:42:49Z","published":"2024-10-02T11:42:49Z","title":"Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence\n  Modeling","summary":"  In the endeavor to make autonomous robots take actions, task planning is a\nmajor challenge that requires translating high-level task descriptions into\nlong-horizon action sequences. Despite recent advances in language model\nagents, they remain prone to planning errors and limited in their ability to\nplan ahead. To address these limitations in robotic planning, we advocate a\nself-refining scheme that iteratively refines a draft plan until an equilibrium\nis reached. Remarkably, this process can be optimized end-to-end from an\nanalytical perspective without the need to curate additional verifiers or\nreward models, allowing us to train self-refining planners in a simple\nsupervised learning fashion. Meanwhile, a nested equilibrium sequence modeling\nprocedure is devised for efficient closed-loop planning that incorporates\nuseful feedback from the environment (or an internal world model). Our method\nis evaluated on the VirtualHome-Env benchmark, showing advanced performance\nwith better scaling for inference computation. Code is available at\nhttps://github.com/Singularity0104/equilibrium-planner.\n","authors":["Jinghan Li","Zhicheng Sun","Fei Li","Cao Sheng","Jiazhong Yu","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2410.01440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01438v1","updated":"2024-10-02T11:40:49Z","published":"2024-10-02T11:40:49Z","title":"Information-Theoretical Principled Trade-off between Jailbreakability\n  and Stealthiness on Vision Language Models","summary":"  In recent years, Vision-Language Models (VLMs) have demonstrated significant\nadvancements in artificial intelligence, transforming tasks across various\ndomains. Despite their capabilities, these models are susceptible to jailbreak\nattacks, which can compromise their safety and reliability. This paper explores\nthe trade-off between jailbreakability and stealthiness in VLMs, presenting a\nnovel algorithm to detect non-stealthy jailbreak attacks and enhance model\nrobustness. We introduce a stealthiness-aware jailbreak attack using diffusion\nmodels, highlighting the challenge of detecting AI-generated content. Our\napproach leverages Fano's inequality to elucidate the relationship between\nattack success rates and stealthiness scores, providing an explainable\nframework for evaluating these threats. Our contributions aim to fortify AI\nsystems against sophisticated attacks, ensuring their outputs remain aligned\nwith ethical standards and user expectations.\n","authors":["Ching-Chia Kao","Chia-Mu Yu","Chun-Shien Lu","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v1","updated":"2024-10-02T11:36:45Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions via\nsubnetworks that can be composed to perform more complex tasks. Recent\ndevelopments in mechanistic interpretability have made progress in identifying\nsubnetworks, often referred to as circuits, which represent the minimal\ncomputational subgraph responsible for a model's behavior on specific tasks.\nHowever, most studies focus on identifying circuits for individual tasks\nwithout investigating how functionally similar circuits relate to each other.\nTo address this gap, we examine the modularity of neural networks by analyzing\ncircuits for highly compositional subtasks within a transformer-based language\nmodel. Specifically, given a probabilistic context-free grammar, we identify\nand compare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through subnetwork set operations to\nrepresent more complex functional capabilities of the model.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v1.pdf","comment":"24 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01432v1","updated":"2024-10-02T11:33:13Z","published":"2024-10-02T11:33:13Z","title":"Adaptive teachers for amortized samplers","summary":"  Amortized inference is the task of training a parametric model, such as a\nneural network, to approximate a distribution with a given unnormalized density\nwhere exact sampling is intractable. When sampling is implemented as a\nsequential decision-making process, reinforcement learning (RL) methods, such\nas generative flow networks, can be used to train the sampling policy.\nOff-policy RL training facilitates the discovery of diverse, high-reward\ncandidates, but existing methods still face challenges in efficient\nexploration. We propose to use an adaptive training distribution (the Teacher)\nto guide the training of the primary amortized sampler (the Student) by\nprioritizing high-loss regions. The Teacher, an auxiliary behavior model, is\ntrained to sample high-error regions of the Student and can generalize across\nunexplored modes, thereby enhancing mode coverage by providing an efficient\ntraining curriculum. We validate the effectiveness of this approach in a\nsynthetic environment designed to present an exploration challenge, two\ndiffusion-based sampling tasks, and four biochemical discovery tasks\ndemonstrating its ability to improve sample efficiency and mode coverage.\n","authors":["Minsu Kim","Sanghyeok Choi","Taeyoung Yun","Emmanuel Bengio","Leo Feng","Jarrid Rector-Brooks","Sungsoo Ahn","Jinkyoo Park","Nikolay Malkin","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.01432v1.pdf","comment":"26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.01431v1","updated":"2024-10-02T11:31:48Z","published":"2024-10-02T11:31:48Z","title":"Scalable Reinforcement Learning-based Neural Architecture Search","summary":"  In this publication, we assess the ability of a novel Reinforcement\nLearning-based solution to the problem of Neural Architecture Search, where a\nReinforcement Learning (RL) agent learns to search for good architectures,\nrather than to return a single optimal architecture. We consider both the\nNAS-Bench-101 and NAS- Bench-301 settings, and compare against various known\nstrong baselines, such as local search and random search. We conclude that our\nReinforcement Learning agent displays strong scalability with regards to the\nsize of the search space, but limited robustness to hyperparameter changes.\n","authors":["Amber Cassimon","Siegfried Mercelis","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2410.01431v1.pdf","comment":"33 Pages, 19 Figures"},{"id":"http://arxiv.org/abs/2410.01426v1","updated":"2024-10-02T11:23:09Z","published":"2024-10-02T11:23:09Z","title":"Approximation by Steklov Neural Network Operators","summary":"  The present paper deals with construction of newly family of Neural Network\noperators, that is,Steklov Neural Network operators. By using Steklov type\nintegral, we introduce a new version of Neural Network operators and we obtain\nsome convergence theorems for the family, such as, pointwise and uniform\nconvergence,rate of convergence via moduli of smoothness of order $r$.\n","authors":["S. N. Karaman","M. Turgay","T. Acar"],"pdf_url":"https://arxiv.org/pdf/2410.01426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.15432v2","updated":"2024-10-02T11:20:22Z","published":"2021-06-29T14:01:40Z","title":"On exploring the potential of quantum auto-encoder for learning quantum\n  systems","summary":"  The frequent interactions between quantum computing and machine learning\nrevolutionize both fields. One prototypical achievement is the quantum\nauto-encoder (QAE), as the leading strategy to relieve the curse of\ndimensionality ubiquitous in the quantum world. Despite its attractive\ncapabilities, practical applications of QAE have yet largely unexplored. To\nnarrow this knowledge gap, here we devise three effective QAE-based learning\nprotocols to address three classically computational hard learning problems\nwhen learning quantum systems, which are low-rank state fidelity estimation,\nquantum Fisher information estimation, and Gibbs state preparation. Attributed\nto the versatility of QAE, our proposals can be readily executed on near-term\nquantum machines. Besides, we analyze the error bounds of the trained protocols\nand showcase the necessary conditions to provide practical utility from the\nperspective of complexity theory. We conduct numerical simulations to confirm\nthe effectiveness of the proposed three protocols. Our work sheds new light on\ndeveloping advanced quantum learning algorithms to accomplish hard quantum\nphysics and quantum information processing tasks.\n","authors":["Yuxuan Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2106.15432v2.pdf","comment":"Accepted to IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2410.01423v1","updated":"2024-10-02T11:16:11Z","published":"2024-10-02T11:16:11Z","title":"Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data\n  Free Distillation","summary":"  This work presents Fair4Free, a novel generative model to generate synthetic\nfair data using data-free distillation in the latent space. Fair4Free can work\non the situation when the data is private or inaccessible. In our approach, we\nfirst train a teacher model to create fair representation and then distil the\nknowledge to a student model (using a smaller architecture). The process of\ndistilling the student model is data-free, i.e. the student model does not have\naccess to the training dataset while distilling. After the distillation, we use\nthe distilled model to generate fair synthetic samples. Our extensive\nexperiments show that our synthetic samples outperform state-of-the-art models\nin all three criteria (fairness, utility and synthetic quality) with a\nperformance increase of 5% for fairness, 8% for utility and 12% in synthetic\nquality for both tabular and image datasets.\n","authors":["Md Fahim Sikder","Daniel de Leng","Fredrik Heintz"],"pdf_url":"https://arxiv.org/pdf/2410.01423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06835v4","updated":"2024-10-02T11:15:25Z","published":"2023-11-12T13:25:28Z","title":"Open-Set Graph Anomaly Detection via Normal Structure Regularisation","summary":"  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). Those labelled training data provide crucial prior\nknowledge about abnormalities for GAD models, enabling substantially reduced\ndetection errors. However, current supervised GAD methods tend to\nover-emphasise fitting the seen anomalies, leading to many errors of detecting\nthe unseen anomalies as normal nodes. Further, existing open-set AD models were\nintroduced to handle Euclidean data, failing to effectively capture\ndiscriminative features from graph structure and node attributes for GAD. In\nthis work, we propose a novel open-set GAD approach, namely normal structure\nregularisation (NSReg), to achieve generalised detection ability to unseen\nanomalies, while maintaining its effectiveness on detecting seen anomalies. The\nkey idea in NSReg is to introduce a regularisation term that enforces the\nlearning of compact, semantically-rich representations of normal nodes based on\ntheir structural relations to other nodes. When being optimised with supervised\nanomaly detection losses, the regularisation term helps incorporate strong\nnormality into the modelling, and thus, it effectively avoids over-fitting the\nseen anomalies and learns a better normality decision boundary, largely\nreducing the false negatives of detecting unseen anomalies as normal. Extensive\nempirical results on seven real-world datasets show that NSReg significantly\noutperforms state-of-the-art competing methods by at least 14% AUC-ROC on the\nunseen anomaly classes and by 10% AUC-ROC on all anomaly classes.\n","authors":["Qizhou Wang","Guansong Pang","Mahsa Salehi","Xiaokun Xia","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2311.06835v4.pdf","comment":null}]},"2024-10-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.02763v1","updated":"2024-10-03T17:59:58Z","published":"2024-10-03T17:59:58Z","title":"Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos","summary":"  There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02763v1.pdf","comment":"Project Page: https://vinoground.github.io"},{"id":"http://arxiv.org/abs/2404.10917v2","updated":"2024-10-03T17:59:55Z","published":"2024-04-16T21:33:05Z","title":"Which questions should I answer? Salience Prediction of Inquisitive\n  Questions","summary":"  Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.\n","authors":["Yating Wu","Ritika Mangla","Alexandros G. Dimakis","Greg Durrett","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2404.10917v2.pdf","comment":"Camera Ready for EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02760v1","updated":"2024-10-03T17:59:30Z","published":"2024-10-03T17:59:30Z","title":"Erasing Conceptual Knowledge from Language Models","summary":"  Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info\n","authors":["Rohit Gandikota","Sheridan Feucht","Samuel Marks","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.02760v1.pdf","comment":"Project Page: https://elm.baulab.info"},{"id":"http://arxiv.org/abs/2410.02756v1","updated":"2024-10-03T17:58:55Z","published":"2024-10-03T17:58:55Z","title":"CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text","summary":"  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on\nMultilingual Coreference Resolution. In this third iteration of the shared\ntask, a novel objective is to also predict empty nodes needed for zero\ncoreference mentions (while the empty nodes were given on input in previous\nyears). This way, coreference resolution can be performed on raw text. We\nevaluate two model variants: a~two-stage approach (where the empty nodes are\npredicted first using a pretrained encoder model and then processed together\nwith sentence words by another pretrained model) and a single-stage approach\n(where a single pretrained encoder model generates empty nodes, coreference\nmentions, and coreference links jointly). In both settings, CorPipe surpasses\nother participants by a large margin of 3.9 and 2.8 percent points,\nrespectively. The source code and the trained model are available at\nhttps://github.com/ufal/crac2024-corpipe .\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2410.02756v1.pdf","comment":"Accepted to CRAC 2024"},{"id":"http://arxiv.org/abs/2410.02755v1","updated":"2024-10-03T17:58:29Z","published":"2024-10-03T17:58:29Z","title":"SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost","summary":"  Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.\n","authors":["Jifan Zhang","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v1","updated":"2024-10-03T17:57:22Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18957v2","updated":"2024-10-03T17:57:07Z","published":"2024-09-27T17:58:50Z","title":"LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction","summary":"  Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2409.18957v2.pdf","comment":"Updated title, abstract, and images"},{"id":"http://arxiv.org/abs/2410.02748v1","updated":"2024-10-03T17:57:01Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11687v2","updated":"2024-10-03T17:56:34Z","published":"2024-06-17T16:05:32Z","title":"Tokenization Falling Short: The Curse of Tokenization","summary":"  Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.\n","authors":["Yekun Chai","Yewei Fang","Qiwei Peng","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2406.11687v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2404.07840v3","updated":"2024-10-03T17:56:12Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We make our code and data publicly available at\nhttps://github.com/ernie-research/gptfluence.\n","authors":["Yekun Chai","Qingyi Liu","Shuohuan Wang","Yu Sun","Qiwei Peng","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2307.10432v3","updated":"2024-10-03T17:55:29Z","published":"2023-07-19T19:40:34Z","title":"PharmacyGPT: The AI Pharmacist","summary":"  In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.\n","authors":["Zhengliang Liu","Zihao Wu","Mengxuan Hu","Bokai Zhao","Lin Zhao","Tianyi Zhang","Haixing Dai","Xianyan Chen","Ye Shen","Sheng Li","Quanzheng Li","Xiang Li","Brian Murray","Tianming Liu","Andrea Sikora"],"pdf_url":"https://arxiv.org/pdf/2307.10432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02744v1","updated":"2024-10-03T17:55:17Z","published":"2024-10-03T17:55:17Z","title":"Neutral residues: revisiting adapters for model extension","summary":"  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.\n","authors":["Franck Signe Talla","Herve Jegou","Edouard Grave"],"pdf_url":"https://arxiv.org/pdf/2410.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02743v1","updated":"2024-10-03T17:55:13Z","published":"2024-10-03T17:55:13Z","title":"MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions","summary":"  Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .\n","authors":["Yekun Chai","Haoran Sun","Huang Fang","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2410.02743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02742v1","updated":"2024-10-03T17:55:09Z","published":"2024-10-03T17:55:09Z","title":"Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models","summary":"  Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.\n","authors":["Haolan Liu","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.02742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02741v1","updated":"2024-10-03T17:54:56Z","published":"2024-10-03T17:54:56Z","title":"Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Lei Xu","Mohammed Asad Karim","Saket Dingliwal","Aparna Elangovan"],"pdf_url":"https://arxiv.org/pdf/2410.02741v1.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.02736v1","updated":"2024-10-03T17:53:30Z","published":"2024-10-03T17:53:30Z","title":"Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge","summary":"  LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.\n","authors":["Jiayi Ye","Yanbo Wang","Yue Huang","Dongping Chen","Qihui Zhang","Nuno Moniz","Tian Gao","Werner Geyer","Chao Huang","Pin-Yu Chen","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02730v1","updated":"2024-10-03T17:49:28Z","published":"2024-10-03T17:49:28Z","title":"DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes\n  and Objects","summary":"  Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent.\n","authors":["Zhaowei Wang","Hongming Zhang","Tianqing Fang","Ye Tian","Yue Yang","Kaixin Ma","Xiaoman Pan","Yangqiu Song","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02730v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.02729v1","updated":"2024-10-03T17:49:09Z","published":"2024-10-03T17:49:09Z","title":"Unified Multi-Modal Interleaved Document Representation for Information\n  Retrieval","summary":"  Information Retrieval (IR) methods aim to identify relevant documents in\nresponse to a given query, which have gained remarkable attention due to their\nsuccessful application in various natural language tasks. However, existing\napproaches typically consider only the textual information within the\ndocuments, which overlooks the fact that documents can contain multiple\nmodalities, including texts, images, and tables. Further, they often segment\neach long document into multiple discrete passages for embedding, preventing\nthem from capturing the overall document context and interactions between\nparagraphs. We argue that these two limitations lead to suboptimal document\nrepresentations for retrieval. In this work, to address them, we aim to produce\nmore comprehensive and nuanced document representations by holistically\nembedding documents interleaved with different modalities. Specifically, we\nachieve this by leveraging the capability of recent vision-language models that\nenable the processing and integration of text, images, and tables into a\nunified format and representation. Moreover, to mitigate the information loss\nfrom segmenting documents into passages, instead of representing and retrieving\npassages individually, we further merge the representations of segmented\npassages into one single document representation, while we additionally\nintroduce a reranking strategy to decouple and identify the relevant passage\nwithin the document if necessary. Then, through extensive experiments on\ndiverse information retrieval scenarios considering both the textual and\nmultimodal queries, we show that our approach substantially outperforms\nrelevant baselines, thanks to the consideration of the multimodal information\ninterleaved within the documents in a unified way.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02725v1","updated":"2024-10-03T17:47:29Z","published":"2024-10-03T17:47:29Z","title":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation","summary":"  Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.\n","authors":["Rohin Manvi","Anikait Singh","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.02725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10710v3","updated":"2024-10-03T17:46:40Z","published":"2024-04-16T16:36:50Z","title":"Autoregressive Pre-Training on Pixels and Texts","summary":"  The integration of visual and textual information represents a promising\ndirection in the advancement of language models. In this paper, we explore the\ndual modality of language--both visual and textual--within an autoregressive\nframework, pre-trained on both document images and texts. Our method employs a\nmultimodal training strategy, utilizing visual data through next patch\nprediction with a regression head and/or textual data through next token\nprediction with a classification head. We focus on understanding the\ninteraction between these two modalities and their combined impact on model\nperformance. Our extensive evaluation across a wide range of benchmarks shows\nthat incorporating both visual and textual data significantly improves the\nperformance of pixel-based language models. Remarkably, we find that a\nunidirectional pixel-based model trained solely on visual data can achieve\ncomparable results to state-of-the-art bidirectional models on several language\nunderstanding tasks. This work uncovers the untapped potential of integrating\nvisual and textual modalities for more effective language modeling. We release\nour code, data, and model checkpoints at\n\\url{https://github.com/ernie-research/pixelgpt}.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02724v1","updated":"2024-10-03T17:45:31Z","published":"2024-10-03T17:45:31Z","title":"Large Language Models as Markov Chains","summary":"  Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.\n","authors":["Oussama Zekri","Ambroise Odonnat","Abdelhakim Benechehab","Linus Bleistein","Nicolas Boullé","Ievgen Redko"],"pdf_url":"https://arxiv.org/pdf/2410.02724v1.pdf","comment":"49 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.02721v1","updated":"2024-10-03T17:40:55Z","published":"2024-10-03T17:40:55Z","title":"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization","summary":"  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.\n","authors":["Ryan C. Barron","Ves Grantcharov","Selma Wanna","Maksim E. Eren","Manish Bhattarai","Nicholas Solovyev","George Tompkins","Charles Nicholas","Kim Ø. Rasmussen","Cynthia Matuszek","Boian S. Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2410.02721v1.pdf","comment":"9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024"},{"id":"http://arxiv.org/abs/2410.02719v1","updated":"2024-10-03T17:39:38Z","published":"2024-10-03T17:39:38Z","title":"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling\n  for Retrieval-Augmented Generation","summary":"  We present UncertaintyRAG, a novel approach for long-context\nRetrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio\n(SNR)-based span uncertainty to estimate similarity between text chunks. This\nspan uncertainty enhances model calibration, improving robustness and\nmitigating semantic inconsistencies introduced by random chunking. Leveraging\nthis insight, we propose an efficient unsupervised learning technique to train\nthe retrieval model, alongside an effective data sampling and scaling strategy.\nUncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving\nstate-of-the-art results while using only 4% of the training data compared to\nother advanced open-source retrieval models under distribution shift settings.\nOur method demonstrates strong calibration through span uncertainty, leading to\nimproved generalization and robustness in long-context RAG tasks. Additionally,\nUncertaintyRAG provides a lightweight retrieval model that can be integrated\ninto any large language model with varying context window lengths, without the\nneed for fine-tuning, showcasing the flexibility of our approach.\n","authors":["Zixuan Li","Jing Xiong","Fanghua Ye","Chuanyang Zheng","Xun Wu","Jianqiao Lu","Zhongwei Wan","Xiaodan Liang","Chengming Li","Zhenan Sun","Lingpeng Kong","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.02719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02713v1","updated":"2024-10-03T17:36:49Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v1.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02712v1","updated":"2024-10-03T17:36:33Z","published":"2024-10-03T17:36:33Z","title":"LLaVA-Critic: Learning to Evaluate Multimodal Models","summary":"  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.\n","authors":["Tianyi Xiong","Xiyao Wang","Dong Guo","Qinghao Ye","Haoqi Fan","Quanquan Gu","Heng Huang","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02712v1.pdf","comment":"Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic"},{"id":"http://arxiv.org/abs/2410.02707v1","updated":"2024-10-03T17:31:31Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02703v1","updated":"2024-10-03T17:27:30Z","published":"2024-10-03T17:27:30Z","title":"Selective Attention Improves Transformer","summary":"  Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.\n","authors":["Yaniv Leviathan","Matan Kalman","Yossi Matias"],"pdf_url":"https://arxiv.org/pdf/2410.02703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12683v2","updated":"2024-10-03T17:27:28Z","published":"2023-12-20T00:49:52Z","title":"Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?","summary":"  The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.\n","authors":["Tannon Kew","Florian Schottmann","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2312.12683v2.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.07071v2","updated":"2024-10-03T17:26:48Z","published":"2024-07-09T17:44:34Z","title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps","summary":"  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n","authors":["Yung-Sung Chuang","Linlu Qiu","Cheng-Yu Hsieh","Ranjay Krishna","Yoon Kim","James Glass"],"pdf_url":"https://arxiv.org/pdf/2407.07071v2.pdf","comment":"EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens"},{"id":"http://arxiv.org/abs/2311.00237v3","updated":"2024-10-03T17:25:02Z","published":"2023-11-01T02:40:42Z","title":"The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis","summary":"  Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey.\n","authors":["Yuxiang Zhou","Jiazheng Li","Yanzheng Xiang","Hanqi Yan","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2311.00237v3.pdf","comment":"Accepted to the main conference of EMNLP 2024. Resources are\n  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources"},{"id":"http://arxiv.org/abs/2410.02694v1","updated":"2024-10-03T17:20:11Z","published":"2024-10-03T17:20:11Z","title":"HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly","summary":"  There have been many benchmarks for evaluating long-context language models\n(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack\n(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate\nto the diverse downstream applications of LCLMs, and the inconsistency further\ncomplicates model comparison. We investigate the underlying reasons behind\ncurrent practices and find that existing benchmarks often provide noisy signals\ndue to low coverage of applications, insufficient lengths, unreliable metrics,\nand incompatibility with base models. In this work, we present HELMET (How to\nEvaluate Long-context Models Effectively and Thoroughly), a comprehensive\nbenchmark encompassing seven diverse, application-centric categories. We also\naddress many issues in previous benchmarks by adding controllable lengths up to\n128k tokens, model-based evaluation for reliable metrics, and few-shot\nprompting for robustly evaluating base models. Consequently, we demonstrate\nthat HELMET offers more reliable and consistent rankings of frontier LCLMs.\nThrough a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks\nlike NIAH are not good predictors of downstream performance; (2) the diverse\ncategories in HELMET exhibit distinct trends and low correlation with each\nother; and (3) while most LCLMs achieve perfect NIAH scores, open-source models\nsignificantly lag behind closed ones when the task requires full-context\nreasoning or following complex instructions -- the gap widens with increased\nlengths. Finally, we recommend using our RAG tasks for fast model development,\nas they are easy to run and more predictive of other downstream performance;\nultimately, we advocate for a holistic evaluation across diverse tasks.\n","authors":["Howard Yen","Tianyu Gao","Minmin Hou","Ke Ding","Daniel Fleischer","Peter Izasak","Moshe Wasserblat","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02694v1.pdf","comment":"Code and data are available here:\n  https://github.com/princeton-nlp/HELMET"},{"id":"http://arxiv.org/abs/2410.02691v1","updated":"2024-10-03T17:18:03Z","published":"2024-10-03T17:18:03Z","title":"On the Proper Treatment of Tokenization in Psycholinguistics","summary":"  Language models are widely used in computational psycholinguistics to test\ntheories that relate the negative log probability (the surprisal) of a region\nof interest (a substring of characters) under a language model to its cognitive\ncost experienced by readers, as operationalized, for example, by gaze duration\non the region. However, the application of modern language models to\npsycholinguistic studies is complicated by the practice of using tokenization\nas an intermediate step in training a model. Doing so results in a language\nmodel over token strings rather than one over character strings. Vexingly,\nregions of interest are generally misaligned with these token strings. The\npaper argues that token-level language models should be (approximately)\nmarginalized into character-level language models before they are used in\npsycholinguistic studies to compute the surprisal of a region of interest;\nthen, the marginalized character-level language model can be used to compute\nthe surprisal of an arbitrary character substring, which we term a focal area,\nthat the experimenter may wish to use as a predictor. Our proposal of\nmarginalizing a token-level model into a character-level one solves this\nmisalignment issue independently of the tokenization scheme. Empirically, we\ndiscover various focal areas whose surprisal is a better psychometric predictor\nthan the surprisal of the region of interest itself.\n","authors":["Mario Giulianelli","Luca Malagutti","Juan Luis Gastaldi","Brian DuSell","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.02691v1.pdf","comment":"Main conference long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.03741v2","updated":"2024-10-03T17:15:24Z","published":"2024-01-08T09:01:29Z","title":"Enhanced Automated Code Vulnerability Repair using Large Language Models","summary":"  This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.\n","authors":["David de-Fitero-Dominguez","Eva Garcia-Lopez","Antonio Garcia-Cabot","Jose-Javier Martinez-Herraiz"],"pdf_url":"https://arxiv.org/pdf/2401.03741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03650v2","updated":"2024-10-03T17:13:04Z","published":"2024-09-05T16:08:19Z","title":"On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.\n","authors":["Yong Lin","Skyler Seto","Maartje ter Hoeve","Katherine Metcalf","Barry-John Theobald","Xuan Wang","Yizhe Zhang","Chen Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03650v2.pdf","comment":"12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.02684v1","updated":"2024-10-03T17:10:41Z","published":"2024-10-03T17:10:41Z","title":"HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router","summary":"  As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Ruibin Yuan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.02684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18725v2","updated":"2024-10-03T17:10:09Z","published":"2024-06-26T19:48:48Z","title":"Jailbreaking LLMs with Arabic Transliteration and Arabizi","summary":"  This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.\n","authors":["Mansour Al Ghanim","Saleh Almohaimeed","Mengxin Zheng","Yan Solihin","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2406.18725v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02683v1","updated":"2024-10-03T17:08:52Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2311.09756v2","updated":"2024-10-03T17:04:50Z","published":"2023-11-16T10:30:26Z","title":"StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning","summary":"  Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v2.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02678v1","updated":"2024-10-03T17:04:48Z","published":"2024-10-03T17:04:48Z","title":"Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data","summary":"  Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.\n","authors":["William Held","Ella Li","Michael Ryan","Weiyan Shi","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02677v1","updated":"2024-10-03T17:04:31Z","published":"2024-10-03T17:04:31Z","title":"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs","summary":"  To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.\n","authors":["Yu Ying Chiu","Liwei Jiang","Bill Yuchen Lin","Chan Young Park","Shuyue Stella Li","Sahithya Ravi","Mehar Bhatia","Maria Antoniak","Yulia Tsvetkov","Vered Shwartz","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02677v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.02675v1","updated":"2024-10-03T17:02:21Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable success achieved by neural networks, particularly\nthose represented by MLP and Transformer, we reveal that they exhibit potential\nflaws in the modeling and reasoning of periodicity, i.e., they tend to memorize\nthe periodic data rather than genuinely understanding the underlying principles\nof periodicity. However, periodicity is a crucial trait in various forms of\nreasoning and generalization, underpinning predictability across natural and\nengineered systems through recurring patterns in observations. In this paper,\nwe propose FAN, a novel network architecture based on Fourier Analysis, which\nempowers the ability to efficiently model and reason about periodic phenomena.\nBy introducing Fourier Series, the periodicity is naturally integrated into the\nstructure and computational processes of the neural network, thus achieving a\nmore accurate expression and prediction of periodic patterns. As a promising\nsubstitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in\nvarious models with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the effectiveness of FAN in modeling and reasoning about\nperiodic functions, and the superiority and generalizability of FAN across a\nrange of real-world tasks, including symbolic formula representation, time\nseries forecasting, and language modeling.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02674v1","updated":"2024-10-03T16:58:21Z","published":"2024-10-03T16:58:21Z","title":"Examining Language Modeling Assumptions Using an Annotated Literary\n  Dialect Corpus","summary":"  We present a dataset of 19th century American literary orthovariant tokens\nwith a novel layer of human-annotated dialect group tags designed to serve as\nthe basis for computational experiments exploring literarily meaningful\northographic variation. We perform an initial broad set of experiments over\nthis dataset using both token (BERT) and character (CANINE)-level contextual\nlanguage models. We find indications that the \"dialect effect\" produced by\nintentional orthographic variation employs multiple linguistic channels, and\nthat these channels are able to be surfaced to varied degrees given particular\nlanguage modelling assumptions. Specifically, we find evidence showing that\nchoice of tokenization scheme meaningfully impact the type of orthographic\ninformation a model is able to surface.\n","authors":["Craig Messner","Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2410.02674v1.pdf","comment":"Accepted to NLP4DH@EMNLP2024"},{"id":"http://arxiv.org/abs/2407.07950v2","updated":"2024-10-03T16:54:59Z","published":"2024-07-10T18:00:05Z","title":"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance","summary":"  The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.\n","authors":["Kaitlyn Zhou","Jena D. Hwang","Xiang Ren","Nouha Dziri","Dan Jurafsky","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2407.07950v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.07565v3","updated":"2024-10-03T16:48:55Z","published":"2024-07-10T11:50:20Z","title":"On Leakage of Code Generation Evaluation Datasets","summary":"  In this paper, we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nTo address this, we release Less Basic Python Problems (LBPP): an\nuncontaminated new benchmark of 161 prompts with their associated Python\nsolutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .\n","authors":["Alexandre Matton","Tom Sherborne","Dennis Aumiller","Elena Tommasone","Milad Alizadeh","Jingyi He","Raymond Ma","Maxime Voisin","Ellen Gilsenan-McMahon","Matthias Gallé"],"pdf_url":"https://arxiv.org/pdf/2407.07565v3.pdf","comment":"EMNLP 2024 Findings. 5 main pages, 9 in total"},{"id":"http://arxiv.org/abs/2410.02660v1","updated":"2024-10-03T16:46:52Z","published":"2024-10-03T16:46:52Z","title":"How to Train Long-Context Language Models (Effectively)","summary":"  We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.\n","authors":["Tianyu Gao","Alexander Wettig","Howard Yen","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02660v1.pdf","comment":"Our code, data, and models are available at\n  https://github.com/princeton-nlp/ProLong"},{"id":"http://arxiv.org/abs/2407.11969v3","updated":"2024-10-03T16:46:09Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v3.pdf","comment":"Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2410.02657v1","updated":"2024-10-03T16:43:17Z","published":"2024-10-03T16:43:17Z","title":"Hate Personified: Investigating the role of LLMs in content moderation","summary":"  For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.\n","authors":["Sarah Masud","Sahajpreet Singh","Viktor Hangya","Alexander Fraser","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2410.02657v1.pdf","comment":"17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains"},{"id":"http://arxiv.org/abs/2402.16382v2","updated":"2024-10-03T16:39:32Z","published":"2024-02-26T08:08:03Z","title":"Immunization against harmful fine-tuning attacks","summary":"  Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","Łukasz Bartoszcze","Jan Batzner","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2402.16382v2.pdf","comment":"Published in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02653v1","updated":"2024-10-03T16:36:35Z","published":"2024-10-03T16:36:35Z","title":"Measuring and Improving Persuasiveness of Generative Models","summary":"  LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.\n","authors":["Somesh Singh","Yaman K Singla","Harini SI","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2410.02653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02650v1","updated":"2024-10-03T16:34:46Z","published":"2024-10-03T16:34:46Z","title":"Undesirable Memorization in Large Language Models: A Survey","summary":"  While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.\n","authors":["Ali Satvaty","Suzan Verberne","Fatih Turkmen"],"pdf_url":"https://arxiv.org/pdf/2410.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02647v1","updated":"2024-10-03T16:33:35Z","published":"2024-10-03T16:33:35Z","title":"Immunogenicity Prediction with Dual Attention Enables Vaccine Target\n  Selection","summary":"  Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce ProVaccine, a novel\ndeep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 9,500 antigen sequences, structures, and immunogenicity\nlabels from bacteria, viruses, and tumors. Extensive experiments demonstrate\nthat ProVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research.\n","authors":["Song Li","Yang Tan","Song Ke","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02647v1.pdf","comment":"18 pages, 11 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.02026v2","updated":"2024-10-03T16:31:59Z","published":"2024-09-03T16:20:22Z","title":"Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization","summary":"  In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.\n","authors":["Sean I. Young"],"pdf_url":"https://arxiv.org/pdf/2409.02026v2.pdf","comment":"Preprint. 17 pages, 4 figures, 5 appendices"},{"id":"http://arxiv.org/abs/2409.11295v2","updated":"2024-10-03T16:30:43Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.02642v1","updated":"2024-10-03T16:25:37Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jiménez Gutiérrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02631v1","updated":"2024-10-03T16:15:04Z","published":"2024-10-03T16:15:04Z","title":"Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning","summary":"  Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.\n","authors":["Tianxiang Hu","Pei Zhang","Baosong Yang","Jun Xie","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.02631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08702v4","updated":"2024-10-03T16:11:43Z","published":"2024-02-13T16:38:01Z","title":"PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling","summary":"  Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.\n","authors":["Yongchao Chen","Jacob Arkin","Yilun Hao","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2402.08702v4.pdf","comment":"62 pages, 14 figures, Published in EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2403.13681v2","updated":"2024-10-03T16:01:01Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?","summary":"  In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01744v2","updated":"2024-10-03T15:57:05Z","published":"2024-10-02T16:55:01Z","title":"Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v2.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2409.05197v2","updated":"2024-10-03T15:55:40Z","published":"2024-09-08T19:22:58Z","title":"Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?","summary":"  State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.\n","authors":["Neeladri Bhuiya","Viktor Schlegel","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2409.05197v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.12191v2","updated":"2024-10-03T15:54:49Z","published":"2024-09-18T17:59:32Z","title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution","summary":"  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .\n","authors":["Peng Wang","Shuai Bai","Sinan Tan","Shijie Wang","Zhihao Fan","Jinze Bai","Keqin Chen","Xuejing Liu","Jialin Wang","Wenbin Ge","Yang Fan","Kai Dang","Mengfei Du","Xuancheng Ren","Rui Men","Dayiheng Liu","Chang Zhou","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.12191v2.pdf","comment":"Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors"},{"id":"http://arxiv.org/abs/2410.02613v1","updated":"2024-10-03T15:51:36Z","published":"2024-10-03T15:51:36Z","title":"NL-Eye: Abductive NLI for Images","summary":"  Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.\n","authors":["Mor Ventura","Michael Toker","Nitay Calderon","Zorik Gekhman","Yonatan Bitton","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2410.02613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02611v1","updated":"2024-10-03T15:50:08Z","published":"2024-10-03T15:50:08Z","title":"IndicSentEval: How Effectively do Multilingual Transformer Models encode\n  Linguistic Properties for Indic Languages?","summary":"  Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].\n","authors":["Akhilesh Aravapalli","Mounika Marreddy","Subba Reddy Oota","Radhika Mamidi","Manish Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.02611v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.02609v1","updated":"2024-10-03T15:49:35Z","published":"2024-10-03T15:49:35Z","title":"Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in\n  Under-Resourced Languages Using Explainable AI","summary":"  The proliferation of fake news has emerged as a significant threat to the\nintegrity of information dissemination, particularly on social media platforms.\nMisinformation can spread quickly due to the ease of creating and disseminating\ncontent, affecting public opinion and sociopolitical events. Identifying false\ninformation is therefore essential to reducing its negative consequences and\nmaintaining the reliability of online news sources. Traditional approaches to\nfake news detection often rely solely on content-based features, overlooking\nthe crucial role of social context in shaping the perception and propagation of\nnews articles. In this paper, we propose a comprehensive approach that\nintegrates social context-based features with news content features to enhance\nthe accuracy of fake news detection in under-resourced languages. We perform\nseveral experiments utilizing a variety of methodologies, including traditional\nmachine learning, neural networks, ensemble learning, and transfer learning.\nAssessment of the outcomes of the experiments shows that the ensemble learning\napproach has the highest accuracy, achieving a 0.99 F1 score. Additionally,\nwhen compared with monolingual models, the fine-tuned model with the target\nlanguage outperformed others, achieving a 0.94 F1 score. We analyze the\nfunctioning of the models, considering the important features that contribute\nto model performance, using explainable AI techniques.\n","authors":["Mesay Gemeda Yigezu","Melkamu Abay Mersha","Girma Yohannis Bade","Jugal Kalita","Olga Kolesnikova","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2410.02609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10960v3","updated":"2024-10-03T15:48:45Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v3.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.18256v3","updated":"2024-10-03T15:48:31Z","published":"2024-06-26T11:08:17Z","title":"Llamipa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.18164v3","updated":"2024-10-03T15:46:16Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2304.08460v3","updated":"2024-10-03T15:46:13Z","published":"2023-04-17T17:36:35Z","title":"LongForm: Effective Instruction Tuning with Reverse Instructions","summary":"  Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.\n","authors":["Abdullatif Köksal","Timo Schick","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.08460v3.pdf","comment":"EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks"},{"id":"http://arxiv.org/abs/2407.12402v2","updated":"2024-10-03T15:45:52Z","published":"2024-07-17T08:28:55Z","title":"TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish","summary":"  Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.\n","authors":["Arda Yüksel","Abdullatif Köksal","Lütfi Kerem Şenel","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2407.12402v2.pdf","comment":"EMNLP 2024 - Findings"},{"id":"http://arxiv.org/abs/2404.14741v2","updated":"2024-10-03T15:44:59Z","published":"2024-04-23T04:47:22Z","title":"Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering","summary":"  To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.\n","authors":["Yao Xu","Shizhu He","Jiabei Chen","Zihao Wang","Yangqiu Song","Hanghang Tong","Guang Liu","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.14741v2.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.02603v1","updated":"2024-10-03T15:44:42Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01769v2","updated":"2024-10-03T15:30:12Z","published":"2024-10-02T17:25:37Z","title":"Quantifying Generalization Complexity for Large Language Models","summary":"  While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.\n","authors":["Zhenting Qi","Hongyin Luo","Xuliang Huang","Zhuokai Zhao","Yibo Jiang","Xiangjun Fan","Himabindu Lakkaraju","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.01769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02584v1","updated":"2024-10-03T15:28:05Z","published":"2024-10-03T15:28:05Z","title":"Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions","summary":"  As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.\n","authors":["Angana Borah","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2410.02584v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2310.04484v3","updated":"2024-10-03T15:20:17Z","published":"2023-10-06T13:28:04Z","title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning","summary":"  Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions.\n","authors":["Wanyun Cui","Qianle Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11194v2","updated":"2024-10-03T15:13:58Z","published":"2024-06-17T04:00:04Z","title":"In-Context Editing: Learning Knowledge from Self-Induced Distributions","summary":"  In scenarios where language models must incorporate new information\nefficiently without extensive retraining, traditional fine-tuning methods are\nprone to overfitting, degraded generalization, and unnatural language\ngeneration. To address these limitations, we introduce Consistent In-Context\nEditing (ICE), a novel approach leveraging the model's in-context learning\ncapability to optimize toward a contextual distribution rather than a one-hot\ntarget. ICE introduces a simple yet effective optimization framework for the\nmodel to internalize new knowledge by aligning its output distributions with\nand without additional context. This method enhances the robustness and\neffectiveness of gradient-based tuning methods, preventing overfitting and\npreserving the model's integrity. We analyze ICE across four critical aspects\nof knowledge editing: accuracy, locality, generalization, and linguistic\nquality, demonstrating its advantages. Experimental results confirm the\neffectiveness of ICE and demonstrate its potential for continual editing,\nensuring that the integrity of the model is preserved while updating\ninformation.\n","authors":["Siyuan Qi","Bangcheng Yang","Kailin Jiang","Xiaobo Wang","Jiaqi Li","Yifan Zhong","Yaodong Yang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.11194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02560v1","updated":"2024-10-03T15:04:27Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Yakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v1.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.02558v1","updated":"2024-10-03T15:04:00Z","published":"2024-10-03T15:04:00Z","title":"Improving Unsupervised Constituency Parsing via Maximizing Semantic\n  Information","summary":"  Unsupervised constituency parsers organize phrases within a sentence into a\ntree-shaped syntactic constituent structure that reflects the organization of\nsentence semantics. However, the traditional objective of maximizing sentence\nlog-likelihood (LL) does not explicitly account for the close relationship\nbetween the constituent structure and the semantics, resulting in a weak\ncorrelation between LL values and parsing accuracy. In this paper, we introduce\na novel objective for training unsupervised parsers: maximizing the information\nbetween constituent structures and sentence semantics (SemInfo). We introduce a\nbag-of-substrings model to represent the semantics and apply the\nprobability-weighted information metric to estimate the SemInfo. Additionally,\nwe develop a Tree Conditional Random Field (TreeCRF)-based model to apply the\nSemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)\ninduction, the state-of-the-art method for unsupervised constituency parsing.\nExperiments demonstrate that SemInfo correlates more strongly with parsing\naccuracy than LL. Our algorithm significantly enhances parsing accuracy by an\naverage of 7.85 points across five PCFG variants and in four languages,\nachieving new state-of-the-art results in three of the four languages.\n","authors":["Junjie Chen","Xiangheng He","Yusuke Miyao","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2410.02558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12471v2","updated":"2024-10-03T14:56:29Z","published":"2024-06-18T10:20:36Z","title":"Fighting Randomness with Randomness: Mitigating Optimisation Instability\n  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation","summary":"  While fine-tuning of pre-trained language models generally helps to overcome\nthe lack of labelled training samples, it also displays model performance\ninstability. This instability mainly originates from randomness in\ninitialisation or data shuffling. To address this, researchers either modify\nthe training process or augment the available samples, which typically results\nin increased computational costs. We propose a new mitigation strategy, called\nDelayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths\nof ensembling, noise regularisation and model interpolation, while retaining\ncomputational efficiency. We compare DENI with 9 representative mitigation\nstrategies across 3 models, 4 tuning strategies and 7 text classification\ndatasets. We show that: 1) DENI outperforms the best performing mitigation\nstrategy (Ensemble), while using only a fraction of its cost; 2) the mitigation\nstrategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,\noutperforming full fine-tuning in specific cases; and 3) combining DENI with\ndata augmentation often leads to even more effective instability mitigation.\n","authors":["Branislav Pecher","Jan Cegin","Robert Belanec","Jakub Simko","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2406.12471v2.pdf","comment":"Accepted to the Findings of the EMNLP'24 Conference"},{"id":"http://arxiv.org/abs/2402.12817v2","updated":"2024-10-03T14:56:24Z","published":"2024-02-20T08:38:19Z","title":"On Sensitivity of Learning with Limited Labelled Data to the Effects of\n  Randomness: Impact of Interactions and Systematic Choices","summary":"  While learning with limited labelled data can improve performance when the\nlabels are lacking, it is also sensitive to the effects of uncontrolled\nrandomness introduced by so-called randomness factors (e.g., varying order of\ndata). We propose a method to systematically investigate the effects of\nrandomness factors while taking the interactions between them into\nconsideration. To measure the true effects of an individual randomness factor,\nour method mitigates the effects of other factors and observes how the\nperformance varies across multiple runs. Applying our method to multiple\nrandomness factors across in-context learning and fine-tuning approaches on 7\nrepresentative text classification tasks and meta-learning on 3 tasks, we show\nthat: 1) disregarding interactions between randomness factors in existing works\ncaused inconsistent findings due to incorrect attribution of the effects of\nrandomness factors, such as disproving the consistent sensitivity of in-context\nlearning to sample order even with random sample selection; and 2) besides\nmutual interactions, the effects of randomness factors, especially sample\norder, are also dependent on more systematic choices unexplored in existing\nworks, such as number of classes, samples per class or choice of prompt format.\n","authors":["Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2402.12817v2.pdf","comment":"Accepted to the EMNLP'24 Main Conference"},{"id":"http://arxiv.org/abs/2409.19700v2","updated":"2024-10-03T14:56:02Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02551v1","updated":"2024-10-03T14:55:22Z","published":"2024-10-03T14:55:22Z","title":"ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration","summary":"  We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.\n","authors":["Zixiang Wang","Yinghao Zhu","Huiya Zhao","Xiaochen Zheng","Tianlong Wang","Wen Tang","Yasha Wang","Chengwei Pan","Ewen M. Harrison","Junyi Gao","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02458v1","updated":"2024-10-03T14:50:33Z","published":"2024-10-03T14:50:33Z","title":"MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation","summary":"  Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n","authors":["Gurucharan Marthi Krishna Kumar","Aman Chadha","Janine Mendola","Amir Shmuel"],"pdf_url":"https://arxiv.org/pdf/2410.02458v1.pdf","comment":"Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2409.15977v3","updated":"2024-10-03T14:45:55Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://tcsinger.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.18045v3","updated":"2024-10-03T14:44:44Z","published":"2024-02-28T04:43:46Z","title":"Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore","summary":"  Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.\n","authors":["Sheikh Shafayat","Eunsu Kim","Juhyun Oh","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.18045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02538v1","updated":"2024-10-03T14:43:43Z","published":"2024-10-03T14:43:43Z","title":"Algorithms For Automatic Accentuation And Transcription Of Russian Texts\n  In Speech Recognition Systems","summary":"  This paper presents an overview of rule-based system for automatic\naccentuation and phonemic transcription of Russian texts for speech connected\ntasks, such as Automatic Speech Recognition (ASR). Two parts of the developed\nsystem, accentuation and transcription, use different approaches to achieve\ncorrect phonemic representations of input phrases. Accentuation is based on\n\"Grammatical dictionary of the Russian language\" of A.A. Zaliznyak and\nwiktionary corpus. To distinguish homographs, the accentuation system also\nutilises morphological information of the sentences based on Recurrent Neural\nNetworks (RNN). Transcription algorithms apply the rules presented in the\nmonograph of B.M. Lobanov and L.I. Tsirulnik \"Computer Synthesis and Voice\nCloning\". The rules described in the present paper are implemented in an\nopen-source module, which can be of use to any scientific study connected to\nASR or Speech To Text (STT) tasks. Automatically marked up text annotations of\nthe Russian Voxforge database were used as training data for an acoustic model\nin CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,\nmean Word Accuracy being 71.2%. The developed toolkit is written in the Python\nlanguage and is accessible on GitHub for any researcher interested.\n","authors":["Olga Iakovenko","Ivan Bondarenko","Mariya Borovikova","Daniil Vodolazsky"],"pdf_url":"https://arxiv.org/pdf/2410.02538v1.pdf","comment":"Speech and Computer 20th International Conference, SPECOM 2018,\n  Leipzig, Germany, Proceedings 20"},{"id":"http://arxiv.org/abs/2402.17512v3","updated":"2024-10-03T14:41:43Z","published":"2024-02-27T13:54:48Z","title":"Latte: Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.\n","authors":["Rares Dolga","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02525v1","updated":"2024-10-03T14:33:34Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17954v3","updated":"2024-10-03T14:29:11Z","published":"2024-02-28T00:24:29Z","title":"Twists, Humps, and Pebbles: Multilingual Speech Recognition Models\n  Exhibit Gender Performance Gaps","summary":"  Current automatic speech recognition (ASR) models are designed to be used\nacross many languages and tasks without substantial changes. However, this\nbroad language coverage hides performance gaps within languages, for example,\nacross genders. Our study systematically evaluates the performance of two\nwidely used multilingual ASR models on three datasets, encompassing 19\nlanguages from eight language families and two speaking conditions. Our\nfindings reveal clear gender disparities, with the advantaged group varying\nacross languages and models. Surprisingly, those gaps are not explained by\nacoustic or lexical properties. However, probing internal model states reveals\na correlation with gendered performance gap. That is, the easier it is to\ndistinguish speaker gender in a language using probes, the more the gap\nreduces, favoring female speakers. Our results show that gender disparities\npersist even in state-of-the-art models. Our findings have implications for the\nimprovement of multilingual ASR systems, underscoring the importance of\naccessibility to training data and nuanced evaluation to predict and mitigate\ngender gaps. We release all code and artifacts at\nhttps://github.com/g8a9/multilingual-asr-gender-gap.\n","authors":["Giuseppe Attanasio","Beatrice Savoldi","Dennis Fucci","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2402.17954v3.pdf","comment":"Accepted at EMNLP 2024. Code and artifacts at\n  https://github.com/g8a9/multilingual-asr-gender-gap"},{"id":"http://arxiv.org/abs/2410.02521v1","updated":"2024-10-03T14:28:40Z","published":"2024-10-03T14:28:40Z","title":"Methods for Automatic Matrix Language Determination of Code-Switched\n  Speech","summary":"  Code-switching (CS) is the process of speakers interchanging between two or\nmore languages which in the modern world becomes increasingly common. In order\nto better describe CS speech the Matrix Language Frame (MLF) theory introduces\nthe concept of a Matrix Language, which is the language that provides the\ngrammatical structure for a CS utterance. In this work the MLF theory was used\nto develop systems for Matrix Language Identity (MLID) determination. The MLID\nof English/Mandarin and English/Spanish CS text and speech was compared to\nacoustic language identity (LID), which is a typical way to identify a language\nin monolingual utterances. MLID predictors from audio show higher correlation\nwith the textual principles than LID in all cases while also outperforming LID\nin an MLID recognition task based on F1 macro (60\\%) and correlation score\n(0.38). This novel approach has identified that non-English languages (Mandarin\nand Spanish) are preferred over the English language as the ML contrary to the\nmonolingual choice of LID.\n","authors":["Olga Iakovenko","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2410.02521v1.pdf","comment":"Accepted at EMNLP"},{"id":"http://arxiv.org/abs/2309.15656v2","updated":"2024-10-03T14:27:14Z","published":"2023-09-27T13:45:38Z","title":"Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis","summary":"  Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.\n","authors":["Ildikó Pilán","Laurent Prévot","Hendrik Buschmeier","Pierre Lison"],"pdf_url":"https://arxiv.org/pdf/2309.15656v2.pdf","comment":"Updated version for SIGdial 2024"},{"id":"http://arxiv.org/abs/2408.03350v2","updated":"2024-10-03T14:20:40Z","published":"2024-08-05T20:19:18Z","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","summary":"  Real-world formal theorem proving often depends on a wealth of context,\nincluding definitions, lemmas, comments, file structure, and other information.\nWe introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new context that is not seen during\ntraining. miniCTX contains theorems sourced from real Lean projects and\ntextbooks, each associated with a context that can span tens of thousands of\ntokens. Models are tasked with proving a theorem given access to code from the\ntheorem's repository, which contains context that is needed for the proof. As a\nbaseline for miniCTX, we tested fine-tuning and prompting methods that\ncondition theorem proving on preceding context. Both approaches substantially\noutperform traditional methods that rely solely on state information. We found\nthat this ability to use context is not captured by previous benchmarks such as\nminiF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting\nand annotating theorem proving data, making it easy to add new projects into\nminiCTX to ensure that contexts are not seen during training. miniCTX offers a\nchallenging and realistic evaluation of neural theorem provers.\n","authors":["Jiewen Hu","Thomas Zhu","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2408.03350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02507v1","updated":"2024-10-03T14:15:00Z","published":"2024-10-03T14:15:00Z","title":"Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration","summary":"  Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.\n","authors":["Weikang Yuan","Junjie Cao","Zhuoren Jiang","Yangyang Kang","Jun Lin","Kaisong Song","tianqianjin lin","Pengwei Yan","Changlong Sun","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18028v2","updated":"2024-10-03T14:11:23Z","published":"2024-09-26T16:34:35Z","title":"Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective","summary":"  A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.\n","authors":["Yotam Wolf","Binyamin Rothberg","Dorin Shteyman","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2409.18028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02503v1","updated":"2024-10-03T14:06:43Z","published":"2024-10-03T14:06:43Z","title":"Mixed-Session Conversation with Egocentric Memory","summary":"  Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.\n","authors":["Jihyoung Jang","Taeyoung Kim","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02503v1.pdf","comment":"EMNLP Findings 2024 (30 pages); Project website:\n  https://mixed-session.github.io/"},{"id":"http://arxiv.org/abs/2407.03277v2","updated":"2024-10-03T14:05:14Z","published":"2024-07-03T17:04:17Z","title":"Evaluating Automatic Metrics with Incremental Machine Translation\n  Systems","summary":"  We introduce a dataset comprising commercial machine translations, gathered\nweekly over six years across 12 translation directions. Since human A/B testing\nis commonly used, we assume commercial systems improve over time, which enables\nus to evaluate machine translation (MT) metrics based on their preference for\nmore recent translations. Our study not only confirms several prior findings,\nsuch as the advantage of neural metrics over non-neural ones, but also explores\nthe debated issue of how MT quality affects metric reliability--an\ninvestigation that smaller datasets in previous research could not sufficiently\nexplore. Overall, our research demonstrates the dataset's value as a testbed\nfor metric evaluation. We release our code at https://github.com/gjwubyron/Evo\n","authors":["Guojun Wu","Shay B. Cohen","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2407.03277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02499v1","updated":"2024-10-03T14:01:01Z","published":"2024-10-03T14:01:01Z","title":"Defining Knowledge: Bridging Epistemology and Large Language Models","summary":"  Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.\n","authors":["Constanza Fierro","Ruchira Dhar","Filippos Stamatiou","Nicolas Garneau","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2410.02499v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02498v1","updated":"2024-10-03T14:00:44Z","published":"2024-10-03T14:00:44Z","title":"Dynamic Gradient Alignment for Online Data Mixing","summary":"  The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.\n","authors":["Simin Fan","David Grangier","Pierre Ablin"],"pdf_url":"https://arxiv.org/pdf/2410.02498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02492v1","updated":"2024-10-03T13:57:07Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2404.07103v3","updated":"2024-10-03T13:55:08Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v3.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2405.13448v2","updated":"2024-10-03T13:53:59Z","published":"2024-05-22T08:38:26Z","title":"Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning","summary":"  Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines.\n","authors":["Yuanhao Yue","Chengyu Wang","Jun Huang","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13448v2.pdf","comment":"emnlp 2024 findings"},{"id":"http://arxiv.org/abs/2407.04069v2","updated":"2024-10-03T13:51:53Z","published":"2024-07-04T17:15:37Z","title":"A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations","summary":"  Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.\n","authors":["Md Tahmid Rahman Laskar","Sawsan Alqahtani","M Saiful Bari","Mizanur Rahman","Mohammad Abdullah Matin Khan","Haidar Khan","Israt Jahan","Amran Bhuiyan","Chee Wei Tan","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2407.04069v2.pdf","comment":"Accepted at EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2312.02783v3","updated":"2024-10-03T13:47:02Z","published":"2023-12-05T14:14:27Z","title":"Large Language Models on Graphs: A Comprehensive Survey","summary":"  Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.\n","authors":["Bowen Jin","Gang Liu","Chi Han","Meng Jiang","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2312.02783v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2401.16332v4","updated":"2024-10-03T13:40:39Z","published":"2024-01-29T17:38:14Z","title":"Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering","summary":"  Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.\n","authors":["Yotam Wolf","Noam Wies","Dorin Shteyman","Binyamin Rothberg","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2401.16332v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17041v4","updated":"2024-10-03T13:31:39Z","published":"2023-11-28T18:53:06Z","title":"Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties","summary":"  A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.\n","authors":["Keunwoo Peter Yu","Zheyuan Zhang","Fengyuan Hu","Shane Storks","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2311.17041v4.pdf","comment":"16 pages, LaTeX; Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2404.15206v3","updated":"2024-10-03T13:23:59Z","published":"2024-04-23T16:39:03Z","title":"Does Instruction Tuning Make LLMs More Consistent?","summary":"  The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall.\n","authors":["Constanza Fierro","Jiaang Li","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2404.15206v3.pdf","comment":"We need to run extra experiments to ensure some of the claims in the\n  paper are fully correct"},{"id":"http://arxiv.org/abs/2410.02465v1","updated":"2024-10-03T13:15:19Z","published":"2024-10-03T13:15:19Z","title":"Response Tuning: Aligning Large Language Models without Instruction","summary":"  Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.\n","authors":["Seokhyun An","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02465v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2410.01242v2","updated":"2024-10-03T13:12:24Z","published":"2024-10-02T05:07:02Z","title":"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance","summary":"  Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.\n","authors":["Haolin Jin","Zechao Sun","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07431v2","updated":"2024-10-03T13:07:25Z","published":"2024-09-11T17:21:59Z","title":"Synthetic continued pretraining","summary":"  Pretraining on large-scale, unstructured internet text enables language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient--to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining with EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If, instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.\n","authors":["Zitong Yang","Neil Band","Shuangping Li","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2409.07431v2.pdf","comment":"Updated organization of experimental results and methods\n  introduction. Released the dataset and model weights artifact"},{"id":"http://arxiv.org/abs/2406.19999v2","updated":"2024-10-03T13:02:11Z","published":"2024-06-28T15:34:26Z","title":"The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models","summary":"  Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models.\n","authors":["Xinyi Chen","Baohao Liao","Jirui Qi","Panagiotis Eustratiadis","Christof Monz","Arianna Bisazza","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.19999v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02441v1","updated":"2024-10-03T12:39:14Z","published":"2024-10-03T12:39:14Z","title":"Embedded Topic Models Enhanced by Wikification","summary":"  Topic modeling analyzes a collection of documents to learn meaningful\npatterns of words. However, previous topic models consider only the spelling of\nwords and do not take into consideration the homography of words. In this\nstudy, we incorporate the Wikipedia knowledge into a neural topic model to make\nit aware of named entities. We evaluate our method on two datasets, 1) news\narticles of \\textit{New York Times} and 2) the AIDA-CoNLL dataset. Our\nexperiments show that our method improves the performance of neural topic\nmodels in generalizability. Moreover, we analyze frequent terms in each topic\nand the temporal dependencies between topics to demonstrate that our\nentity-aware topic models can capture the time-series development of topics\nwell.\n","authors":["Takashi Shibuya","Takehito Utsuro"],"pdf_url":"https://arxiv.org/pdf/2410.02441v1.pdf","comment":"Accepted at EMNLP 2024 Workshop NLP for Wikipedia"},{"id":"http://arxiv.org/abs/2410.02433v1","updated":"2024-10-03T12:28:13Z","published":"2024-10-03T12:28:13Z","title":"Better Call SAUL: Fluent and Consistent Language Model Editing with\n  Generation Regularization","summary":"  To ensure large language models contain up-to-date knowledge, they need to be\nupdated regularly. However, model editing is challenging as it might also\naffect knowledge that is unrelated to the new data. State-of-the-art methods\nidentify parameters associated with specific knowledge and then modify them via\ndirect weight updates. However, these locate-and-edit methods suffer from heavy\ncomputational overhead and lack theoretical validation. In contrast, directly\nfine-tuning the model on requested edits affects the model's behavior on\nunrelated knowledge, and significantly damages the model's generation fluency\nand consistency. To address these challenges, we propose SAUL, a streamlined\nmodel editing method that uses sentence concatenation with augmented random\nfacts for generation regularization. Evaluations on three model editing\nbenchmarks show that SAUL is a practical and reliable solution for model\nediting outperforming state-of-the-art methods while maintaining generation\nquality and reducing computational overhead.\n","authors":["Mingyang Wang","Lukas Lange","Heike Adel","Jannik Strötgen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2410.02433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02429v1","updated":"2024-10-03T12:24:18Z","published":"2024-10-03T12:24:18Z","title":"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.\n","authors":["Tuo An","Yunjiao Zhou","Han Zou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02429v1.pdf","comment":"21 pages, 10 figures, submitted to ICLR 2025 Conference"},{"id":"http://arxiv.org/abs/2410.02428v1","updated":"2024-10-03T12:21:17Z","published":"2024-10-03T12:21:17Z","title":"Collective Critics for Creative Story Generation","summary":"  Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.\n","authors":["Minwook Bae","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02428v1.pdf","comment":"EMNLP 2024 (36 pages)"},{"id":"http://arxiv.org/abs/2406.13092v2","updated":"2024-10-03T12:20:10Z","published":"2024-06-18T22:44:50Z","title":"Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language\n  Story Understanding","summary":"  Story video-text alignment, a core task in computational story understanding,\naims to align video clips with corresponding sentences in their descriptions.\nHowever, progress on the task has been held back by the scarcity of manually\nannotated video-text correspondence and the heavy concentration on English\nnarrations of Hollywood movies. To address these issues, in this paper, we\nconstruct a large-scale multilingual video story dataset named Multilingual\nSynopses of Movie Narratives (M-SYMON), containing 13,166 movie summary videos\nfrom 7 languages, as well as manual annotation of fine-grained video-text\ncorrespondences for 101.5 hours of video. Training on the human annotated data\nfrom SyMoN outperforms the SOTA methods by 15.7 and 16.2 percentage points on\nClip Accuracy and Sentence IoU scores, respectively, demonstrating the\neffectiveness of the annotations. As benchmarks for future research, we create\n6 baseline approaches with different multilingual training strategies, compare\ntheir performance in both intra-lingual and cross-lingual setups, exemplifying\nthe challenges of multilingual video-text alignment. The dataset is released\nat: https://github.com/insundaycathy/M-SyMoN\n","authors":["Yidan Sun","Jianfei Yu","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2406.13092v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.02426v1","updated":"2024-10-03T12:19:49Z","published":"2024-10-03T12:19:49Z","title":"Learning the Latent Rules of a Game from Data: A Chess Story","summary":"  We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.\n","authors":["Ben Fauber"],"pdf_url":"https://arxiv.org/pdf/2410.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02425v1","updated":"2024-10-03T12:19:06Z","published":"2024-10-03T12:19:06Z","title":"LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services","summary":"  As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.\n","authors":["Małgorzata Łazuka","Andreea Anghel","Thomas Parnell"],"pdf_url":"https://arxiv.org/pdf/2410.02425v1.pdf","comment":"Accepted to the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC '24)"},{"id":"http://arxiv.org/abs/2410.02417v1","updated":"2024-10-03T12:07:34Z","published":"2024-10-03T12:07:34Z","title":"MenakBERT -- Hebrew Diacriticizer","summary":"  Diacritical marks in the Hebrew language give words their vocalized form. The\ntask of adding diacritical marks to plain Hebrew text is still dominated by a\nsystem that relies heavily on human-curated resources. Recent models trained on\ndiacritized Hebrew texts still present a gap in performance. We use a recently\ndeveloped char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a\ncharacter level transformer pretrained on Hebrew text and fine-tuned to produce\ndiacritical marks for Hebrew sentences. We continue to show how finetuning a\nmodel for diacritizing transfers to a task such as part of speech tagging.\n","authors":["Ido Cohen","Jacob Gidron","Idan Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.02417v1.pdf","comment":"Published at ISCOL2022 as a poster"},{"id":"http://arxiv.org/abs/2406.11096v3","updated":"2024-10-03T11:57:00Z","published":"2024-06-16T22:59:18Z","title":"The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models","summary":"  Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs.\n","authors":["Bolei Ma","Xinpeng Wang","Tiancheng Hu","Anna-Carolina Haensch","Michael A. Hedderich","Barbara Plank","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2406.11096v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2212.00596v2","updated":"2024-10-03T11:42:43Z","published":"2022-12-01T15:48:51Z","title":"Language models and brains align due to more than next-word prediction\n  and word-level information","summary":"  Pretrained language models have been shown to significantly predict brain\nrecordings of people comprehending language. Recent work suggests that the\nprediction of the next word is a key mechanism that contributes to this\nalignment. What is not yet understood is whether prediction of the next word is\nnecessary for this observed alignment or simply sufficient, and whether there\nare other shared mechanisms or information that are similarly important. In\nthis work, we take a step towards understanding the reasons for brain alignment\nvia two simple perturbations in popular pretrained language models. These\nperturbations help us design contrasts that can control for different types of\ninformation. By contrasting the brain alignment of these differently perturbed\nmodels, we show that improvements in alignment with brain recordings are due to\nmore than improvements in next-word prediction and word-level information.\n","authors":["Gabriele Merlin","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2212.00596v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02396v1","updated":"2024-10-03T11:17:58Z","published":"2024-10-03T11:17:58Z","title":"Parameter Competition Balancing for Model Merging","summary":"  While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.\n","authors":["Guodong Du","Junlin Lee","Jing Li","Runhua Jiang","Yifei Guo","Shuyang Yu","Hanting Liu","Sim Kuan Goh","Ho-Kin Tang","Daojing He","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02396v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.13560v2","updated":"2024-10-03T11:17:43Z","published":"2024-06-19T13:48:19Z","title":"Lexically Grounded Subword Segmentation","summary":"  We present three innovations in tokenization and subword segmentation. First,\nwe propose to use unsupervised morphological analysis with Morfessor as\npre-tokenization. Second, we present an algebraic method for obtaining subword\nembeddings grounded in a word embedding space. Based on that, we design a novel\nsubword segmentation algorithm that uses the embeddings, ensuring that the\nprocedure considers lexical meaning. Third, we introduce an efficient\nsegmentation algorithm based on a subword bigram model that can be initialized\nwith the lexically aware segmentation method to avoid using Morfessor and large\nembedding tables at inference time. We evaluate the proposed approaches using\ntwo intrinsic metrics and measure their performance on two downstream tasks:\npart-of-speech tagging and machine translation. Our experiments show\nsignificant improvements in the morphological plausibility of the segmentation\nwhen evaluated using segmentation precision on morpheme boundaries and improved\nR\\'enyi efficiency in 8 languages. Although the proposed tokenization methods\ndo not have a large impact on automatic translation quality, we observe\nconsistent performance gains in the arguably more morphological task of\npart-of-speech tagging.\n","authors":["Jindřich Libovický","Jindřich Helcl"],"pdf_url":"https://arxiv.org/pdf/2406.13560v2.pdf","comment":"Camera-ready, EMNLP Main conf"},{"id":"http://arxiv.org/abs/2406.13663v3","updated":"2024-10-03T11:03:22Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v3.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2410.02381v1","updated":"2024-10-03T11:01:25Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.02889v2","updated":"2024-10-03T11:01:14Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v2.pdf","comment":"20 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.02378v1","updated":"2024-10-03T10:51:02Z","published":"2024-10-03T10:51:02Z","title":"Towards Comprehensive Detection of Chinese Harmful Memes","summary":"  This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes\nhave proliferated on the Chinese Internet, while research on detecting Chinese\nharmful memes significantly lags behind due to the absence of reliable datasets\nand effective detectors. To this end, we focus on the comprehensive detection\nof Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful\nmeme dataset, which consists of 12,000 samples with fine-grained annotations\nfor various meme types. Additionally, we propose a baseline detector,\nMultimodal Knowledge Enhancement (MKE), incorporating contextual information of\nmeme content generated by the LLM to enhance the understanding of Chinese\nmemes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of\nMKE. The resources for this paper are available at\nhttps://github.com/DUT-lujunyu/ToxiCN_MM.\n","authors":["Junyu Lu","Bo Xu","Xiaokun Zhang","Hongbo Wang","Haohao Zhu","Dongyu Zhang","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2410.02378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02365v1","updated":"2024-10-03T10:24:24Z","published":"2024-10-03T10:24:24Z","title":"From Concrete to Abstract: A Multimodal Generative Approach to Abstract\n  Concept Learning","summary":"  Understanding and manipulating concrete and abstract concepts is fundamental\nto human intelligence. Yet, they remain challenging for artificial agents. This\npaper introduces a multimodal generative approach to high order abstract\nconcept learning, which integrates visual and categorical linguistic\ninformation from concrete ones. Our model initially grounds subordinate level\nconcrete concepts, combines them to form basic level concepts, and finally\nabstracts to superordinate level concepts via the grounding of basic-level\nconcepts. We evaluate the model language learning ability through\nlanguage-to-visual and visual-to-language tests with high order abstract\nconcepts. Experimental results demonstrate the proficiency of the model in both\nlanguage understanding and language naming tasks.\n","authors":["Haodong Xie","Rahul Singh Maharjan","Federico Tavella","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2410.02365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v1","updated":"2024-10-03T10:06:27Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02343v1","updated":"2024-10-03T09:53:48Z","published":"2024-10-03T09:53:48Z","title":"Listening to the Wise Few: Select-and-Copy Attention Heads for\n  Multiple-Choice QA","summary":"  A standard way to evaluate the abilities of LLM involves presenting a\nmultiple-choice question and selecting the option with the highest logit as the\nmodel's predicted answer. However, such a format for evaluating LLMs has\nlimitations, since even if the model knows the correct answer, it may struggle\nto select the corresponding letter simply due to difficulties in following this\nrigid format. To address this, we introduce new scores that better capture and\nreveal model's underlying knowledge: the Query-Key Score (QK-score), derived\nfrom the interaction between query and key representations in attention heads,\nand the Attention Score, based on attention weights. These scores are extracted\nfrom specific \\textit{select-and-copy} heads, which show consistent performance\nacross popular Multi-Choice Question Answering (MCQA) datasets. Based on these\nscores, our method improves knowledge extraction, yielding up to 16\\% gain for\nLLaMA2-7B and up to 10\\% for larger models on popular MCQA benchmarks. At the\nsame time, the accuracy on a simple synthetic dataset, where the model\nexplicitly knows the right answer, increases by almost 60\\%, achieving nearly\nperfect accuracy, therefore demonstrating the method's efficiency in mitigating\nMCQA format limitations. To support our claims, we conduct experiments on\nmodels ranging from 7 billion to 70 billion parameters in both zero- and\nfew-shot setups.\n","authors":["Eduard Tulchinskii","Laida Kushnareva","Kristian Kuznetsov","Anastasia Voznyuk","Andrei Andriiainen","Irina Piontkovskaya","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2410.02343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02338v1","updated":"2024-10-03T09:48:09Z","published":"2024-10-03T09:48:09Z","title":"How Much Can RAG Help the Reasoning of LLM?","summary":"  Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.\n","authors":["Jingyu Liu","Jiaen Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18369v2","updated":"2024-10-03T09:45:47Z","published":"2024-05-28T17:08:31Z","title":"PromptWizard: Task-Aware Prompt Optimization Framework","summary":"  Large language models (LLMs) have transformed AI across diverse domains, with\nprompting being central to their success in guiding model outputs. However,\nmanual prompt engineering is both labor-intensive and domain-specific,\nnecessitating the need for automated solutions. We introduce PromptWizard, a\nnovel, fully automated framework for discrete prompt optimization, utilizing a\nself-evolving, self-adapting mechanism. Through a feedback-driven critique and\nsynthesis process, PromptWizard achieves an effective balance between\nexploration and exploitation, iteratively refining both prompt instructions and\nin-context examples to generate human-readable, task-specific prompts. This\nguided approach systematically improves prompt quality, resulting in superior\nperformance across 45 tasks. PromptWizard excels even with limited training\ndata, smaller LLMs, and various LLM architectures. Additionally, our cost\nanalysis reveals a substantial reduction in API calls, token usage, and overall\ncost, demonstrating PromptWizard's efficiency, scalability, and advantages over\nexisting prompt optimization strategies.\n","authors":["Eshaan Agarwal","Joykirat Singh","Vivek Dani","Raghav Magazine","Tanuja Ganu","Akshay Nambi"],"pdf_url":"https://arxiv.org/pdf/2405.18369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12319v2","updated":"2024-10-03T09:38:48Z","published":"2024-06-18T06:43:04Z","title":"On the Adversarial Vulnerability of Pairwise Evaluation Using Large\n  Language Models","summary":"  Pairwise evaluation using large language models (LLMs) is widely adopted for\nevaluating generated outputs. However, the reliability of LLM evaluators is\noften compromised by their biased preferences, such as favoring verbosity and\nan authoritative tone. In this work, we find that the evaluation setup itself\ncan significantly amplify these biases, where pairwise evaluators exhibit more\nundesirable tendencies than pointwise evaluators. Our analysis further reveals\nthat even when pairwise evaluators make incorrect judgments, they can still\naccurately identify shortcomings in low-quality outputs. As a simple remedy, we\nalso propose incorporating pointwise reasoning into pairwise evaluation.\nExperimental results show that our method improves the performance of pairwise\nevaluators on adversarial samples across various models. We hope our findings\nencourage further exploration into the reliability of LLM evaluators.\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04459v3","updated":"2024-10-03T09:32:31Z","published":"2024-07-05T12:09:40Z","title":"Generalists vs. Specialists: Evaluating Large Language Models for Urdu","summary":"  In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b,\nwith special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that\nhave been fine-tuned on specific tasks. We focus on seven classification and\nseven generation tasks to evaluate the performance of these models on Urdu\nlanguage. Urdu has 70 million native speakers, yet it remains underrepresented\nin Natural Language Processing (NLP). Despite the frequent advancements in\nLarge Language Models (LLMs), their performance in low-resource languages,\nincluding Urdu, still needs to be explored. We also conduct a human evaluation\nfor the generation tasks and compare the results with the evaluations performed\nby GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose\nmodels consistently outperform general-purpose models across various tasks. We\nalso find that the evaluation done by GPT-4-Turbo for generation tasks aligns\nmore closely with human evaluation compared to the evaluation the evaluation\ndone by Llama-3-8b. This paper contributes to the NLP community by providing\ninsights into the effectiveness of general and specific-purpose LLMs for\nlow-resource languages.\n","authors":["Samee Arif","Abdul Hameed Azeemi","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2407.04459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02330v1","updated":"2024-10-03T09:28:59Z","published":"2024-10-03T09:28:59Z","title":"Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection","summary":"  As a manner to augment pre-trained large language models (LLM), knowledge\ninjection is critical to develop vertical domain large models and has been\nwidely studied. Although most current approaches, including parameter-efficient\nfine-tuning (PEFT) and block expansion methods, uniformly apply knowledge\nacross all LLM layers, it raises the question: are all layers equally crucial\nfor knowledge injection? We begin by evaluating the importance of each layer in\nfinding the optimal layer range for knowledge injection. Intuitively, the more\nimportant layers should play a more critical role in knowledge injection and\ndeserve a denser injection. We observe performance dips in question-answering\nbenchmarks after the removal or expansion of the shallow layers, and the\ndegradation shrinks as the layer gets deeper, indicating that the shallow\nlayers hold the key to knowledge injection. This insight leads us to propose\nthe S strategy, a post-pretraining strategy of selectively enhancing shallow\nlayers while pruning the less effective deep ones. Based on this strategy, we\nintroduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the\ncorpus of code $\\&$ math and demonstrated the effectiveness of our strategy.\nFurther experiments across different LLM, Mistral-7B, and a legal corpus\nconfirmed the general applicability of the approach, underscoring its\nwide-ranging efficacy. Our code is available at:\n\\https://github.com/txchen-USTC/Llama-Slayer\n","authors":["Tianxiang Chen","Zhentao Tan","Tao Gong","Yue Wu","Qi Chu","Bin Liu","Jieping Ye","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16050v2","updated":"2024-10-03T09:24:56Z","published":"2024-02-25T10:27:46Z","title":"Efficient Temporal Extrapolation of Multimodal Large Language Models\n  with Temporal Grounding Bridge","summary":"  Despite progress in multimodal large language models (MLLMs), the challenge\nof interpreting long-form videos in response to linguistic queries persists,\nlargely due to the inefficiency in temporal grounding and limited pre-trained\ncontext window size. In this work, we introduce Temporal Grounding Bridge\n(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding\ncapabilities and broadens their contextual scope. Our framework significantly\nenhances the temporal capabilities of current MLLMs through three key\ninnovations: an efficient multi-span temporal grounding algorithm applied to\nlow-dimension temporal features projected from flow; a multimodal length\nextrapolation training paradigm that utilizes low-dimension temporal features\nto extend the training context window size; and a bootstrapping framework that\nbridges our model with pluggable MLLMs without requiring annotation. We\nvalidate TGB across seven video benchmarks and demonstrate substantial\nperformance improvements compared with prior MLLMs. Notably, our model,\ninitially trained on sequences of four frames, effectively handles sequences up\nto 16 longer without sacrificing performance, highlighting its scalability and\neffectiveness in real-world applications. Our code is publicly available at\nhttps://github.com/bigai-nlco/VideoTGB\n","authors":["Yuxuan Wang","Yueqian Wang","Pengfei Wu","Jianxin Liang","Dongyan Zhao","Yang Liu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.16050v2.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.12924v2","updated":"2024-10-03T09:21:57Z","published":"2024-09-04T03:17:19Z","title":"WaveletGPT: Wavelets Meet Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02320v1","updated":"2024-10-03T08:56:29Z","published":"2024-10-03T08:56:29Z","title":"Post-edits Are Preferences Too","summary":"  Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors \\emph{create} $s_1$ and know that it should be better\nthan $s_2$. We attempt to use these implicit preferences for PO and show that\nit helps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.\n","authors":["Nathaniel Berger","Stefan Riezler","Miriam Exel","Matthias Huck"],"pdf_url":"https://arxiv.org/pdf/2410.02320v1.pdf","comment":"To appear at the Ninth Conference on Machine Translation (WMT24)"},{"id":"http://arxiv.org/abs/2406.02069v3","updated":"2024-10-03T08:46:42Z","published":"2024-06-04T07:51:30Z","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling","summary":"  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.\n","authors":["Zefan Cai","Yichi Zhang","Bofei Gao","Yuliang Liu","Tianyu Liu","Keming Lu","Wayne Xiong","Yue Dong","Baobao Chang","Junjie Hu","Wen Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.02069v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02308v1","updated":"2024-10-03T08:44:17Z","published":"2024-10-03T08:44:17Z","title":"Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large\n  Language Models","summary":"  Phrases are fundamental linguistic units through which humans convey\nsemantics. This study critically examines the capacity of API-based large\nlanguage models (LLMs) to comprehend phrase semantics, utilizing three\nhuman-annotated datasets. We assess the performance of LLMs in executing phrase\nsemantic reasoning tasks guided by natural language instructions and explore\nthe impact of common prompting techniques, including few-shot demonstrations\nand Chain-of-Thought reasoning. Our findings reveal that LLMs greatly\noutperform traditional embedding methods across the datasets; however, they do\nnot show a significant advantage over fine-tuned methods. The effectiveness of\nadvanced prompting strategies shows variability. We conduct detailed error\nanalyses to interpret the limitations faced by LLMs in comprehending phrase\nsemantics. Code and data can be found at\nhttps://github.com/memray/llm_phrase_semantics.\n","authors":["Rui Meng","Ye Liu","Lifu Tu","Daqing He","Yingbo Zhou","Semih Yavuz"],"pdf_url":"https://arxiv.org/pdf/2410.02308v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17233v2","updated":"2024-10-03T08:43:25Z","published":"2024-06-25T02:37:53Z","title":"Self-Constructed Context Decompilation with Fined-grained Alignment\n  Enhancement","summary":"  Decompilation transforms compiled code back into a high-level programming\nlanguage for analysis when source code is unavailable. Previous work has\nprimarily focused on enhancing decompilation performance by increasing the\nscale of model parameters or training data for pre-training. Based on the\ncharacteristics of the decompilation task, we propose two methods: (1) Without\nfine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method\nrecompiles the LLM's decompilation results to construct pairs for in-context\nlearning, helping the model improve decompilation performance. (2) Fine-grained\nAlignment Enhancement (FAE), which meticulously aligns assembly code with\nsource code at the statement level by leveraging debugging information, is\nemployed during the fine-tuning phase to achieve further improvements in\ndecompilation. By integrating these two methods, we achieved a Re-Executability\nperformance improvement of approximately 3.90% on the Decompile-Eval benchmark,\nestablishing a new state-of-the-art performance of 52.41%. The code, data, and\nmodels are available at https://github.com/AlongWY/sccdec.\n","authors":["Yunlong Feng","Dechuan Teng","Yang Xu","Honglin Mu","Xiao Xu","Libo Qin","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2406.17233v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02298v1","updated":"2024-10-03T08:34:17Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02297v1","updated":"2024-10-03T08:27:59Z","published":"2024-10-03T08:27:59Z","title":"Make Compound Sentences Simple to Analyze: Learning to Split Sentences\n  for Aspect-based Sentiment Analysis","summary":"  In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods\nhave shown promising results and achieved substantial advancements. However,\ndespite these advancements, the tasks of extracting sentiment quadruplets,\nwhich capture the nuanced sentiment expressions within a sentence, remain\nsignificant challenges. In particular, compound sentences can potentially\ncontain multiple quadruplets, making the extraction task increasingly difficult\nas sentence complexity grows. To address this issue, we are focusing on\nsimplifying sentence structures to facilitate the easier recognition of these\nelements and crafting a model that integrates seamlessly with various ABSA\ntasks. In this paper, we propose Aspect Term Oriented Sentence Splitter\n(ATOSS), which simplifies compound sentence into simpler and clearer forms,\nthereby clarifying their structure and intent. As a plug-and-play module, this\napproach retains the parameters of the ABSA model while making it easier to\nidentify essential intent within input sentences. Extensive experimental\nresults show that utilizing ATOSS outperforms existing methods in both ASQP and\nACOS tasks, which are the primary tasks for extracting sentiment quadruplets.\n","authors":["Yongsik Seo","Sungwon Song","Ryang Heo","Jieyong Kim","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02297v1.pdf","comment":"Accepted at EMNLP 2024 (Findings, long paper)"},{"id":"http://arxiv.org/abs/2410.02296v1","updated":"2024-10-03T08:27:54Z","published":"2024-10-03T08:27:54Z","title":"Language Models are Graph Learners","summary":"  Language Models (LMs) are increasingly challenging the dominance of\ndomain-specific models, including Graph Neural Networks (GNNs) and Graph\nTransformers (GTs), in graph learning tasks. Following this trend, we propose a\nnovel approach that empowers off-the-shelf LMs to achieve performance\ncomparable to state-of-the-art GNNs on node classification tasks, without\nrequiring any architectural modification. By preserving the LM's original\narchitecture, our approach retains a key benefit of LM instruction tuning: the\nability to jointly train on diverse datasets, fostering greater flexibility and\nefficiency. To achieve this, we introduce two key augmentation strategies: (1)\nEnriching LMs' input using topological and semantic retrieval methods, which\nprovide richer contextual information, and (2) guiding the LMs' classification\nprocess through a lightweight GNN classifier that effectively prunes class\ncandidates. Our experiments on real-world datasets show that backbone Flan-T5\nmodels equipped with these augmentation strategies outperform state-of-the-art\ntext-output node classifiers and are comparable to top-performing vector-output\nnode classifiers. By bridging the gap between specialized task-specific node\nclassifiers and general LMs, this work paves the way for more versatile and\nwidely applicable graph learning models. We will open-source the code upon\npublication.\n","authors":["Zhe Xu","Kaveh Hassani","Si Zhang","Hanqing Zeng","Michihiro Yasunaga","Limei Wang","Dongqi Fu","Ning Yao","Bo Long","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2410.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18430v2","updated":"2024-10-03T08:24:40Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2410.02293v1","updated":"2024-10-03T08:23:06Z","published":"2024-10-03T08:23:06Z","title":"Efficient Second-Order Neural Network Optimization via Adaptive Trust\n  Region Methods","summary":"  Second-order optimization methods offer notable advantages in training deep\nneural networks by utilizing curvature information to achieve faster\nconvergence. However, traditional second-order techniques are computationally\nprohibitive, primarily due to the large matrix inversions and high memory\ndemands they require. While adaptive trust-region methods have been developed\nto mitigate these issues, their performance is often hindered by conservative\nestimates of key parameters, such as the Lipschitz constant of the Hessian,\nresulting in suboptimal outcomes. In this paper, we introduce\nSecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to\novercome these limitations. SOAA approximates the Fisher information matrix\nusing a diagonal representation, reducing computational complexity from\n\\(O(n^{2})\\) to \\(O(n)\\), thereby making it suitable for large-scale deep\nlearning models, including large language models (LLMs). Additionally, the\nalgorithm integrates an adaptive trust-region mechanism that dynamically\nadjusts the trust region size based on observed loss reduction, ensuring both\nrobust convergence and computational efficiency. We empirically demonstrate\nthat SOAA achieves faster and more stable convergence compared to first-order\noptimizers, such as Adam, under similar computational constraints. However, the\ndiagonal approximation of the Fisher information matrix may be less effective\nin capturing higher-order interactions between gradients, suggesting potential\nareas for further refinement and future research.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.02293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02284v1","updated":"2024-10-03T08:07:55Z","published":"2024-10-03T08:07:55Z","title":"Correlation and Navigation in the Vocabulary Key Representation Space of\n  Language Models","summary":"  Language model (LM) decoding is based on the next-token prediction (NTP)\nprobability distribution. For neural LMs (e.g., Transformer-based), NTP\ndistribution is essentially a softmax-regularized dot product between an\nencoded input context (query) and fixed vocabulary representations (keys). In\nthis paper, we study the effect of the key distribution on the NTP\ndistribution, with a focus on whether the similarity between keys will trigger\nspurious correlations in NTP. Through knowledge-probing tasks, we show that in\nthe NTP distribution, the few top-ranked tokens are typically accurate.\nHowever, the middle-ranked prediction is highly biased towards the tokens that\nare distributionally (not necessarily semantically) similar to these top ones.\nFor instance, if \"P\" is predicted as the top-1 token, \"A\"-\"Z\" will all be\nranked high in NTP, no matter whether they can lead to correct decoding\nresults. This hurts the sampling diversity and makes the sampling of correct,\nlong-tail results hopeless and noisy. We attempt to alleviate this issue via a\nnovel in-context method that iteratively pushes the query representation away\nfrom explored regions. Specifically, we include the explored decoding results\nin the context and prompt the LM to generate something else, which encourages\nthe LM to produce a query representation that has small dot products with\nexplored keys. Experiments on knowledge-probing tasks show that our method\nleads to efficient navigation away from explored keys to correct new keys. We\nfurther extend our method to open-ended and chain-of-thought (for reasoning)\ngeneration. Experiment results show that ICN contributes to better generation\ndiversity and improved self-consistency voting performance. Finally, we discuss\npotential training issues caused by the fixed key space together with the\nchallenges and possible ways to address them in future research.\n","authors":["Letian Peng","Chenyang An","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2410.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02283v1","updated":"2024-10-03T08:07:14Z","published":"2024-10-03T08:07:14Z","title":"Morphological evaluation of subwords vocabulary used by BETO language\n  model","summary":"  Subword tokenization algorithms used by Large Language Models are\nsignificantly more efficient and can independently build the necessary\nvocabulary of words and subwords without human intervention. However, those\nsubwords do not always align with real morphemes, potentially impacting the\nmodels' performance, though it remains uncertain when this might occur. In\nprevious research, we proposed a method to assess the morphological quality of\nvocabularies, focusing on the overlap between these vocabularies and the\nmorphemes of a given language. Our evaluation method was built on three quality\nmeasures, relevance, cohesion, and morphological accuracy, and a procedure for\ntheir assessment. By applying this method to vocabularies created by three\nsubword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that\nthese vocabularies generally exhibit very low morphological quality. In this\narticle, we apply this evaluation to the tokenizer of BETO, a BERT language\nmodel trained on large Spanish corpora. This evaluation, along with our\nprevious results, helped us conclude that its vocabulary has a low\nmorphological quality, and we also found that training the tokenizer in a\nlarger corpus does not improve the morphological quality of the generated\nvocabulary. Additionally, this evaluation helps clarify the algorithm used by\nthe tokenizer, that is, Wordpiece, given the inconsistencies between the\nauthors' claims and the model's configuration.\n","authors":["Óscar García-Sierra","Ana Fernández-Pampillón Cesteros","Miguel Ortega-Martín"],"pdf_url":"https://arxiv.org/pdf/2410.02283v1.pdf","comment":"in Spanish language"},{"id":"http://arxiv.org/abs/2406.11341v2","updated":"2024-10-03T08:07:01Z","published":"2024-06-17T08:59:04Z","title":"A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences","summary":"  The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.\n","authors":["Leonardo Bertolazzi","Albert Gatt","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.11341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11016v2","updated":"2024-10-03T08:05:14Z","published":"2024-06-16T17:19:23Z","title":"Optimized Speculative Sampling for GPU Hardware Accelerators","summary":"  In this work, we optimize speculative sampling for parallel hardware\naccelerators to improve sampling speed. We notice that substantial portions of\nthe intermediate matrices necessary for speculative sampling can be computed\nconcurrently. This allows us to distribute the workload across multiple GPU\nthreads, enabling simultaneous operations on matrix segments within thread\nblocks. This results in profiling time improvements ranging from 6% to 13%\nrelative to the baseline implementation, without compromising accuracy. To\nfurther accelerate speculative sampling, probability distributions\nparameterized by softmax are approximated by sigmoid. This approximation\napproach results in significantly greater relative improvements in profiling\ntime, ranging from 37% to 94%, with a minor decline in accuracy. We conduct\nextensive experiments on both automatic speech recognition and summarization\ntasks to validate the effectiveness of our optimization methods.\n","authors":["Dominik Wagner","Seanie Lee","Ilja Baumann","Philipp Seeberger","Korbinian Riedhammer","Tobias Bocklet"],"pdf_url":"https://arxiv.org/pdf/2406.11016v2.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02281v1","updated":"2024-10-03T08:03:40Z","published":"2024-10-03T08:03:40Z","title":"Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition","summary":"  The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.\n","authors":["Arthur Amalvy","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2410.02281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19335v2","updated":"2024-10-03T07:59:36Z","published":"2024-04-30T08:01:49Z","title":"StablePT: Towards Stable Prompting for Few-shot Learning via Input\n  Separation","summary":"  Large language models have shown their ability to become effective few-shot\nlearners with prompting, revolutionizing the paradigm of learning with data\nscarcity. However, this approach largely depends on the quality of prompt\ninitialization, and always exhibits large variability among different runs.\nSuch property makes prompt tuning highly unreliable and vulnerable to poorly\nconstructed prompts, which limits its extension to more real-world\napplications. To tackle this issue, we propose to treat the hard prompt and\nsoft prompt as separate inputs to mitigate noise brought by the prompt\ninitialization. Furthermore, we optimize soft prompts with contrastive learning\nfor utilizing class-aware information in the training process to maintain model\nperformance. Experimental results demonstrate that \\sysname outperforms\nstate-of-the-art methods by 6.97% in accuracy and reduces the standard\ndeviation by 1.92 on average. Furthermore, extensive experiments underscore its\nrobustness and stability across 8 datasets covering various tasks. Codes are\navailable at https://github.com/lccc0528/Stable/tree/main.\n","authors":["Xiaoming Liu","Chen Liu","Zhaohan Zhang","Chengzhengxu Li","Longtian Wang","Yu Lan","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2404.19335v2.pdf","comment":"EMNLP 2024 Findings"}],"Sound":[{"id":"http://arxiv.org/abs/2406.11064v2","updated":"2024-10-03T15:38:04Z","published":"2024-06-16T20:41:03Z","title":"Continual Test-time Adaptation for End-to-end Speech Recognition on\n  Noisy Speech","summary":"  Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made\nsignificant strides but still struggles with performance on out-of-domain\nsamples due to domain shifts in real-world scenarios. Test-Time Adaptation\n(TTA) methods address this issue by adapting models using test samples at\ninference time. However, current ASR TTA methods have largely focused on\nnon-continual TTA, which limits cross-sample knowledge learning compared to\ncontinual TTA. In this work, we first propose a Fast-slow TTA framework for ASR\nthat leverages the advantage of continual and non-continual TTA. Following this\nframework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based\ncontinual TTA method for ASR. To enhance DSUTA robustness for time-varying\ndata, we design a dynamic reset strategy to automatically detect domain shifts\nand reset the model, making it more effective at handling multi-domain data.\nOur method demonstrates superior performance on various noisy ASR datasets,\noutperforming both non-continual and continual TTA baselines while maintaining\nrobustness to domain changes without requiring domain boundary information.\n","authors":["Guan-Ting Lin","Wei-Ping Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.11064v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02560v1","updated":"2024-10-03T15:04:27Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Yakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v1.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2409.15977v3","updated":"2024-10-03T14:45:55Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://tcsinger.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17932v2","updated":"2024-10-03T14:34:52Z","published":"2024-06-25T20:47:10Z","title":"SonicSense: Object Perception from In-Hand Acoustic Vibration","summary":"  We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception.\n","authors":["Jiaxun Liu","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17932v2.pdf","comment":"Our project website is at: http://generalroboticslab.com/SonicSense"},{"id":"http://arxiv.org/abs/2404.19441v3","updated":"2024-10-03T12:23:26Z","published":"2024-04-30T10:44:33Z","title":"ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized\n  Transformers","summary":"  Neural speech codecs aim to compress input signals into minimal bits while\nmaintaining content quality in a low-latency manner. However, existing neural\ncodecs often trade model complexity for reconstruction performance. These\ncodecs primarily use convolutional blocks for feature transformation, which are\nnot inherently suited for capturing the local redundancies in speech signals.\nTo compensate, they require either adversarial discriminators or a large number\nof model parameters to enhance audio quality. In response to these challenges,\nwe introduce the Efficient Speech Codec (ESC), a lightweight,\nparameter-efficient speech codec based on a cross-scale residual vector\nquantization scheme and transformers. Our model employs mirrored hierarchical\nwindow transformer blocks and performs step-wise decoding from coarse-to-fine\nfeature representations. To enhance bitrate efficiency, we propose a novel\ncombination of vector quantization techniques along with a pre-training\nparadigm. Extensive experiments demonstrate that ESC can achieve high-fidelity\nspeech reconstruction with significantly lower model complexity, making it a\npromising alternative to existing convolutional audio codecs.\n","authors":["Yuzhe Gu","Enmao Diao"],"pdf_url":"https://arxiv.org/pdf/2404.19441v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.12924v2","updated":"2024-10-03T09:21:57Z","published":"2024-09-04T03:17:19Z","title":"WaveletGPT: Wavelets Meet Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02271v1","updated":"2024-10-03T07:46:51Z","published":"2024-10-03T07:46:51Z","title":"CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical\n  Temporal Structure Augmentation","summary":"  Modeling temporal characteristics plays a significant role in the\nrepresentation learning of audio waveform. We propose Contrastive Long-form\nLanguage-Audio Pretraining (\\textbf{CoLLAP}) to significantly extend the\nperception window for both the input audio (up to 5 minutes) and the language\ndescriptions (exceeding 250 words), while enabling contrastive learning across\nmodalities and temporal dynamics. Leveraging recent Music-LLMs to generate\nlong-form music captions for full-length songs, augmented with musical temporal\nstructures, we collect 51.3K audio-text pairs derived from the large-scale\nAudioSet training dataset, where the average audio length reaches 288 seconds.\nWe propose a novel contrastive learning architecture that fuses language\nrepresentations with structured audio representations by segmenting each song\ninto clips and extracting their embeddings. With an attention mechanism, we\ncapture multimodal temporal correlations, allowing the model to automatically\nweigh and enhance the final fusion score for improved contrastive alignment.\nFinally, we develop two variants of the CoLLAP model with different types of\nbackbone language models. Through comprehensive experiments on multiple\nlong-form music-text retrieval datasets, we demonstrate consistent performance\nimprovement in retrieval accuracy compared with baselines. We also show the\npretrained CoLLAP models can be transferred to various music information\nretrieval tasks, with heterogeneous long-form multimodal contexts.\n","authors":["Junda Wu","Warren Li","Zachary Novack","Amit Namburi","Carol Chen","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.02271v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.20196v2","updated":"2024-10-03T07:39:20Z","published":"2024-09-30T11:13:35Z","title":"Melody Is All You Need For Music Generation","summary":"  We present the Melody Guided Music Generation (MMGen) model, the first novel\napproach using melody to guide the music generation that, despite a pretty\nsimple method and extremely limited resources, achieves excellent performance.\nSpecifically, we first align the melody with audio waveforms and their\nassociated descriptions using the multimodal alignment module. Subsequently, we\ncondition the diffusion module on the learned melody representations. This\nallows MMGen to generate music that matches the style of the provided audio\nwhile also producing music that reflects the content of the given text\ndescription. To address the scarcity of high-quality data, we construct a\nmulti-modal dataset, MusicSet, which includes melody, text, and audio, and will\nbe made publicly available. We conduct extensive experiments which demonstrate\nthe superiority of the proposed model both in terms of experimental metrics and\nactual performance quality.\n","authors":["Shaopeng Wei","Manzhen Wei","Haoyu Wang","Yu Zhao","Gang Kou"],"pdf_url":"https://arxiv.org/pdf/2409.20196v2.pdf","comment":"9 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2410.02239v1","updated":"2024-10-03T06:24:56Z","published":"2024-10-03T06:24:56Z","title":"A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to\n  Evaluate the Intelligibility of L2 Speech Using a Native Speaker's Shadowings","summary":"  Utterances by L2 speakers can be unintelligible due to mispronunciation and\nimproper prosody. In computer-aided language learning systems, textual feedback\nis often provided using a speech recognition engine. However, an ideal form of\nfeedback for L2 speakers should be so fine-grained that it enables them to\ndetect and diagnose unintelligible parts of L2 speakers' utterances. Inspired\nby language teachers who correct students' pronunciation through a\nvoice-to-voice process, this pilot study utilizes a unique semi-parallel\ndataset composed of non-native speakers' (L2) reading aloud, shadowing of\nnative speakers (L1) and their script-shadowing utterances. We explore the\ntechnical possibility of replicating the process of an L1 speaker's shadowing\nL2 speech using Voice Conversion techniques, to create a virtual shadower\nsystem. Experimental results demonstrate the feasibility of the VC system in\nsimulating L1's shadowing behavior. The output of the virtual shadower system\nshows a reasonable similarity to the real L1 shadowing utterances in both\nlinguistic and acoustic aspects.\n","authors":["Haopeng Geng","Daisuke Saito","Nobuaki Minematsu"],"pdf_url":"https://arxiv.org/pdf/2410.02239v1.pdf","comment":"Accepted by APSIPA ASC 2024. arXiv admin note: text overlap with\n  arXiv:2409.11742"},{"id":"http://arxiv.org/abs/2409.15101v3","updated":"2024-10-03T05:52:18Z","published":"2024-09-23T15:12:05Z","title":"GALD-SE: Guided Anisotropic Lightweight Diffusion for Efficient Speech\n  Enhancement","summary":"  Speech enhancement is designed to enhance the intelligibility and quality of\nspeech across diverse noise conditions. Recently, diffusion model has gained\nlots of attention in speech enhancement area, achieving competitive results.\nCurrent diffusion-based methods blur the signal with isotropic Gaussian noise\nand recover clean speech from the prior. However, these methods often suffer\nfrom a substantial computational burden. We argue that the inefficiency\npartially stems from the oversight that speech enhancement is not purely a\ngenerative task; it primarily involves noise reduction and completion of\nmissing information, while the clean clues in the original mixture do not need\nto be regenerated. In this paper, we propose a method that introduces noise\nwith anisotropic guidance during the diffusion process, allowing the neural\nnetwork to preserve clean clues within noisy recordings. This approach\nsubstantially reduces computational complexity while exhibiting robustness\nagainst various forms of noise interference and speech distortion. Experiments\ndemonstrate that the proposed method achieves state-of-the-art results with\nonly approximately 4.5 million parameters, a number significantly lower than\nthat required by other diffusion methods. This effectively narrows the model\nsize disparity between diffusion-based and predictive speech enhancement\napproaches. Additionally, the proposed method performs well in very noisy\nscenarios, demonstrating its potential for applications in highly challenging\nenvironments.\n","authors":["Chengzhong Wang","Jianjun Gu","Dingding Yao","Zelin Qiu","Junfeng Li","Yonghong Yan"],"pdf_url":"https://arxiv.org/pdf/2409.15101v3.pdf","comment":"We make reassessment and update the author list. All authors have\n  approved this version of the manuscript"},{"id":"http://arxiv.org/abs/2406.12428v2","updated":"2024-10-03T05:17:25Z","published":"2024-06-18T09:23:54Z","title":"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency\n  Spoken Dialogue Systems","summary":"  Multimodal language models that process both text and speech have a potential\nfor applications in spoken dialogue systems. However, current models face two\nmajor challenges in response generation latency: (1) generating a spoken\nresponse requires the prior generation of a written response, and (2) speech\nsequences are significantly longer than text sequences. This study addresses\nthese issues by extending the input and output sequences of the language model\nto support the parallel generation of text and speech. Our experiments on\nspoken question answering tasks demonstrate that our approach improves latency\nwhile maintaining the quality of response content. Additionally, we show that\nlatency can be further reduced by generating speech in multiple sequences. Demo\nsamples are available at https://rinnakk.github.io/research/publications/PSLM.\n","authors":["Kentaro Mitsui","Koh Mitsuda","Toshiaki Wakatsuki","Yukiya Hono","Kei Sawada"],"pdf_url":"https://arxiv.org/pdf/2406.12428v2.pdf","comment":"9 pages, 6 figures, 4 tables, accepted for Findings of EMNLP 2024.\n  Demo samples: https://rinnakk.github.io/research/publications/PSLM"},{"id":"http://arxiv.org/abs/2410.02144v1","updated":"2024-10-03T02:07:59Z","published":"2024-10-03T02:07:59Z","title":"SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model","summary":"  We present SoundMorpher, a sound morphing method that generates perceptually\nuniform morphing trajectories using a diffusion model. Traditional sound\nmorphing methods models the intractable relationship between morph factor and\nperception of the stimuli for resulting sounds under a linear assumption, which\noversimplifies the complex nature of sound perception and limits their morph\nquality. In contrast, SoundMorpher explores an explicit proportional mapping\nbetween the morph factor and the perceptual stimuli of morphed sounds based on\nMel-spectrogram. This approach enables smoother transitions between\nintermediate sounds and ensures perceptually consistent transformations, which\ncan be easily extended to diverse sound morphing tasks. Furthermore, we present\na set of quantitative metrics to comprehensively assess sound morphing systems\nbased on three objective criteria, namely, correspondence, perceptual\nintermediateness, and smoothness. We provide extensive experiments to\ndemonstrate the effectiveness and versatility of SoundMorpher in real-world\nscenarios, highlighting its potential impact on various applications such as\ncreative music composition, film post-production and interactive audio\ntechnologies.\n","authors":["Xinlei Niu","Jing Zhang","Charles Patrick Martin"],"pdf_url":"https://arxiv.org/pdf/2410.02144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02130v1","updated":"2024-10-03T01:23:44Z","published":"2024-10-03T01:23:44Z","title":"MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation","summary":"  We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, MDSGen employs denoising masked diffusion\ntransformers, facilitating efficient generation without reliance on pre-trained\ndiffusion models. Evaluated on the benchmark VGGSound dataset, our smallest\nmodel (5M parameters) achieves 97.9% alignment accuracy, using 172x fewer\nparameters, 371% less memory, and offering 36x faster inference than the\ncurrent 860M-parameter state-of-the-art model (93.9% accuracy). The larger\nmodel (131M parameters) reaches nearly 99% accuracy while requiring 6.5x fewer\nparameters. These results highlight the scalability and effectiveness of our\napproach.\n","authors":["Trung X. Pham","Tri Ton","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.02130v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.08188v3","updated":"2024-10-03T00:22:13Z","published":"2024-09-12T16:26:29Z","title":"Efficient Sparse Coding with the Adaptive Locally Competitive Algorithm\n  for Speech Classification","summary":"  Researchers are exploring novel computational paradigms such as sparse coding\nand neuromorphic computing to bridge the efficiency gap between the human brain\nand conventional computers in complex tasks. A key area of focus is\nneuromorphic audio processing. While the Locally Competitive Algorithm has\nemerged as a promising solution for sparse coding, offering potential for\nreal-time and low-power processing on neuromorphic hardware, its applications\nin neuromorphic speech classification have not been thoroughly studied. The\nAdaptive Locally Competitive Algorithm builds upon the Locally Competitive\nAlgorithm by dynamically adjusting the modulation parameters of the filter bank\nto fine-tune the filters' sensitivity. This adaptability enhances lateral\ninhibition, improving reconstruction quality, sparsity, and convergence time,\nwhich is crucial for real-time applications. This paper demonstrates the\npotential of the Locally Competitive Algorithm and its adaptive variant as\nrobust feature extractors for neuromorphic speech classification. Results show\nthat the Locally Competitive Algorithm achieves better speech classification\naccuracy at the expense of higher power consumption compared to the LAUSCHER\ncochlea model used for benchmarking. On the other hand, the Adaptive Locally\nCompetitive Algorithm mitigates this power consumption issue without\ncompromising the accuracy. The dynamic power consumption is reduced to a range\nof 4 to 13 milliwatts on neuromorphic hardware, three orders of magnitude less\nthan setups using Graphics Processing Units. These findings position the\nAdaptive Locally Competitive Algorithm as a compelling solution for efficient\nspeech classification systems, promising substantial advancements in balancing\nspeech classification accuracy and power efficiency.\n","authors":["Soufiyan Bahadi","Eric Plourde","Jean Rouat"],"pdf_url":"https://arxiv.org/pdf/2409.08188v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2410.03037v1","updated":"2024-10-03T22:48:04Z","published":"2024-10-03T22:48:04Z","title":"Disentangling Textual and Acoustic Features of Neural Speech\n  Representations","summary":"  Neural speech models build deeply entangled internal representations, which\ncapture a variety of features (e.g., fundamental frequency, loudness, syntactic\ncategory, or semantic content of a word) in a distributed encoding. This\ncomplexity makes it difficult to track the extent to which such representations\nrely on textual and acoustic information, or to suppress the encoding of\nacoustic features that may pose privacy risks (e.g., gender or speaker\nidentity) in critical, real-world applications. In this paper, we build upon\nthe Information Bottleneck principle to propose a disentanglement framework\nthat separates complex speech representations into two distinct components: one\nencoding content (i.e., what can be transcribed as text) and the other encoding\nacoustic features relevant to a given downstream task. We apply and evaluate\nour framework to emotion recognition and speaker identification downstream\ntasks, quantifying the contribution of textual and acoustic features at each\nmodel layer. Additionally, we explore the application of our disentanglement\nframework as an attribution method to identify the most salient speech frame\nrepresentations from both the textual and acoustic perspectives.\n","authors":["Hosein Mohebbi","Grzegorz Chrupała","Willem Zuidema","Afra Alishahi","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2410.03037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06592v2","updated":"2024-10-03T22:05:05Z","published":"2024-02-09T18:12:11Z","title":"Self-consistent context aware conformer transducer for speech\n  recognition","summary":"  We introduce a novel neural network module that adeptly handles recursive\ndata flow in neural network architectures. At its core, this module employs a\nself-consistent approach where a set of recursive equations is solved\niteratively, halting when the difference between two consecutive iterations\nfalls below a defined threshold. Leveraging this mechanism, we construct a new\nneural network architecture, an extension of the conformer transducer, which\nenriches automatic speech recognition systems with a stream of contextual\ninformation. Our method notably improves the accuracy of recognizing rare words\nwithout adversely affecting the word error rate for common vocabulary. We\ninvestigate the improvement in accuracy for these uncommon words using our\nnovel model, both independently and in conjunction with shallow fusion with a\ncontext language model. Our findings reveal that the combination of both\napproaches can improve the accuracy of detecting rare words by as much as 4.5\ntimes. Our proposed self-consistent recursive methodology is versatile and\nadaptable, compatible with many recently developed encoders, and has the\npotential to drive model improvements in speech recognition and beyond.\n","authors":["Konstantin Kolokolov","Pavel Pekichev","Karthik Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2402.06592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16221v3","updated":"2024-10-03T21:37:49Z","published":"2024-08-29T02:35:53Z","title":"SSDM: Scalable Speech Dysfluency Modeling","summary":"  Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions\\cite{lian2023unconstrained-udm,\nlian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second,\nthere is a lack of a large-scale dysfluency corpus. Third, there is not an\neffective learning framework. In this paper, we propose \\textit{SSDM: Scalable\nSpeech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable\nforced alignment; (2) introduces connectionist subsequence aligner (CSA) to\nachieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency\ncorpus called Libri-Dys; and (4) develops an end-to-end system by leveraging\nthe power of large language models (LLMs). We expect SSDM to serve as a\nstandard in the area of dysfluency modeling. Demo is available at\n\\url{https://berkeley-speech-group.github.io/SSDM/}.\n","authors":["Jiachen Lian","Xuanru Zhou","Zoe Ezzes","Jet Vonk","Brittany Morin","David Baquirin","Zachary Mille","Maria Luisa Gorno Tempini","Gopala Krishna Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.16221v3.pdf","comment":"2024 NeurIPS"},{"id":"http://arxiv.org/abs/2410.03791v1","updated":"2024-10-03T21:26:58Z","published":"2024-10-03T21:26:58Z","title":"People are poorly equipped to detect AI-powered voice clones","summary":"  As generative AI continues its ballistic trajectory, everything from text to\naudio, image, and video generation continues to improve in mimicking\nhuman-generated content. Through a series of perceptual studies, we report on\nthe realism of AI-generated voices in terms of identity matching and\nnaturalness. We find human participants cannot reliably identify short\nrecordings (less than 20 seconds) of AI-generated voices. Specifically,\nparticipants mistook the identity of an AI-voice for its real counterpart 80%\nof the time, and correctly identified a voice as AI-generated only 60% of the\ntime. In all cases, performance is independent of the demographics of the\nspeaker or listener.\n","authors":["Sarah Barrington","Hany Farid"],"pdf_url":"https://arxiv.org/pdf/2410.03791v1.pdf","comment":null}],"Speech Processing":[{"id":"http://arxiv.org/abs/2406.11064v2","updated":"2024-10-03T15:38:04Z","published":"2024-06-16T20:41:03Z","title":"Continual Test-time Adaptation for End-to-end Speech Recognition on\n  Noisy Speech","summary":"  Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made\nsignificant strides but still struggles with performance on out-of-domain\nsamples due to domain shifts in real-world scenarios. Test-Time Adaptation\n(TTA) methods address this issue by adapting models using test samples at\ninference time. However, current ASR TTA methods have largely focused on\nnon-continual TTA, which limits cross-sample knowledge learning compared to\ncontinual TTA. In this work, we first propose a Fast-slow TTA framework for ASR\nthat leverages the advantage of continual and non-continual TTA. Following this\nframework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based\ncontinual TTA method for ASR. To enhance DSUTA robustness for time-varying\ndata, we design a dynamic reset strategy to automatically detect domain shifts\nand reset the model, making it more effective at handling multi-domain data.\nOur method demonstrates superior performance on various noisy ASR datasets,\noutperforming both non-continual and continual TTA baselines while maintaining\nrobustness to domain changes without requiring domain boundary information.\n","authors":["Guan-Ting Lin","Wei-Ping Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.11064v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02560v1","updated":"2024-10-03T15:04:27Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Yakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v1.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2409.15977v3","updated":"2024-10-03T14:45:55Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://tcsinger.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17932v2","updated":"2024-10-03T14:34:52Z","published":"2024-06-25T20:47:10Z","title":"SonicSense: Object Perception from In-Hand Acoustic Vibration","summary":"  We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception.\n","authors":["Jiaxun Liu","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17932v2.pdf","comment":"Our project website is at: http://generalroboticslab.com/SonicSense"},{"id":"http://arxiv.org/abs/2404.19441v3","updated":"2024-10-03T12:23:26Z","published":"2024-04-30T10:44:33Z","title":"ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized\n  Transformers","summary":"  Neural speech codecs aim to compress input signals into minimal bits while\nmaintaining content quality in a low-latency manner. However, existing neural\ncodecs often trade model complexity for reconstruction performance. These\ncodecs primarily use convolutional blocks for feature transformation, which are\nnot inherently suited for capturing the local redundancies in speech signals.\nTo compensate, they require either adversarial discriminators or a large number\nof model parameters to enhance audio quality. In response to these challenges,\nwe introduce the Efficient Speech Codec (ESC), a lightweight,\nparameter-efficient speech codec based on a cross-scale residual vector\nquantization scheme and transformers. Our model employs mirrored hierarchical\nwindow transformer blocks and performs step-wise decoding from coarse-to-fine\nfeature representations. To enhance bitrate efficiency, we propose a novel\ncombination of vector quantization techniques along with a pre-training\nparadigm. Extensive experiments demonstrate that ESC can achieve high-fidelity\nspeech reconstruction with significantly lower model complexity, making it a\npromising alternative to existing convolutional audio codecs.\n","authors":["Yuzhe Gu","Enmao Diao"],"pdf_url":"https://arxiv.org/pdf/2404.19441v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02371v1","updated":"2024-10-03T10:45:10Z","published":"2024-10-03T10:45:10Z","title":"NTU-NPU System for Voice Privacy 2024 Challenge","summary":"  In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.\n","authors":["Nikita Kuzmin","Hieu-Thi Luong","Jixun Yao","Lei Xie","Kong Aik Lee","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2410.02371v1.pdf","comment":"System description for VPC 2024"},{"id":"http://arxiv.org/abs/2410.02364v1","updated":"2024-10-03T10:23:39Z","published":"2024-10-03T10:23:39Z","title":"State-of-the-art Embeddings with Video-free Segmentation of the Source\n  VoxCeleb Data","summary":"  In this paper, we refine and validate our method for training speaker\nembedding extractors using weak annotations. More specifically, we use only the\naudio stream of the source VoxCeleb videos and the names of the celebrities\nwithout knowing the time intervals in which they appear in the recording. We\nexperiment with hyperparameters and embedding extractors based on ResNet and\nWavLM. We show that the method achieves state-of-the-art results in speaker\nverification, comparable with training the extractors in a standard supervised\nway on the VoxCeleb dataset. We also extend it by considering segments\nbelonging to unknown speakers appearing alongside the celebrities, which are\ntypically being discarded. Overall, our approach can be used for directly\ntraining state-of-the-art embedding extractors or as an alternative to the\nVoxCeleb-like pipeline for dataset creation without needing image modality.\n","authors":["Sara Barahona","Ladislav Mošner","Themos Stafylakis","Oldřich Plchot","Junyi Peng","Lukáš Burget","Jan Černocký"],"pdf_url":"https://arxiv.org/pdf/2410.02364v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2409.12924v2","updated":"2024-10-03T09:21:57Z","published":"2024-09-04T03:17:19Z","title":"WaveletGPT: Wavelets Meet Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02271v1","updated":"2024-10-03T07:46:51Z","published":"2024-10-03T07:46:51Z","title":"CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical\n  Temporal Structure Augmentation","summary":"  Modeling temporal characteristics plays a significant role in the\nrepresentation learning of audio waveform. We propose Contrastive Long-form\nLanguage-Audio Pretraining (\\textbf{CoLLAP}) to significantly extend the\nperception window for both the input audio (up to 5 minutes) and the language\ndescriptions (exceeding 250 words), while enabling contrastive learning across\nmodalities and temporal dynamics. Leveraging recent Music-LLMs to generate\nlong-form music captions for full-length songs, augmented with musical temporal\nstructures, we collect 51.3K audio-text pairs derived from the large-scale\nAudioSet training dataset, where the average audio length reaches 288 seconds.\nWe propose a novel contrastive learning architecture that fuses language\nrepresentations with structured audio representations by segmenting each song\ninto clips and extracting their embeddings. With an attention mechanism, we\ncapture multimodal temporal correlations, allowing the model to automatically\nweigh and enhance the final fusion score for improved contrastive alignment.\nFinally, we develop two variants of the CoLLAP model with different types of\nbackbone language models. Through comprehensive experiments on multiple\nlong-form music-text retrieval datasets, we demonstrate consistent performance\nimprovement in retrieval accuracy compared with baselines. We also show the\npretrained CoLLAP models can be transferred to various music information\nretrieval tasks, with heterogeneous long-form multimodal contexts.\n","authors":["Junda Wu","Warren Li","Zachary Novack","Amit Namburi","Carol Chen","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.02271v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.20196v2","updated":"2024-10-03T07:39:20Z","published":"2024-09-30T11:13:35Z","title":"Melody Is All You Need For Music Generation","summary":"  We present the Melody Guided Music Generation (MMGen) model, the first novel\napproach using melody to guide the music generation that, despite a pretty\nsimple method and extremely limited resources, achieves excellent performance.\nSpecifically, we first align the melody with audio waveforms and their\nassociated descriptions using the multimodal alignment module. Subsequently, we\ncondition the diffusion module on the learned melody representations. This\nallows MMGen to generate music that matches the style of the provided audio\nwhile also producing music that reflects the content of the given text\ndescription. To address the scarcity of high-quality data, we construct a\nmulti-modal dataset, MusicSet, which includes melody, text, and audio, and will\nbe made publicly available. We conduct extensive experiments which demonstrate\nthe superiority of the proposed model both in terms of experimental metrics and\nactual performance quality.\n","authors":["Shaopeng Wei","Manzhen Wei","Haoyu Wang","Yu Zhao","Gang Kou"],"pdf_url":"https://arxiv.org/pdf/2409.20196v2.pdf","comment":"9 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2410.02239v1","updated":"2024-10-03T06:24:56Z","published":"2024-10-03T06:24:56Z","title":"A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to\n  Evaluate the Intelligibility of L2 Speech Using a Native Speaker's Shadowings","summary":"  Utterances by L2 speakers can be unintelligible due to mispronunciation and\nimproper prosody. In computer-aided language learning systems, textual feedback\nis often provided using a speech recognition engine. However, an ideal form of\nfeedback for L2 speakers should be so fine-grained that it enables them to\ndetect and diagnose unintelligible parts of L2 speakers' utterances. Inspired\nby language teachers who correct students' pronunciation through a\nvoice-to-voice process, this pilot study utilizes a unique semi-parallel\ndataset composed of non-native speakers' (L2) reading aloud, shadowing of\nnative speakers (L1) and their script-shadowing utterances. We explore the\ntechnical possibility of replicating the process of an L1 speaker's shadowing\nL2 speech using Voice Conversion techniques, to create a virtual shadower\nsystem. Experimental results demonstrate the feasibility of the VC system in\nsimulating L1's shadowing behavior. The output of the virtual shadower system\nshows a reasonable similarity to the real L1 shadowing utterances in both\nlinguistic and acoustic aspects.\n","authors":["Haopeng Geng","Daisuke Saito","Nobuaki Minematsu"],"pdf_url":"https://arxiv.org/pdf/2410.02239v1.pdf","comment":"Accepted by APSIPA ASC 2024. arXiv admin note: text overlap with\n  arXiv:2409.11742"},{"id":"http://arxiv.org/abs/2409.15101v3","updated":"2024-10-03T05:52:18Z","published":"2024-09-23T15:12:05Z","title":"GALD-SE: Guided Anisotropic Lightweight Diffusion for Efficient Speech\n  Enhancement","summary":"  Speech enhancement is designed to enhance the intelligibility and quality of\nspeech across diverse noise conditions. Recently, diffusion model has gained\nlots of attention in speech enhancement area, achieving competitive results.\nCurrent diffusion-based methods blur the signal with isotropic Gaussian noise\nand recover clean speech from the prior. However, these methods often suffer\nfrom a substantial computational burden. We argue that the inefficiency\npartially stems from the oversight that speech enhancement is not purely a\ngenerative task; it primarily involves noise reduction and completion of\nmissing information, while the clean clues in the original mixture do not need\nto be regenerated. In this paper, we propose a method that introduces noise\nwith anisotropic guidance during the diffusion process, allowing the neural\nnetwork to preserve clean clues within noisy recordings. This approach\nsubstantially reduces computational complexity while exhibiting robustness\nagainst various forms of noise interference and speech distortion. Experiments\ndemonstrate that the proposed method achieves state-of-the-art results with\nonly approximately 4.5 million parameters, a number significantly lower than\nthat required by other diffusion methods. This effectively narrows the model\nsize disparity between diffusion-based and predictive speech enhancement\napproaches. Additionally, the proposed method performs well in very noisy\nscenarios, demonstrating its potential for applications in highly challenging\nenvironments.\n","authors":["Chengzhong Wang","Jianjun Gu","Dingding Yao","Zelin Qiu","Junfeng Li","Yonghong Yan"],"pdf_url":"https://arxiv.org/pdf/2409.15101v3.pdf","comment":"We make reassessment and update the author list. All authors have\n  approved this version of the manuscript"},{"id":"http://arxiv.org/abs/2406.12428v2","updated":"2024-10-03T05:17:25Z","published":"2024-06-18T09:23:54Z","title":"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency\n  Spoken Dialogue Systems","summary":"  Multimodal language models that process both text and speech have a potential\nfor applications in spoken dialogue systems. However, current models face two\nmajor challenges in response generation latency: (1) generating a spoken\nresponse requires the prior generation of a written response, and (2) speech\nsequences are significantly longer than text sequences. This study addresses\nthese issues by extending the input and output sequences of the language model\nto support the parallel generation of text and speech. Our experiments on\nspoken question answering tasks demonstrate that our approach improves latency\nwhile maintaining the quality of response content. Additionally, we show that\nlatency can be further reduced by generating speech in multiple sequences. Demo\nsamples are available at https://rinnakk.github.io/research/publications/PSLM.\n","authors":["Kentaro Mitsui","Koh Mitsuda","Toshiaki Wakatsuki","Yukiya Hono","Kei Sawada"],"pdf_url":"https://arxiv.org/pdf/2406.12428v2.pdf","comment":"9 pages, 6 figures, 4 tables, accepted for Findings of EMNLP 2024.\n  Demo samples: https://rinnakk.github.io/research/publications/PSLM"},{"id":"http://arxiv.org/abs/2410.00527v2","updated":"2024-10-03T03:52:35Z","published":"2024-10-01T09:11:03Z","title":"Wanna Hear Your Voice: Adaptive, Effective, and Language-Agnostic\n  Approach in Voice Extraction","summary":"  The research on audio clue-based target speaker extraction (TSE) has mostly\nfocused on modeling the mixture and reference speech, achieving high\nperformance in English due to the availability of large datasets. However, less\nattention has been given to the consistent properties of human speech across\nlanguages. To bridge this gap, we introduce WHYV (Wanna Hear Your Voice), which\naddresses the challenge of transferring TSE models from one language to another\nwithout fine-tuning. In this work, we proposed a gating mechanism that be able\nto modify specific frequencies based on the speaker's acoustic features. The\nmodel achieves an SI-SDR of 17.3544 on clean English speech and 13.2032 on\nclean speech mixed with Wham! noise, outperforming all other models in its\nability to adapt to different languages.\n","authors":["The Hieu Pham","Phuong Thanh Tran Nguyen","Xuan Tho Nguyen","Tan Dat Nguyen","Duc Dung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.00527v2.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.02144v1","updated":"2024-10-03T02:07:59Z","published":"2024-10-03T02:07:59Z","title":"SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model","summary":"  We present SoundMorpher, a sound morphing method that generates perceptually\nuniform morphing trajectories using a diffusion model. Traditional sound\nmorphing methods models the intractable relationship between morph factor and\nperception of the stimuli for resulting sounds under a linear assumption, which\noversimplifies the complex nature of sound perception and limits their morph\nquality. In contrast, SoundMorpher explores an explicit proportional mapping\nbetween the morph factor and the perceptual stimuli of morphed sounds based on\nMel-spectrogram. This approach enables smoother transitions between\nintermediate sounds and ensures perceptually consistent transformations, which\ncan be easily extended to diverse sound morphing tasks. Furthermore, we present\na set of quantitative metrics to comprehensively assess sound morphing systems\nbased on three objective criteria, namely, correspondence, perceptual\nintermediateness, and smoothness. We provide extensive experiments to\ndemonstrate the effectiveness and versatility of SoundMorpher in real-world\nscenarios, highlighting its potential impact on various applications such as\ncreative music composition, film post-production and interactive audio\ntechnologies.\n","authors":["Xinlei Niu","Jing Zhang","Charles Patrick Martin"],"pdf_url":"https://arxiv.org/pdf/2410.02144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02130v1","updated":"2024-10-03T01:23:44Z","published":"2024-10-03T01:23:44Z","title":"MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation","summary":"  We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, MDSGen employs denoising masked diffusion\ntransformers, facilitating efficient generation without reliance on pre-trained\ndiffusion models. Evaluated on the benchmark VGGSound dataset, our smallest\nmodel (5M parameters) achieves 97.9% alignment accuracy, using 172x fewer\nparameters, 371% less memory, and offering 36x faster inference than the\ncurrent 860M-parameter state-of-the-art model (93.9% accuracy). The larger\nmodel (131M parameters) reaches nearly 99% accuracy while requiring 6.5x fewer\nparameters. These results highlight the scalability and effectiveness of our\napproach.\n","authors":["Trung X. Pham","Tri Ton","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.02130v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.08188v3","updated":"2024-10-03T00:22:13Z","published":"2024-09-12T16:26:29Z","title":"Efficient Sparse Coding with the Adaptive Locally Competitive Algorithm\n  for Speech Classification","summary":"  Researchers are exploring novel computational paradigms such as sparse coding\nand neuromorphic computing to bridge the efficiency gap between the human brain\nand conventional computers in complex tasks. A key area of focus is\nneuromorphic audio processing. While the Locally Competitive Algorithm has\nemerged as a promising solution for sparse coding, offering potential for\nreal-time and low-power processing on neuromorphic hardware, its applications\nin neuromorphic speech classification have not been thoroughly studied. The\nAdaptive Locally Competitive Algorithm builds upon the Locally Competitive\nAlgorithm by dynamically adjusting the modulation parameters of the filter bank\nto fine-tune the filters' sensitivity. This adaptability enhances lateral\ninhibition, improving reconstruction quality, sparsity, and convergence time,\nwhich is crucial for real-time applications. This paper demonstrates the\npotential of the Locally Competitive Algorithm and its adaptive variant as\nrobust feature extractors for neuromorphic speech classification. Results show\nthat the Locally Competitive Algorithm achieves better speech classification\naccuracy at the expense of higher power consumption compared to the LAUSCHER\ncochlea model used for benchmarking. On the other hand, the Adaptive Locally\nCompetitive Algorithm mitigates this power consumption issue without\ncompromising the accuracy. The dynamic power consumption is reduced to a range\nof 4 to 13 milliwatts on neuromorphic hardware, three orders of magnitude less\nthan setups using Graphics Processing Units. These findings position the\nAdaptive Locally Competitive Algorithm as a compelling solution for efficient\nspeech classification systems, promising substantial advancements in balancing\nspeech classification accuracy and power efficiency.\n","authors":["Soufiyan Bahadi","Eric Plourde","Jean Rouat"],"pdf_url":"https://arxiv.org/pdf/2409.08188v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2410.03037v1","updated":"2024-10-03T22:48:04Z","published":"2024-10-03T22:48:04Z","title":"Disentangling Textual and Acoustic Features of Neural Speech\n  Representations","summary":"  Neural speech models build deeply entangled internal representations, which\ncapture a variety of features (e.g., fundamental frequency, loudness, syntactic\ncategory, or semantic content of a word) in a distributed encoding. This\ncomplexity makes it difficult to track the extent to which such representations\nrely on textual and acoustic information, or to suppress the encoding of\nacoustic features that may pose privacy risks (e.g., gender or speaker\nidentity) in critical, real-world applications. In this paper, we build upon\nthe Information Bottleneck principle to propose a disentanglement framework\nthat separates complex speech representations into two distinct components: one\nencoding content (i.e., what can be transcribed as text) and the other encoding\nacoustic features relevant to a given downstream task. We apply and evaluate\nour framework to emotion recognition and speaker identification downstream\ntasks, quantifying the contribution of textual and acoustic features at each\nmodel layer. Additionally, we explore the application of our disentanglement\nframework as an attribution method to identify the most salient speech frame\nrepresentations from both the textual and acoustic perspectives.\n","authors":["Hosein Mohebbi","Grzegorz Chrupała","Willem Zuidema","Afra Alishahi","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2410.03037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06592v2","updated":"2024-10-03T22:05:05Z","published":"2024-02-09T18:12:11Z","title":"Self-consistent context aware conformer transducer for speech\n  recognition","summary":"  We introduce a novel neural network module that adeptly handles recursive\ndata flow in neural network architectures. At its core, this module employs a\nself-consistent approach where a set of recursive equations is solved\niteratively, halting when the difference between two consecutive iterations\nfalls below a defined threshold. Leveraging this mechanism, we construct a new\nneural network architecture, an extension of the conformer transducer, which\nenriches automatic speech recognition systems with a stream of contextual\ninformation. Our method notably improves the accuracy of recognizing rare words\nwithout adversely affecting the word error rate for common vocabulary. We\ninvestigate the improvement in accuracy for these uncommon words using our\nnovel model, both independently and in conjunction with shallow fusion with a\ncontext language model. Our findings reveal that the combination of both\napproaches can improve the accuracy of detecting rare words by as much as 4.5\ntimes. Our proposed self-consistent recursive methodology is versatile and\nadaptable, compatible with many recently developed encoders, and has the\npotential to drive model improvements in speech recognition and beyond.\n","authors":["Konstantin Kolokolov","Pavel Pekichev","Karthik Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2402.06592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16221v3","updated":"2024-10-03T21:37:49Z","published":"2024-08-29T02:35:53Z","title":"SSDM: Scalable Speech Dysfluency Modeling","summary":"  Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions\\cite{lian2023unconstrained-udm,\nlian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second,\nthere is a lack of a large-scale dysfluency corpus. Third, there is not an\neffective learning framework. In this paper, we propose \\textit{SSDM: Scalable\nSpeech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable\nforced alignment; (2) introduces connectionist subsequence aligner (CSA) to\nachieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency\ncorpus called Libri-Dys; and (4) develops an end-to-end system by leveraging\nthe power of large language models (LLMs). We expect SSDM to serve as a\nstandard in the area of dysfluency modeling. Demo is available at\n\\url{https://berkeley-speech-group.github.io/SSDM/}.\n","authors":["Jiachen Lian","Xuanru Zhou","Zoe Ezzes","Jet Vonk","Brittany Morin","David Baquirin","Zachary Mille","Maria Luisa Gorno Tempini","Gopala Krishna Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.16221v3.pdf","comment":"2024 NeurIPS"},{"id":"http://arxiv.org/abs/2410.03007v1","updated":"2024-10-03T21:33:07Z","published":"2024-10-03T21:33:07Z","title":"FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech\n  Language Model","summary":"  In this study, we aim to explore Multitask Speech Language Model (SpeechLM)\nefficient inference via token reduction. Unlike other modalities such as vision\nor text, speech has unique temporal dependencies, making previous efficient\ninference works on other modalities not directly applicable. Furthermore,\nmethods for efficient SpeechLM inference on long sequence and sparse signals\nremain largely unexplored. Then we propose FastAdaSP, a weighted token merging\nframework specifically designed for various speech-related tasks to improve the\ntrade-off between efficiency and performance. Experimental results on WavLLM\nand Qwen-Audio show that our method achieves the state-of-the-art (SOTA)\nefficiency-performance trade-off compared with other baseline methods.\nSpecifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding\nthroughput without any degradation on tasks like Emotion Recognition (ER) and\nSpoken Question Answering (SQA). The code will be available at\nhttps://github.com/yichen14/FastAdaSP\n","authors":["Yichen Lu","Jiaqi Song","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.03007v1.pdf","comment":"EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.03791v1","updated":"2024-10-03T21:26:58Z","published":"2024-10-03T21:26:58Z","title":"People are poorly equipped to detect AI-powered voice clones","summary":"  As generative AI continues its ballistic trajectory, everything from text to\naudio, image, and video generation continues to improve in mimicking\nhuman-generated content. Through a series of perceptual studies, we report on\nthe realism of AI-generated voices in terms of identity matching and\nnaturalness. We find human participants cannot reliably identify short\nrecordings (less than 20 seconds) of AI-generated voices. Specifically,\nparticipants mistook the identity of an AI-voice for its real counterpart 80%\nof the time, and correctly identified a voice as AI-generated only 60% of the\ntime. In all cases, performance is independent of the demographics of the\nspeaker or listener.\n","authors":["Sarah Barrington","Hany Farid"],"pdf_url":"https://arxiv.org/pdf/2410.03791v1.pdf","comment":null}]},"2024-10-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.03663v1","updated":"2024-10-04T17:59:41Z","published":"2024-10-04T17:59:41Z","title":"Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge\n  Distillation from Multiple Large Language Models","summary":"  Large language models (LLMs) have exhibited complex reasoning abilities by\ngenerating question rationales and demonstrated exceptional performance in\nnatural language processing (NLP) tasks. However, these reasoning capabilities\ngenerally emerge in models with tens of billions of parameters, creating\nsignificant computational challenges for real-world deployment. Recent research\nhas concentrated on improving open-source smaller models through knowledge\ndistillation (KD) from commercial LLMs. Nevertheless, most of these studies\nrely solely on the responses from one single LLM as the gold rationale for\ntraining. In this paper, we introduce a novel Mistake-Aware Peer-Review\nDistillation (MAPD) approach: 1) Instead of merely obtaining gold rationales\nfrom teachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.\n","authors":["Zhuochun Li","Yuelyu Ji","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.03663v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03659v1","updated":"2024-10-04T17:59:28Z","published":"2024-10-04T17:59:28Z","title":"Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.\n","authors":["Tinghui Zhu","Qin Liu","Fei Wang","Zhengzhong Tu","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03659v1.pdf","comment":"Website:\n  https://darthzhu.github.io/cross-modality-knowledge-conflict/"},{"id":"http://arxiv.org/abs/2410.03658v1","updated":"2024-10-04T17:59:00Z","published":"2024-10-04T17:59:00Z","title":"RAFT: Realistic Attacks to Fool Text Detectors","summary":"  Large language models (LLMs) have exhibited remarkable fluency across various\ntasks. However, their unethical applications, such as disseminating\ndisinformation, have become a growing concern. Although recent works have\nproposed a number of LLM detection methods, their robustness and reliability\nremain unclear. In this paper, we present RAFT: a grammar error-free black-box\nattack against existing LLM detectors. In contrast to previous attacks for\nlanguage models, our method exploits the transferability of LLM embeddings at\nthe word-level while preserving the original text quality. We leverage an\nauxiliary embedding to greedily select candidate words to perturb against the\ntarget detector. Experiments reveal that our attack effectively compromises all\ndetectors in the study across various domains by up to 99%, and are\ntransferable across source models. Manual human evaluation studies show our\nattacks are realistic and indistinguishable from original human-written text.\nWe also show that examples generated by RAFT can be used to train adversarially\nrobust detectors. Our work shows that current LLM detectors are not\nadversarially robust, underscoring the urgent need for more resilient detection\nmechanisms.\n","authors":["James Wang","Ran Li","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2410.03658v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.06520v2","updated":"2024-10-04T17:50:34Z","published":"2024-08-12T22:40:01Z","title":"Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and\n  Hindsight Modular Reflections for Task Planning with LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nRetrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework\nthat decomposes complex tasks into sub-tasks using an LLM-based high-level\npolicy, in which a complex task is decomposed into sub-tasks by a high-level\npolicy on-the-fly. The sub-tasks, defined by goals, are assigned to the\nlow-level policy to complete. To improve the agent's performance in\nmulti-episode execution, we propose Hindsight Modular Reflection (HMR), where,\ninstead of reflecting on the full trajectory, we let the agent reflect on\nshorter sub-trajectories to improve reflection efficiency. We evaluated the\ndecision-making ability of the proposed RAHL in three benchmark\nenvironments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL can\nachieve an improvement in performance in 9%, 42%, and 10% in 5 episodes of\nexecution in strong baselines. Furthermore, we also implemented RAHL on the\nBoston Dynamics SPOT robot. The experiment shows that the robot can scan the\nenvironment, find entrances, and navigate to new rooms controlled by the LLM\npolicy.\n","authors":["Chuanneng Sun","Songjun Huang","Dario Pompili"],"pdf_url":"https://arxiv.org/pdf/2408.06520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03642v1","updated":"2024-10-04T17:48:29Z","published":"2024-10-04T17:48:29Z","title":"Aligning LLMs with Individual Preferences via Interaction","summary":"  As large language models (LLMs) demonstrate increasingly advanced\ncapabilities, aligning their behaviors with human values and preferences\nbecomes crucial for their wide adoption. While previous research focuses on\ngeneral alignment to principles such as helpfulness, harmlessness, and honesty,\nthe need to account for individual and diverse preferences has been largely\noverlooked, potentially undermining customized human experiences. To address\nthis gap, we train LLMs that can ''interact to align'', essentially cultivating\nthe meta-skill of LLMs to implicitly infer the unspoken personalized\npreferences of the current user through multi-turn conversations, and then\ndynamically align their following behaviors and responses to these inferred\npreferences. Our approach involves establishing a diverse pool of 3,310\ndistinct user personas by initially creating seed examples, which are then\nexpanded through iterative self-generation and filtering. Guided by distinct\nuser personas, we leverage multi-LLM collaboration to develop a multi-turn\npreference dataset containing 3K+ multi-turn conversations in tree structures.\nFinally, we apply supervised fine-tuning and reinforcement learning to enhance\nLLMs using this dataset. For evaluation, we establish the ALOE (ALign With\nCustOmized PrEferences) benchmark, consisting of 100 carefully selected\nexamples and well-designed metrics to measure the customized alignment\nperformance during conversations. Experimental results demonstrate the\neffectiveness of our method in enabling dynamic, personalized alignment via\ninteraction.\n","authors":["Shujin Wu","May Fung","Cheng Qian","Jeonghwan Kim","Dilek Hakkani-Tur","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.03642v1.pdf","comment":"The code and dataset are made public at\n  https://github.com/ShujinWu-0814/ALOE"},{"id":"http://arxiv.org/abs/2405.20974v3","updated":"2024-10-04T17:23:48Z","published":"2024-05-31T16:21:16Z","title":"SaySelf: Teaching LLMs to Express Confidence with Self-Reflective\n  Rationales","summary":"  Large language models (LLMs) often generate inaccurate or fabricated\ninformation and generally fail to indicate their confidence, which limits their\nbroader applications. Previous work elicits confidence from LLMs by direct or\nself-consistency prompting, or constructing specific datasets for supervised\nfinetuning. The prompting-based approaches have inferior performance, and the\ntraining-based approaches are limited to binary or inaccurate group-level\nconfidence estimates. In this work, we present the advanced SaySelf, a training\nframework that teaches LLMs to express more accurate fine-grained confidence\nestimates. In addition, beyond the confidence scores, SaySelf initiates the\nprocess of directing LLMs to produce self-reflective rationales that clearly\nidentify gaps in their parametric knowledge and explain their uncertainty. This\nis achieved by using an LLM to automatically summarize the uncertainties in\nspecific knowledge via natural language. The summarization is based on the\nanalysis of the inconsistency in multiple sampled reasoning chains, and the\nresulting data is utilized for supervised fine-tuning. Moreover, we utilize\nreinforcement learning with a meticulously crafted reward function to calibrate\nthe confidence estimates, motivating LLMs to deliver accurate, high-confidence\npredictions and to penalize overconfidence in erroneous outputs. Experimental\nresults in both in-distribution and out-of-distribution datasets demonstrate\nthe effectiveness of SaySelf in reducing the confidence calibration error and\nmaintaining the task performance. We show that the generated self-reflective\nrationales are reasonable and can further contribute to the calibration. The\ncode is made public at https://github.com/xu1868/SaySelf.\n","authors":["Tianyang Xu","Shujin Wu","Shizhe Diao","Xiaoze Liu","Xingyao Wang","Yangyi Chen","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2405.20974v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03617v1","updated":"2024-10-04T17:17:19Z","published":"2024-10-04T17:17:19Z","title":"What Matters for Model Merging at Scale?","summary":"  Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.\n","authors":["Prateek Yadav","Tu Vu","Jonathan Lai","Alexandra Chronopoulou","Manaal Faruqui","Mohit Bansal","Tsendsuren Munkhdalai"],"pdf_url":"https://arxiv.org/pdf/2410.03617v1.pdf","comment":"20 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2410.03608v1","updated":"2024-10-04T17:09:08Z","published":"2024-10-04T17:09:08Z","title":"TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and\n  Generation","summary":"  Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).\n","authors":["Jonathan Cook","Tim Rocktäschel","Jakob Foerster","Dennis Aumiller","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14505v2","updated":"2024-10-04T17:08:17Z","published":"2024-08-24T07:59:36Z","title":"Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware\n  Reprogramming","summary":"  Spatio-temporal forecasting is pivotal in numerous real-world applications,\nincluding transportation planning, energy management, and climate monitoring.\nIn this work, we aim to harness the reasoning and generalization abilities of\nPre-trained Language Models (PLMs) for more effective spatio-temporal\nforecasting, particularly in data-scarce scenarios. However, recent studies\nuncover that PLMs, which are primarily trained on textual data, often falter\nwhen tasked with modeling the intricate correlations in numerical time series,\nthereby limiting their effectiveness in comprehending spatio-temporal data. To\nbridge the gap, we propose RePST, a physics-aware PLM reprogramming framework\ntailored for spatio-temporal forecasting. Specifically, we first propose a\nphysics-aware decomposer that adaptively disentangles spatially correlated time\nseries into interpretable sub-components, which facilitates PLM to understand\nsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.\nMoreover, we propose a selective discrete reprogramming scheme, which\nintroduces an expanded spatio-temporal vocabulary space to project\nspatio-temporal series into discrete representations. This scheme minimizes the\ninformation loss during reprogramming and enriches the representations derived\nby PLMs. Extensive experiments on real-world datasets show that the proposed\nRePST outperforms twelve state-of-the-art baseline methods, particularly in\ndata-scarce scenarios, highlighting the effectiveness and superior\ngeneralization capabilities of PLMs for spatio-temporal forecasting.\n","authors":["Hao Wang","Jindong Han","Wei Fan","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03600v1","updated":"2024-10-04T16:58:41Z","published":"2024-10-04T16:58:41Z","title":"Efficiently Identifying Watermarked Segments in Mixed-Source Texts","summary":"  Text watermarks in large language models (LLMs) are increasingly used to\ndetect synthetic text, mitigating misuse cases like fake news and academic\ndishonesty. While existing watermarking detection techniques primarily focus on\nclassifying entire documents as watermarked or not, they often neglect the\ncommon scenario of identifying individual watermark segments within longer,\nmixed-source documents. Drawing inspiration from plagiarism detection systems,\nwe propose two novel methods for partial watermark detection. First, we develop\na geometry cover detection framework aimed at determining whether there is a\nwatermark segment in long text. Second, we introduce an adaptive online\nlearning algorithm to pinpoint the precise location of watermark segments\nwithin the text. Evaluated on three popular watermarking techniques\n(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves\nhigh accuracy, significantly outperforming baseline methods. Moreover, our\nframework is adaptable to other watermarking techniques, offering new insights\nfor precise watermark detection.\n","authors":["Xuandong Zhao","Chenwen Liao","Yu-Xiang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.03600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09598v2","updated":"2024-10-04T16:57:08Z","published":"2024-09-15T03:25:55Z","title":"Improving Statistical Significance in Human Evaluation of Automatic\n  Metrics via Soft Pairwise Accuracy","summary":"  Selecting an automatic metric that best emulates human annotators is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\nscores, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric scores. We show that SPA is more stable than PA with\nrespect to changes in the number of systems/segments used for evaluation. We\nalso show that PA can only assign a small set of distinct output values to\nmetrics, and this results in many metrics being artificially assigned the exact\nsame PA score. We demonstrate that SPA fixes this issue. Finally, we show that\nSPA is more discriminative than PA, producing more statistically significant\ncomparisons between metrics. SPA was selected as the official system-level\nmetric for the 2024 WMT Metrics Shared Task.\n","authors":["Brian Thompson","Nitika Mathur","Daniel Deutsch","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2409.09598v2.pdf","comment":"Accepted at WMT 2024"},{"id":"http://arxiv.org/abs/2404.15155v2","updated":"2024-10-04T16:56:06Z","published":"2024-04-22T06:30:05Z","title":"MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making","summary":"  Foundation models are becoming valuable tools in medicine. Yet despite their\npromise, the best way to leverage Large Language Models (LLMs) in complex\nmedical tasks remains an open question. We introduce a novel multi-agent\nframework, named Medical Decision-making Agents (MDAgents) that helps address\nthis gap by automatically assigning a collaboration structure to a team of\nLLMs. The assigned solo or group collaboration structure is tailored to the\nmedical task at hand, emulating real-world medical decision-making processes\nadapted to tasks of varying complexities. We evaluate our framework and\nbaseline methods using state-of-the-art LLMs across a suite of real-world\nmedical knowledge and medical diagnosis benchmarks. MDAgents achieved the best\nperformance in seven out of ten benchmarks on tasks requiring an understanding\nof medical knowledge and multi-modal reasoning, showing a significant\nimprovement of up to 6.5% (p < 0.05) compared to previous methods' best\nperformances. Ablation studies reveal that MDAgents effectively determines\nmedical complexity to optimize for efficiency and accuracy across diverse\nmedical tasks. Notably, the combination of moderator review and external\nmedical knowledge in group collaboration resulted in an average accuracy\nimprovement of 11.8%. Our code can be found at\nhttps://github.com/mitmedialab/MDAgents.\n","authors":["Yubin Kim","Chanwoo Park","Hyewon Jeong","Yik Siu Chan","Xuhai Xu","Daniel McDuff","Hyeonhoon Lee","Marzyeh Ghassemi","Cynthia Breazeal","Hae Won Park"],"pdf_url":"https://arxiv.org/pdf/2404.15155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03595v1","updated":"2024-10-04T16:55:30Z","published":"2024-10-04T16:55:30Z","title":"Understanding Reasoning in Chain-of-Thought from the Hopfieldian View","summary":"  Large Language Models have demonstrated remarkable abilities across various\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\nenhance reasoning capabilities. However, existing research primarily focuses on\nimproving performance, lacking a comprehensive framework to explain and\nunderstand the fundamental factors behind CoT's success. To bridge this gap, we\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\ncognitive neuroscience. We establish a connection between CoT reasoning and key\ncognitive elements such as stimuli, actions, neural populations, and\nrepresentation spaces. From our view, we can understand the reasoning process\nas the movement between these representation spaces. Building on this insight,\nwe develop a method for localizing reasoning errors in the response of CoTs.\nMoreover, we propose the Representation-of-Thought (RoT) framework, which\nleverages the robustness of low-dimensional representation spaces to enhance\nthe robustness of the reasoning process in CoTs. Experimental results\ndemonstrate that RoT improves the robustness and interpretability of CoT\nreasoning while offering fine-grained control over the reasoning process.\n","authors":["Lijie Hu","Liang Liu","Shu Yang","Xin Chen","Zhen Tan","Muhammad Asif Ali","Mengdi Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03595v1.pdf","comment":"28 pages, a new version of \"A Hopfieldian View-based Interpretation\n  for Chain-of-Thought Reasoning\""},{"id":"http://arxiv.org/abs/2410.03594v1","updated":"2024-10-04T16:54:30Z","published":"2024-10-04T16:54:30Z","title":"Explicit, Implicit, and Scattered: Revisiting Event Extraction to\n  Capture Complex Arguments","summary":"  Prior works formulate the extraction of event-specific arguments as a span\nextraction problem, where event arguments are explicit -- i.e. assumed to be\ncontiguous spans of text in a document. In this study, we revisit this\ndefinition of Event Extraction (EE) by introducing two key argument types that\ncannot be modeled by existing EE frameworks. First, implicit arguments are\nevent arguments which are not explicitly mentioned in the text, but can be\ninferred through context. Second, scattered arguments are event arguments that\nare composed of information scattered throughout the text. These two argument\ntypes are crucial to elicit the full breadth of information required for proper\nevent modeling.\n  To support the extraction of explicit, implicit, and scattered arguments, we\ndevelop a novel dataset, DiscourseEE, which includes 7,464 argument annotations\nfrom online health discourse. Notably, 51.2% of the arguments are implicit, and\n17.4% are scattered, making DiscourseEE a unique corpus for complex event\nextraction. Additionally, we formulate argument extraction as a text generation\nproblem to facilitate the extraction of complex argument types. We provide a\ncomprehensive evaluation of state-of-the-art models and highlight critical open\nchallenges in generative event extraction. Our data and codebase are available\nat https://omar-sharif03.github.io/DiscourseEE.\n","authors":["Omar Sharif","Joseph Gatto","Madhusudan Basak","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2410.03594v1.pdf","comment":"Accepted in EMNLP-2024 (Main). 21 pages, 8 figures, and 11 tables"},{"id":"http://arxiv.org/abs/2407.11229v2","updated":"2024-10-04T16:52:57Z","published":"2024-07-15T20:29:24Z","title":"Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness","summary":"  Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.\n","authors":["Srija Mukhopadhyay","Adnan Qidwai","Aparna Garimella","Pritika Ramu","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.11229v2.pdf","comment":"22 pages, 9 Tables, 5 figures, 22 examples"},{"id":"http://arxiv.org/abs/2312.06149v4","updated":"2024-10-04T16:48:51Z","published":"2023-12-11T06:35:33Z","title":"Unlocking Anticipatory Text Generation: A Constrained Approach for Large\n  Language Models Decoding","summary":"  Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).\n","authors":["Lifu Tu","Semih Yavuz","Jin Qu","Jiacheng Xu","Rui Meng","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.06149v4.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.02018v2","updated":"2024-10-04T16:46:00Z","published":"2024-06-04T06:57:47Z","title":"Why Would You Suggest That? Human Trust in Language Model Responses","summary":"  The emergence of Large Language Models (LLMs) has revealed a growing need for\nhuman-AI collaboration, especially in creative decision-making scenarios where\ntrust and reliance are paramount. Through human studies and model evaluations\non the open-ended News Headline Generation task from the LaMP benchmark, we\nanalyze how the framing and presence of explanations affect user trust and\nmodel performance. Overall, we provide evidence that adding an explanation in\nthe model response to justify its reasoning significantly increases\nself-reported user trust in the model when the user has the opportunity to\ncompare various responses. Position and faithfulness of these explanations are\nalso important factors. However, these gains disappear when users are shown\nresponses independently, suggesting that humans trust all model responses,\nincluding deceptive ones, equitably when they are shown in isolation. Our\nfindings urge future research to delve deeper into the nuanced evaluation of\ntrust in human-machine teaming systems.\n","authors":["Manasi Sharma","Ho Chit Siu","Rohan Paleja","Jaime D. Peña"],"pdf_url":"https://arxiv.org/pdf/2406.02018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13069v3","updated":"2024-10-04T16:42:20Z","published":"2024-06-18T21:31:19Z","title":"Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG","summary":"  How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a\nnovel search tool inspired by indexing of genomic data. We compare the novelty\nof LM-generated text to human-written text and explore factors that affect\ngeneration novelty, focusing on the Pythia models. We find that, for $n > 4$,\nLM-generated text is less novel than human-written text, though it is more\nnovel for smaller $n$. Larger LMs and more constrained decoding strategies both\ndecrease novelty. Finally, we show that LMs complete $n$-grams with lower loss\nif they are more frequent in the training data. Overall, our results reveal\nfactors influencing the novelty of LM-generated text, and we release Rusty-DAWG\nto facilitate further pretraining data research.\n","authors":["William Merrill","Noah A. Smith","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2406.13069v3.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.13213v2","updated":"2024-10-04T16:29:58Z","published":"2024-02-20T18:24:47Z","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","summary":"  We study 14 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigororous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","authors":["Benjamin Plaut","Nguyen X. Khanh","Tu Trinh"],"pdf_url":"https://arxiv.org/pdf/2402.13213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03576v1","updated":"2024-10-04T16:26:12Z","published":"2024-10-04T16:26:12Z","title":"Table Question Answering for Low-resourced Indic Languages","summary":"  TableQA is the task of answering questions over tables of structured\ninformation, returning individual cells or tables as output. TableQA research\nhas focused primarily on high-resource languages, leaving medium- and\nlow-resource languages with little progress due to scarcity of annotated data\nand neural models. We address this gap by introducing a fully automatic\nlarge-scale tableQA data generation process for low-resource languages with\nlimited budget. We incorporate our data generation method on two Indic\nlanguages, Bengali and Hindi, which have no tableQA datasets or models. TableQA\nmodels trained on our large-scale datasets outperform state-of-the-art LLMs. We\nfurther study the trained models on different aspects, including mathematical\nreasoning capabilities and zero-shot cross-lingual transfer. Our work is the\nfirst on low-resource tableQA focusing on scalable data generation and\nevaluation procedures. Our proposed data generation method can be applied to\nany low-resource language with a web presence. We release datasets, models, and\ncode (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).\n","authors":["Vaishali Pal","Evangelos Kanoulas","Andrew Yates","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2410.03576v1.pdf","comment":"Accepted at EMNLP,2024"},{"id":"http://arxiv.org/abs/2410.03568v1","updated":"2024-10-04T16:18:29Z","published":"2024-10-04T16:18:29Z","title":"Towards Linguistically-Aware and Language-Independent Tokenization for\n  Large Language Models (LLMs)","summary":"  This paper presents a comprehensive study on the tokenization techniques\nemployed by state-of-the-art large language models (LLMs) and their\nimplications on the cost and availability of services across different\nlanguages, especially low resource languages. The analysis considers multiple\nLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base\nembeddings), and DaVinci (employing r50k_base embeddings), as well as the\nwidely used BERT base tokenizer. The study evaluates the tokenization\nvariability observed across these models and investigates the challenges of\nlinguistic representation in subword tokenization. The research underscores the\nimportance of fostering linguistically-aware development practices, especially\nfor languages that are traditionally under-resourced. Moreover, this paper\nintroduces case studies that highlight the real-world implications of\ntokenization choices, particularly in the context of electronic health record\n(EHR) systems. This research aims to promote generalizable Internationalization\n(I18N) practices in the development of AI services in this domain and beyond,\nwith a strong emphasis on inclusivity, particularly for languages traditionally\nunderrepresented in AI applications.\n","authors":["Abrar Rahman","Garry Bowlin","Binit Mohanty","Sean McGunigal"],"pdf_url":"https://arxiv.org/pdf/2410.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00191v2","updated":"2024-10-04T16:11:20Z","published":"2024-06-28T19:02:59Z","title":"MetaKP: On-Demand Keyphrase Generation","summary":"  Traditional keyphrase prediction methods predict a single set of keyphrases\nper document, failing to cater to the diverse needs of users and downstream\napplications. To bridge the gap, we introduce on-demand keyphrase generation, a\nnovel paradigm that requires keyphrases that conform to specific high-level\ngoals or intents. For this task, we present MetaKP, a large-scale benchmark\ncomprising four datasets, 7500 documents, and 3760 goals across news and\nbiomedical domains with human-annotated keyphrases. Leveraging MetaKP, we\ndesign both supervised and unsupervised methods, including a multi-task\nfine-tuning approach and a self-consistency prompting method with large\nlanguage models. The results highlight the challenges of supervised\nfine-tuning, whose performance is not robust to distribution shifts. By\ncontrast, the proposed self-consistency prompting approach greatly improves the\nperformance of large language models, enabling GPT-4o to achieve 0.548 SemF1,\nsurpassing the performance of a fully fine-tuned BART-base model. Finally, we\ndemonstrate the potential of our method to serve as a general NLP\ninfrastructure, exemplified by its application in epidemic event detection from\nsocial media.\n","authors":["Di Wu","Xiaoxian Shen","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2407.00191v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.00741v2","updated":"2024-10-04T16:10:38Z","published":"2024-10-01T14:33:22Z","title":"VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP\n  Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has been widely studied and\napplied in numerous applications. However, the emphasis on brief summary texts\nduring pre-training prevents CLIP from understanding long descriptions. This\nissue is particularly acute regarding videos given that videos often contain\nabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra\nLength) model, which aims to unleash the long-description understanding\ncapability of video CLIP models. Firstly, we establish an automatic data\ncollection system and gather a large-scale VILD pre-training dataset with VIdeo\nand Long-Description pairs. Then, we propose Text-similarity-guided Primary\nComponent Matching (TPCM) to better learn the distribution of feature space\nwhile expanding the long description capability. We also introduce two new\ntasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware\nDescription Ranking (HDR) for further understanding improvement. Finally, we\nconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating the\nlong-description capability more comprehensively. Extensive experimental\nresults on widely-used text-video retrieval benchmarks with both short and long\ndescriptions and our LVDR benchmark can fully demonstrate the effectiveness of\nour method.\n","authors":["Jiapeng Wang","Chengyu Wang","Kunzhe Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2410.00741v2.pdf","comment":"EMNLP 2024 Main conference"},{"id":"http://arxiv.org/abs/2402.12821v3","updated":"2024-10-04T16:07:29Z","published":"2024-02-20T08:41:23Z","title":"Identifying Factual Inconsistencies in Summaries: Grounding LLM\n  Inference via Task Taxonomy","summary":"  Factual inconsistencies pose a significant hurdle for the faithful\nsummarization by generative models. While a major direction to enhance\ninconsistency detection is to derive stronger Natural Language Inference (NLI)\nmodels, we propose an orthogonal aspect that underscores the importance of\nincorporating task-specific taxonomy into the inference. To this end, we\nconsolidate key error types of inconsistent facts in summaries, and incorporate\nthem to facilitate both the zero-shot and supervised paradigms of LLMs.\nExtensive experiments on ten datasets of five distinct domains suggest that,\nzero-shot LLM inference could benefit from the explicit solution space depicted\nby the error type taxonomy, and achieves state-of-the-art performance overall,\nsurpassing specialized non-LLM baselines, as well as recent LLM baselines. We\nfurther distill models that fuse the taxonomy into parameters through our\ndesigned prompt completions and supervised training strategies, efficiently\nsubstituting state-of-the-art zero-shot inference with much larger LLMs.\n","authors":["Liyan Xu","Zhenlin Su","Mo Yu","Jin Xu","Jinho D. Choi","Jie Zhou","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12821v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03553v1","updated":"2024-10-04T16:02:50Z","published":"2024-10-04T16:02:50Z","title":"Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding","summary":"  Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.\n","authors":["Wei Wu","Chao Wang","Liyi Chen","Mingze Yin","Yiheng Zhu","Kun Fu","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03545v1","updated":"2024-10-04T15:58:15Z","published":"2024-10-04T15:58:15Z","title":"Enhancing Data Quality through Simple De-duplication: Navigating\n  Responsible Computational Social Science Research","summary":"  Research in natural language processing (NLP) for Computational Social\nScience (CSS) heavily relies on data from social media platforms. This data\nplays a crucial role in the development of models for analysing\nsocio-linguistic phenomena within online communities. In this work, we conduct\nan in-depth examination of 20 datasets extensively used in NLP for CSS to\ncomprehensively examine data quality. Our analysis reveals that social media\ndatasets exhibit varying levels of data duplication. Consequently, this gives\nrise to challenges like label inconsistencies and data leakage, compromising\nthe reliability of models. Our findings also suggest that data duplication has\nan impact on the current claims of state-of-the-art performance, potentially\nleading to an overestimation of model effectiveness in real-world scenarios.\nFinally, we propose new protocols and best practices for improving dataset\ndevelopment from social media data and its usage.\n","authors":["Yida Mu","Mali Jin","Xingyi Song","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2410.03545v1.pdf","comment":"Accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03543v1","updated":"2024-10-04T15:57:58Z","published":"2024-10-04T15:57:58Z","title":"Re-examining Sexism and Misogyny Classification with Annotator Attitudes","summary":"  Gender-Based Violence (GBV) is an increasing problem online, but existing\ndatasets fail to capture the plurality of possible annotator perspectives or\nensure the representation of affected groups. We revisit two important stages\nin the moderation pipeline for GBV: (1) manual data labelling; and (2)\nautomated classification. For (1), we examine two datasets to investigate the\nrelationship between annotator identities and attitudes and the responses they\ngive to two GBV labelling tasks. To this end, we collect demographic and\nattitudinal information from crowd-sourced annotators using three validated\nsurveys from Social Psychology. We find that higher Right Wing Authoritarianism\nscores are associated with a higher propensity to label text as sexist, while\nfor Social Dominance Orientation and Neosexist Attitudes, higher scores are\nassociated with a negative tendency to do so. For (2), we conduct\nclassification experiments using Large Language Models and five prompting\nstrategies, including infusing prompts with annotator information. We find: (i)\nannotator attitudes affect the ability of classifiers to predict their labels;\n(ii) including attitudinal information can boost performance when we use\nwell-structured brief annotator descriptions; and (iii) models struggle to\nreflect the increased complexity and imbalanced classes of the new label sets.\n","authors":["Aiqi Jiang","Nikolas Vitsakis","Tanvi Dinkar","Gavin Abercrombie","Ioannis Konstas"],"pdf_url":"https://arxiv.org/pdf/2410.03543v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03531v1","updated":"2024-10-04T15:52:29Z","published":"2024-10-04T15:52:29Z","title":"MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale\n  Extraction","summary":"  Unsupervised rationale extraction aims to extract text snippets to support\nmodel predictions without explicit rationale annotation. Researchers have made\nmany efforts to solve this task. Previous works often encode each aspect\nindependently, which may limit their ability to capture meaningful internal\ncorrelations between aspects. While there has been significant work on\nmitigating spurious correlations, our approach focuses on leveraging the\nbeneficial internal correlations to improve multi-aspect rationale extraction.\nIn this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain\nand predict multiple aspects simultaneously. Concretely, we propose a\nMulti-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to\nencode multiple text chunks simultaneously. Furthermore, multiple special\ntokens are prepended in front of the text with each corresponding to one\ncertain aspect. Finally, multi-task training is deployed to reduce the training\noverhead. Experimental results on two unsupervised rationale extraction\nbenchmarks show that MARE achieves state-of-the-art performance. Ablation\nstudies further demonstrate the effectiveness of our method. Our codes have\nbeen available at https://github.com/CSU-NLP-Group/MARE.\n","authors":["Han Jiang","Junwen Duan","Zhe Qu","Jianxin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03531v1.pdf","comment":"Accepted in EMNLP2024(Main) conference"},{"id":"http://arxiv.org/abs/2410.03529v1","updated":"2024-10-04T15:50:10Z","published":"2024-10-04T15:50:10Z","title":"No Need to Talk: Asynchronous Mixture of Language Models","summary":"  We introduce SmallTalk LM, an innovative method for training a mixture of\nlanguage models in an almost asynchronous manner. Each model of the mixture\nspecializes in distinct parts of the data distribution, without the need of\nhigh-bandwidth communication between the nodes training each model. At\ninference, a lightweight router directs a given sequence to a single expert,\naccording to a short prefix. This inference scheme naturally uses a fraction of\nthe parameters from the overall mixture model. Our experiments on language\nmodeling demonstrate tha SmallTalk LM achieves significantly lower perplexity\nthan dense model baselines for the same total training FLOPs and an almost\nidentical inference cost. Finally, in our downstream evaluations we outperform\nthe dense baseline on $75\\%$ of the tasks.\n","authors":["Anastasiia Filippova","Angelos Katharopoulos","David Grangier","Ronan Collobert"],"pdf_url":"https://arxiv.org/pdf/2410.03529v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.03524v1","updated":"2024-10-04T15:44:47Z","published":"2024-10-04T15:44:47Z","title":"Steering Large Language Models between Code Execution and Textual\n  Reasoning","summary":"  While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling law. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.\n","authors":["Yongchao Chen","Harsh Jhamtani","Srinagesh Sharma","Chuchu Fan","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03524v1.pdf","comment":"32 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.05493v2","updated":"2024-10-04T15:34:48Z","published":"2024-03-08T18:04:03Z","title":"To Err Is Human, but Llamas Can Learn It Too","summary":"  This study explores enhancing grammatical error correction (GEC) through\nartificial error generation (AEG) using language models (LMs). Specifically, we\nfine-tune Llama 2-based LMs for error generation and find that this approach\nyields synthetic errors akin to human errors. Next, we train GEC Llama models\nwith the help of these artificial errors and outperform previous\nstate-of-the-art error correction models, with gains ranging between 0.8 and 6\nF0.5 points across all tested languages (German, Ukrainian, and Estonian).\nMoreover, we demonstrate that generating errors by fine-tuning smaller\nsequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and\nGPT-4) also results in synthetic errors beneficially affecting error generation\nmodels.\n","authors":["Agnes Luhtaru","Taido Purason","Martin Vainikko","Maksym Del","Mark Fishel"],"pdf_url":"https://arxiv.org/pdf/2403.05493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17600v2","updated":"2024-10-04T15:22:29Z","published":"2024-06-25T14:42:17Z","title":"\"Seeing the Big through the Small\": Can LLMs Approximate Human Judgment\n  Distributions on NLI from a Few Explanations?","summary":"  Human label variation (HLV) is a valuable source of information that arises\nwhen multiple human annotators provide different labels for valid reasons. In\nNatural Language Inference (NLI) earlier approaches to capturing HLV involve\neither collecting annotations from many crowd workers to represent human\njudgment distribution (HJD) or use expert linguists to provide detailed\nexplanations for their chosen labels. While the former method provides denser\nHJD information, obtaining it is resource-intensive. In contrast, the latter\noffers richer textual information but it is challenging to scale up to many\nhuman judges. Besides, large language models (LLMs) are increasingly used as\nevaluators (\"LLM judges\") but with mixed results, and few works aim to study\nHJDs. This study proposes to exploit LLMs to approximate HJDs using a small\nnumber of expert labels and explanations. Our experiments show that a few\nexplanations significantly improve LLMs' ability to approximate HJDs with and\nwithout explicit labels, thereby providing a solution to scale up annotations\nfor HJD. However, fine-tuning smaller soft-label aware models with the\nLLM-generated model judgment distributions (MJDs) presents partially\ninconsistent results: while similar in distance, their resulting fine-tuned\nmodels and visualized distributions differ substantially. We show the\nimportance of complementing instance-level distance measures with a\nglobal-level shape metric and visualization to more effectively evaluate MJDs\nagainst human judgment distributions.\n","authors":["Beiduo Chen","Xinpeng Wang","Siyao Peng","Robert Litschko","Anna Korhonen","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2406.17600v2.pdf","comment":"Accepted by EMNLP 2024 Findings, 24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.03502v1","updated":"2024-10-04T15:15:36Z","published":"2024-10-04T15:15:36Z","title":"CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical\n  Large Language Models in Clinical Scenarios","summary":"  With the proliferation of Large Language Models (LLMs) in diverse domains,\nthere is a particular need for unified evaluation standards in clinical medical\nscenarios, where models need to be examined very thoroughly. We present\nCliMedBench, a comprehensive benchmark with 14 expert-guided core clinical\nscenarios specifically designed to assess the medical ability of LLMs across 7\npivot dimensions. It comprises 33,735 questions derived from real-world medical\nreports of top-tier tertiary hospitals and authentic examination exercises. The\nreliability of this benchmark has been confirmed in several ways. Subsequent\nexperiments with existing LLMs have led to the following findings: (i) Chinese\nmedical LLMs underperform on this benchmark, especially where medical reasoning\nand factual consistency are vital, underscoring the need for advances in\nclinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs\ndemonstrate substantial potential in medical clinics, while the limited input\ncapacity of many medical LLMs hinders their practical use. These findings\nreveal both the strengths and limitations of LLMs in clinical scenarios and\noffer critical insights for medical research.\n","authors":["Zetian Ouyang","Yishuai Qiu","Linlin Wang","Gerard de Melo","Ya Zhang","Yanfeng Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2410.03502v1.pdf","comment":"accepted by ENMLP-2024"},{"id":"http://arxiv.org/abs/2406.15352v2","updated":"2024-10-04T15:15:26Z","published":"2024-06-21T17:59:51Z","title":"A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student\n  Feedback to Make Mnemonic Learning Stick","summary":"  Keyword mnemonics are memorable explanations that link new terms to simpler\nkeywords. Prior work generates mnemonics for students, but they do not train\nmodels using mnemonics students prefer and aid learning. We build SMART, a\nmnemonic generator trained on feedback from real students learning new terms.\nTo train SMART, we first fine-tune LLaMA-2 on a curated set of user-written\nmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics\ngenerated by SMART in a flashcard app to find preferences on mnemonics students\nfavor. We gather 2684 preferences from 45 students across two types: expressed\n(inferred from ratings) and observed (inferred from student learning), yielding\nthree key findings. First, expressed and observed preferences disagree; what\nstudents think is helpful does not always capture what is truly helpful.\nSecond, Bayesian models can synthesize complementary data from multiple\npreference types into a single effectiveness signal. SMART is tuned via Direct\nPreference Optimization on this signal, which resolves ties and missing labels\nin the typical method of pairwise comparisons, augmenting data for LLM output\nquality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much\nlower deployment costs, showing the utility of capturing diverse student\nfeedback to align LLMs in education.\n","authors":["Nishant Balepur","Matthew Shu","Alexander Hoyle","Alison Robey","Shi Feng","Seraphina Goldfarb-Tarrant","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2406.15352v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.05673v3","updated":"2024-10-04T15:14:55Z","published":"2024-06-09T07:06:58Z","title":"Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples","summary":"  The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reinforcement learning aims to find limited highest-reward\nsolutions while neglecting the solution diversity. To fill this gap, we propose\nFlow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method\naimed at improving reasoning quality and diversity with minimal data. FoR\nformulates multi-step LLM reasoning as a Markovian flow on a DAG-structured\nreasoning graph. This formulation allows us to incorporate and adapt principled\nGFlowNet approaches, for finetuning LLMs to sample diverse reasoning paths with\nprobabilities proportional to the (unnormalized) reward of target problems.\nExtensive experiments show that, with limited training examples (e.g., 15\nexamples), FoR enables the discovery of diverse, creative, high-quality\nsolutions, greatly outperforming a wide range of existing inference and\ntraining methods across five challenging puzzle-solving tasks, including\nBlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube\n(spatial reasoning), 1D-ARC (abstraction reasoning), and PrOntoQA (logical\nreasoning). Code is available at https://github.com/Yu-Fangxu/FoR.\n","authors":["Fangxu Yu","Lai Jiang","Haoqiang Kang","Shibo Hao","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.05673v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14393v3","updated":"2024-10-04T15:10:36Z","published":"2024-06-20T15:12:27Z","title":"Jailbreaking as a Reward Misspecification Problem","summary":"  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.\n","authors":["Zhihui Xie","Jiahui Gao","Lei Li","Zhenguo Li","Qi Liu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2406.14393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12291v2","updated":"2024-10-04T15:08:14Z","published":"2024-02-19T17:05:29Z","title":"KARL: Knowledge-Aware Retrieval and Representations aid Retention and\n  Learning in Students","summary":"  Flashcard schedulers rely on 1) student models to predict the flashcards a\nstudent knows; and 2) teaching policies to pick which cards to show next via\nthese predictions. Prior student models, however, just use study data like the\nstudent's past responses, ignoring the text on cards. We propose content-aware\nscheduling, the first schedulers exploiting flashcard content. To give the\nfirst evidence that such schedulers enhance student learning, we build KARL, a\nsimple but effective content-aware student model employing deep knowledge\ntracing (DKT), retrieval, and BERT to predict student recall. We train KARL by\ncollecting a new dataset of 123,143 study logs on diverse trivia questions.\nKARL bests existing student models in AUC and calibration error. To ensure our\nimproved predictions lead to better student learning, we create a novel\ndelta-based teaching policy to deploy KARL online. Based on 32 study paths from\n27 users, KARL improves learning efficiency over SOTA, showing KARL's strength\nand encouraging researchers to look beyond historical study data to fully\ncapture student abilities.\n","authors":["Matthew Shu","Nishant Balepur","Shi Feng","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2402.12291v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03492v1","updated":"2024-10-04T15:04:28Z","published":"2024-10-04T15:04:28Z","title":"Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores","summary":"  Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation.\n","authors":["Robert E. Blackwell","Jon Barry","Anthony G. Cohn"],"pdf_url":"https://arxiv.org/pdf/2410.03492v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.02338v2","updated":"2024-10-04T14:59:04Z","published":"2024-10-03T09:48:09Z","title":"How Much Can RAG Help the Reasoning of LLM?","summary":"  Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.\n","authors":["Jingyu Liu","Jiaen Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16741v2","updated":"2024-10-04T14:54:08Z","published":"2024-07-23T17:50:43Z","title":"OpenHands: An Open Platform for AI Software Developers as Generalist\n  Agents","summary":"  Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the\ndevelopment of powerful and flexible AI agents that interact with the world in\nsimilar ways to those of a human developer: by writing code, interacting with a\ncommand line, and browsing the web. We describe how the platform allows for the\nimplementation of new agents, safe interaction with sandboxed environments for\ncode execution, coordination between multiple agents, and incorporation of\nevaluation benchmarks. Based on our currently incorporated benchmarks, we\nperform an evaluation of agents over 15 challenging tasks, including software\nengineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.\nReleased under the permissive MIT license, OpenHands is a community project\nspanning academia and industry with more than 2.1K contributions from over 188\ncontributors.\n","authors":["Xingyao Wang","Boxuan Li","Yufan Song","Frank F. Xu","Xiangru Tang","Mingchen Zhuge","Jiayi Pan","Yueqi Song","Bowen Li","Jaskirat Singh","Hoang H. Tran","Fuqiang Li","Ren Ma","Mingzhang Zheng","Bill Qian","Yanjun Shao","Niklas Muennighoff","Yizhe Zhang","Binyuan Hui","Junyang Lin","Robert Brennan","Hao Peng","Heng Ji","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2407.16741v2.pdf","comment":"Code: https://github.com/All-Hands-AI/OpenHands"},{"id":"http://arxiv.org/abs/2407.17125v3","updated":"2024-10-04T14:36:36Z","published":"2024-07-24T09:48:48Z","title":"To Know or Not To Know? Analyzing Self-Consistency of Large Language\n  Models under Ambiguity","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. This paper focuses\non entity type ambiguity, analyzing the proficiency and consistency of\nstate-of-the-art LLMs in applying factual knowledge when prompted with\nambiguous entities. To do so, we propose an evaluation protocol that\ndisentangles knowing from applying knowledge, and test state-of-the-art LLMs on\n49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing\nthe correct entity reading, achieving an average accuracy of only 85%, and as\nlow as 75% with underspecified prompts. The results also reveal systematic\ndiscrepancies in LLM behavior, showing that while the models may possess\nknowledge, they struggle to apply it consistently, exhibit biases toward\npreferred readings, and display self-inconsistencies. This highlights the need\nto address entity ambiguity in the future for more trustworthy LLMs.\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.10902v3","updated":"2024-10-04T14:32:01Z","published":"2024-08-20T14:45:23Z","title":"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs","summary":"  Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.\n","authors":["John Mendonça","Isabel Trancoso","Alon Lavie"],"pdf_url":"https://arxiv.org/pdf/2408.10902v3.pdf","comment":"Accepted to EMNLP2024 (findings)"},{"id":"http://arxiv.org/abs/2410.03466v1","updated":"2024-10-04T14:31:37Z","published":"2024-10-04T14:31:37Z","title":"Is Safer Better? The Impact of Guardrails on the Argumentative Strength\n  of LLMs in Hate Speech Countering","summary":"  The potential effectiveness of counterspeech as a hate speech mitigation\nstrategy is attracting increasing interest in the NLG research community,\nparticularly towards the task of automatically producing it. However,\nautomatically generated responses often lack the argumentative richness which\ncharacterises expert-produced counterspeech. In this work, we focus on two\naspects of counterspeech generation to produce more cogent responses. First, by\ninvestigating the tension between helpfulness and harmlessness of LLMs, we test\nwhether the presence of safety guardrails hinders the quality of the\ngenerations. Secondly, we assess whether attacking a specific component of the\nhate speech results in a more effective argumentative strategy to fight online\nhate. By conducting an extensive human and automatic evaluation, we show how\nthe presence of safety guardrails can be detrimental also to a task that\ninherently aims at fostering positive social interactions. Moreover, our\nresults show that attacking a specific component of the hate speech, and in\nparticular its implicit negative stereotype and its hateful parts, leads to\nhigher-quality generations.\n","authors":["Helena Bonaldi","Greta Damo","Nicolás Benjamín Ocampo","Elena Cabrio","Serena Villata","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2410.03466v1.pdf","comment":"To appear in Proceedings of the 2024 Conference on Empirical Methods\n  in Natural Language Processing (long paper)"},{"id":"http://arxiv.org/abs/2410.03461v1","updated":"2024-10-04T14:21:27Z","published":"2024-10-04T14:21:27Z","title":"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval Augmented Generation","summary":"  While retrieval augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. One common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10 % of their computational cost.\n","authors":["Tobias Leemann","Periklis Petridis","Giuseppe Vietri","Dionysis Manousakas","Aaron Roth","Sergul Aydore"],"pdf_url":"https://arxiv.org/pdf/2410.03461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17512v4","updated":"2024-10-04T14:19:27Z","published":"2024-02-27T13:54:48Z","title":"Latte: Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.\n","authors":["Rares Dolga","Lucas Maystre","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03458v1","updated":"2024-10-04T14:17:56Z","published":"2024-10-04T14:17:56Z","title":"Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges","summary":"  Vietnamese, a low-resource language, is typically categorized into three\nprimary dialect groups that belong to Northern, Central, and Southern Vietnam.\nHowever, each province within these regions exhibits its own distinct\npronunciation variations. Despite the existence of various speech recognition\ndatasets, none of them has provided a fine-grained classification of the 63\ndialects specific to individual provinces of Vietnam. To address this gap, we\nintroduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive\ndataset capturing the rich diversity of 63 provincial dialects spoken across\nVietnam. Our dataset comprises 102.56 hours of audio, consisting of\napproximately 19,000 utterances, and the associated transcripts contain over\n1.2 million words. To provide benchmarks and simultaneously demonstrate the\nchallenges of our dataset, we fine-tune state-of-the-art pre-trained models for\ntwo downstream tasks: (1) Dialect identification and (2) Speech recognition.\nThe empirical results suggest two implications including the influence of\ngeographical factors on dialects, and the constraints of current approaches in\nspeech recognition tasks involving multi-dialect speech data. Our dataset is\navailable for research purposes.\n","authors":["Nguyen Van Dinh","Thanh Chi Dang","Luan Thanh Nguyen","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.03458v1.pdf","comment":"Main EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03457v1","updated":"2024-10-04T14:15:56Z","published":"2024-10-04T14:15:56Z","title":"CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies\n  Written by LLM-Assisted Crowds","summary":"  Detecting logical fallacies in texts can help users spot argument flaws, but\nautomating this detection is not easy. Manually annotating fallacies in\nlarge-scale, real-world text data to create datasets for developing and\nvalidating detection models is costly. This paper introduces CoCoLoFa, the\nlargest known logical fallacy dataset, containing 7,706 comments for 648 news\narticles, with each comment labeled for fallacy presence and type. We recruited\n143 crowd workers to write comments embodying specific fallacy types (e.g.,\nslippery slope) in response to news articles. Recognizing the complexity of\nthis writing task, we built an LLM-powered assistant into the workers'\ninterface to aid in drafting and refining their comments. Experts rated the\nwriting quality and labeling validity of CoCoLoFa as high and reliable.\nBERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy\ndetection (F1=0.86) and classification (F1=0.87) performance on its test set,\noutperforming the state-of-the-art LLMs. Our work shows that combining\ncrowdsourcing and LLMs enables us to more effectively construct datasets for\ncomplex linguistic phenomena that crowd workers find challenging to produce on\ntheir own.\n","authors":["Min-Hsuan Yeh","Ruyuan Wan","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03457v1.pdf","comment":"In Proceedings of the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.03447v1","updated":"2024-10-04T14:09:05Z","published":"2024-10-04T14:09:05Z","title":"How Language Models Prioritize Contextual Grammatical Cues?","summary":"  Transformer-based language models have shown an excellent ability to\neffectively capture and utilize contextual information. Although various\nanalysis techniques have been used to quantify and trace the contribution of\nsingle contextual cues to a target task such as subject-verb agreement or\ncoreference resolution, scenarios in which multiple relevant cues are available\nin the context remain underexplored. In this paper, we investigate how language\nmodels handle gender agreement when multiple gender cue words are present, each\ncapable of independently disambiguating a target gender pronoun. We analyze two\nwidely used Transformer-based models: BERT, an encoder-based, and GPT-2, a\ndecoder-based model. Our analysis employs two complementary approaches: context\nmixing analysis, which tracks information flow within the model, and a variant\nof activation patching, which measures the impact of cues on the model's\nprediction. We find that BERT tends to prioritize the first cue in the context\nto form both the target word representations and the model's prediction, while\nGPT-2 relies more on the final cue. Our findings reveal striking differences in\nhow encoder-based and decoder-based models prioritize and use contextual\ninformation for their predictions.\n","authors":["Hamidreza Amirzadeh","Afra Alishahi","Hosein Mohebbi"],"pdf_url":"https://arxiv.org/pdf/2410.03447v1.pdf","comment":"Accepted to BlackboxNLP 2024"},{"id":"http://arxiv.org/abs/2410.03446v1","updated":"2024-10-04T14:08:02Z","published":"2024-10-04T14:08:02Z","title":"On Uncertainty In Natural Language Processing","summary":"  The last decade in deep learning has brought on increasingly capable systems\nthat are deployed on a wide variety of applications. In natural language\nprocessing, the field has been transformed by a number of breakthroughs\nincluding large language models, which are used in increasingly many\nuser-facing applications. In order to reap the benefits of this technology and\nreduce potential harms, it is important to quantify the reliability of model\npredictions and the uncertainties that shroud their development.\n  This thesis studies how uncertainty in natural language processing can be\ncharacterized from a linguistic, statistical and neural perspective, and how it\ncan be reduced and quantified through the design of the experimental pipeline.\nWe further explore uncertainty quantification in modeling by theoretically and\nempirically investigating the effect of inductive model biases in text\nclassification tasks. The corresponding experiments include data for three\ndifferent languages (Danish, English and Finnish) and tasks as well as a large\nset of different uncertainty quantification approaches. Additionally, we\npropose a method for calibrated sampling in natural language generation based\non non-exchangeable conformal prediction, which provides tighter token sets\nwith better coverage of the actual continuation. Lastly, we develop an approach\nto quantify confidence in large black-box language models using auxiliary\npredictors, where the confidence is predicted from the input to and generated\noutput text of the target model alone.\n","authors":["Dennis Ulmer"],"pdf_url":"https://arxiv.org/pdf/2410.03446v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2410.03440v1","updated":"2024-10-04T13:53:33Z","published":"2024-10-04T13:53:33Z","title":"Exploring the Benefit of Activation Sparsity in Pre-training","summary":"  Pre-trained Transformers inherently possess the characteristic of sparse\nactivation, where only a small fraction of the neurons are activated for each\ntoken. While sparse activation has been explored through post-training methods,\nits potential in pre-training remains untapped. In this work, we first study\nhow activation properties change during pre-training. Our examination reveals\nthat Transformers exhibit sparse activation throughout the majority of the\npre-training process while the activation correlation keeps evolving as\ntraining progresses. Leveraging this observation, we propose Switchable\nSparse-Dense Learning (SSD). SSD adaptively switches between the\nMixtures-of-Experts (MoE) based sparse training and the conventional dense\ntraining during the pre-training process, leveraging the efficiency of sparse\ntraining and avoiding the static activation correlation of sparse training.\nCompared to dense training, SSD achieves comparable performance with identical\nmodel size and reduces pre-training costs. Moreover, the models trained with\nSSD can be directly used as MoE models for sparse inference and achieve the\nsame performance as dense models with up to $2\\times$ faster inference speed.\nCodes are available at https://github.com/thunlp/moefication.\n","authors":["Zhengyan Zhang","Chaojun Xiao","Qiujieli Qin","Yankai Lin","Zhiyuan Zeng","Xu Han","Zhiyuan Liu","Ruobing Xie","Maosong Sun","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.03440v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2410.03439v1","updated":"2024-10-04T13:52:32Z","published":"2024-10-04T13:52:32Z","title":"ToolGen: Unified Tool Retrieval and Calling via Generation","summary":"  As large language models (LLMs) advance, their inability to autonomously\nexecute tasks by directly interacting with external tools remains a critical\nlimitation. Traditional methods rely on inputting tool descriptions as context,\nwhich is constrained by context length and requires separate, often\ninefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that\nintegrates tool knowledge directly into the LLM's parameters by representing\neach tool as a unique token. This enables the LLM to generate tool calls and\narguments as part of its next token prediction capabilities, seamlessly\nblending tool invocation with language generation. Our framework allows the LLM\nto access and utilize a vast amount of tools with no additional retrieval step,\nsignificantly enhancing both performance and scalability. Experimental results\nwith over 47,000 tools show that ToolGen not only achieves superior results in\nboth tool retrieval and autonomous task completion but also sets the stage for\na new era of AI agents that can adapt to tools across diverse domains. By\nfundamentally transforming tool retrieval into a generative process, ToolGen\npaves the way for more versatile, efficient, and autonomous AI systems. ToolGen\nenables end-to-end tool learning and opens opportunities for integration with\nother advanced techniques such as chain-of-thought and reinforcement learning,\nthereby expanding the practical capabilities of LLMs.\n","authors":["Renxi Wang","Xudong Han","Lei Ji","Shu Wang","Timothy Baldwin","Haonan Li"],"pdf_url":"https://arxiv.org/pdf/2410.03439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03435v1","updated":"2024-10-04T13:51:19Z","published":"2024-10-04T13:51:19Z","title":"A General Framework for Producing Interpretable Semantic Text Embeddings","summary":"  Semantic text embedding is essential to many tasks in Natural Language\nProcessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that\ndemand transparency. Recent approaches have improved interpretability by\nleveraging domain-expert-crafted or LLM-generated questions, but these methods\nrely heavily on expert input or well-prompt design, which restricts their\ngeneralizability and ability to generate discriminative questions across a wide\nrange of tasks. To address these challenges, we introduce \\algo{CQG-MBQA}\n(Contrastive Question Generation - Multi-task Binary Question Answering), a\ngeneral framework for producing interpretable semantic text embeddings across\ndiverse tasks. Our framework systematically generates highly discriminative,\nlow cognitive load yes/no questions through the \\algo{CQG} method and answers\nthem efficiently with the \\algo{MBQA} model, resulting in interpretable\nembeddings in a cost-effective manner. We validate the effectiveness and\ninterpretability of \\algo{CQG-MBQA} through extensive experiments and ablation\nstudies, demonstrating that it delivers embedding quality comparable to many\nadvanced black-box models while maintaining inherently interpretability.\nAdditionally, \\algo{CQG-MBQA} outperforms other interpretable text embedding\nmethods across various downstream tasks.\n","authors":["Yiqun Sun","Qiang Huang","Yixuan Tang","Anthony K. H. Tung","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.03435v1.pdf","comment":"19 pages, 5 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2410.03430v1","updated":"2024-10-04T13:40:15Z","published":"2024-10-04T13:40:15Z","title":"Images Speak Volumes: User-Centric Assessment of Image Generation for\n  Accessible Communication","summary":"  Explanatory images play a pivotal role in accessible and easy-to-read (E2R)\ntexts. However, the images available in online databases are not tailored\ntoward the respective texts, and the creation of customized images is\nexpensive. In this large-scale study, we investigated whether text-to-image\ngeneration models can close this gap by providing customizable images quickly\nand easily. We benchmarked seven, four open- and three closed-source, image\ngeneration models and provide an extensive evaluation of the resulting images.\nIn addition, we performed a user study with people from the E2R target group to\nexamine whether the images met their requirements. We find that some of the\nmodels show remarkable performance, but none of the models are ready to be used\nat a larger scale without human supervision. Our research is an important step\ntoward facilitating the creation of accessible information for E2R creators and\ntailoring accessible images to the target group's needs.\n","authors":["Miriam Anschütz","Tringa Sylaj","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2410.03430v1.pdf","comment":"To be published at TSAR workshop 2024\n  (https://tsar-workshop.github.io/)"},{"id":"http://arxiv.org/abs/2410.03429v1","updated":"2024-10-04T13:39:21Z","published":"2024-10-04T13:39:21Z","title":"How Hard is this Test Set? NLI Characterization by Exploiting Training\n  Dynamics","summary":"  Natural Language Inference (NLI) evaluation is crucial for assessing language\nunderstanding models; however, popular datasets suffer from systematic spurious\ncorrelations that artificially inflate actual model performance. To address\nthis, we propose a method for the automated creation of a challenging test set\nwithout relying on the manual construction of artificial and unrealistic\nexamples. We categorize the test set of popular NLI datasets into three\ndifficulty levels by leveraging methods that exploit training dynamics. This\ncategorization significantly reduces spurious correlation measures, with\nexamples labeled as having the highest difficulty showing markedly decreased\nperformance and encompassing more realistic and diverse linguistic phenomena.\nWhen our characterization method is applied to the training set, models trained\nwith only a fraction of the data achieve comparable performance to those\ntrained on the full dataset, surpassing other dataset characterization\ntechniques. Our research addresses limitations in NLI dataset construction,\nproviding a more authentic evaluation of model performance with implications\nfor diverse NLU applications.\n","authors":["Adrian Cosma","Stefan Ruseti","Mihai Dascalu","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2410.03429v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.15925v2","updated":"2024-10-04T13:37:12Z","published":"2024-02-24T23:01:21Z","title":"MultiContrievers: Analysis of Dense Retrieval Representations","summary":"  Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.\n","authors":["Seraphina Goldfarb-Tarrant","Pedro Rodriguez","Jane Dwivedi-Yu","Patrick Lewis"],"pdf_url":"https://arxiv.org/pdf/2402.15925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03421v1","updated":"2024-10-04T13:31:09Z","published":"2024-10-04T13:31:09Z","title":"One2set + Large Language Model: Best Partners for Keyphrase Generation","summary":"  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n","authors":["Liangying Shao","Liang Zhang","Minlong Peng","Guoqi Ma","Hao Yue","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.03421v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02713v2","updated":"2024-10-04T13:29:09Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v2.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02560v2","updated":"2024-10-04T13:25:38Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Iakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v2.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.03415v1","updated":"2024-10-04T13:25:32Z","published":"2024-10-04T13:25:32Z","title":"Surgical, Cheap, and Flexible: Mitigating False Refusal in Language\n  Models via Single Vector Ablation","summary":"  Training a language model to be both helpful and harmless requires careful\ncalibration of refusal behaviours: Models should refuse to follow malicious\ninstructions or give harmful advice (e.g. \"how do I kill someone?\"), but they\nshould not refuse safe requests, even if they superficially resemble unsafe\nones (e.g. \"how do I kill a Python process?\"). Avoiding such false refusal, as\nprior work has shown, is challenging even for highly-capable language models.\nIn this paper, we propose a simple and surgical method for mitigating false\nrefusal in language models via single vector ablation. For a given model, we\nextract a false refusal vector and show that ablating this vector reduces false\nrefusal rate without negatively impacting model safety and general model\ncapabilities. We also show that our approach can be used for fine-grained\ncalibration of model safety. Our approach is training-free and model-agnostic,\nmaking it useful for mitigating the problem of false refusal in current and\nfuture language models.\n","authors":["Xinpeng Wang","Chengzhi Hu","Paul Röttger","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.03415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03414v2","updated":"2024-10-04T13:24:59Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v2.pdf","comment":"48 pages, 24 figures"},{"id":"http://arxiv.org/abs/2410.03412v1","updated":"2024-10-04T13:23:50Z","published":"2024-10-04T13:23:50Z","title":"Team MTS @ AutoMin 2021: An Overview of Existing Summarization\n  Approaches and Comparison to Unsupervised Summarization Techniques","summary":"  Remote communication through video or audio conferences has become more\npopular than ever because of the worldwide pandemic. These events, therefore,\nhave provoked the development of systems for automatic minuting of spoken\nlanguage leading to AutoMin 2021 challenge. The following paper illustrates the\nresults of the research that team MTS has carried out while participating in\nthe Automatic Minutes challenge. In particular, in this paper we analyze\nexisting approaches to text and speech summarization, propose an unsupervised\nsummarization technique based on clustering and provide a pipeline that\nincludes an adapted automatic speech recognition block able to run on real-life\nrecordings. The proposed unsupervised technique outperforms pre-trained\nsummarization models on the automatic minuting task with Rouge 1, Rouge 2 and\nRouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,\nRouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,\n0.098, 1.857, 2.304, 1.911 on the test set accordingly\n","authors":["Olga Iakovenko","Anna Andreeva","Anna Lapidus","Liana Mikaelyan"],"pdf_url":"https://arxiv.org/pdf/2410.03412v1.pdf","comment":"First Shared Task on Automatic Minuting at Interspeech 2021"},{"id":"http://arxiv.org/abs/2407.13297v2","updated":"2024-10-04T13:20:11Z","published":"2024-07-18T08:56:02Z","title":"SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning","summary":"  Specialized lexicons are collections of words with associated constraints\nsuch as special definitions, specific roles, and intended target audiences.\nThese constraints are necessary for content generation and documentation tasks\n(e.g., writing technical manuals or children's reading materials), where the\ngoal is to reduce the ambiguity of text content and increase its overall\nreadability for a specific group of audience. Understanding how large language\nmodels can capture these constraints can help researchers build better, more\nimpactful tools for wider use beyond the NLP community. Towards this end, we\nintroduce SpeciaLex, a benchmark for evaluating a language model's ability to\nfollow specialized lexicon-based constraints across 18 diverse subtasks with\n1,785 test instances covering core tasks of Checking, Identification,\nRewriting, and Open Generation. We present an empirical evaluation of 15 open\nand closed-source LLMs and discuss insights on how factors such as model scale,\nopenness, setup, and recency affect performance upon evaluating with the\nbenchmark.\n","authors":["Joseph Marvin Imperial","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2407.13297v2.pdf","comment":"Camera-ready for EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.15227v2","updated":"2024-10-04T13:15:14Z","published":"2024-06-21T15:11:33Z","title":"A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation","summary":"  This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.\n","authors":["Irune Zubiaga","Aitor Soroa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2406.15227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11687v3","updated":"2024-10-04T13:06:24Z","published":"2024-06-17T16:05:32Z","title":"Tokenization Falling Short: On Subword Robustness in Large Language\n  Models","summary":"  Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.\n","authors":["Yekun Chai","Yewei Fang","Qiwei Peng","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2406.11687v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2401.12585v5","updated":"2024-10-04T13:03:49Z","published":"2024-01-23T09:33:31Z","title":"SLANG: New Concept Comprehension of Large Language Models","summary":"  The dynamic nature of language, particularly evident in the realm of slang\nand memes on the Internet, poses serious challenges to the adaptability of\nlarge language models (LLMs). Traditionally anchored to static datasets, these\nmodels often struggle to keep up with the rapid linguistic evolution\ncharacteristic of online communities. This research aims to bridge this gap by\nenhancing LLMs' comprehension of the evolving new concepts on the Internet,\nwithout the high cost of continual retraining. In pursuit of this goal, we\nintroduce $\\textbf{SLANG}$, a benchmark designed to autonomously integrate\nnovel data and assess LLMs' ability to comprehend emerging concepts, alongside\n$\\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to\nunderstand new phrases and their colloquial context. Our benchmark and approach\ninvolves understanding real-world instances of linguistic shifts, serving as\ncontextual beacons, to form more precise and contextually relevant connections\nbetween newly emerging expressions and their meanings. The empirical analysis\nshows that our causal inference-based approach outperforms the baseline methods\nin terms of precision and relevance in the comprehension of Internet slang and\nmemes.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.12585v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03394v1","updated":"2024-10-04T12:57:00Z","published":"2024-10-04T12:57:00Z","title":"Killing Two Flies with One Stone: An Attempt to Break LLMs Using\n  English->Icelandic Idioms and Proper Names","summary":"  This paper presents the submission of the \\'Arni Magn\\'usson Institute's team\nto the WMT24 test suite subtask, focusing on idiomatic expressions and proper\nnames for the English->Icelandic translation direction.\n  Intuitively and empirically, idioms and proper names are known to be a\nsignificant challenge for modern translation models. We create two different\ntest suites. The first evaluates the competency of MT systems in translating\ncommon English idiomatic expressions, as well as testing whether systems can\ndistinguish between those expressions and the same phrases when used in a\nliteral context. The second test suite consists of place names that should be\ntranslated into their Icelandic exonyms (and correctly inflected) and pairs of\nIcelandic names that share a surface form between the male and female variants,\nso that incorrect translations impact meaning as well as readability.\n  The scores reported are relatively low, especially for idiomatic expressions\nand place names, and indicate considerable room for improvement.\n","authors":["Bjarki Ármannsson","Hinrik Hafsteinsson","Atli Jasonarson","Steinþór Steingrímsson"],"pdf_url":"https://arxiv.org/pdf/2410.03394v1.pdf","comment":"WMT24 MT Test Suites subtask. 8 pages, 5 tables"},{"id":"http://arxiv.org/abs/2406.09206v2","updated":"2024-10-04T12:55:45Z","published":"2024-06-13T15:06:11Z","title":"Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models","summary":"  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .\n","authors":["Christopher Schröder","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2406.09206v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02115v2","updated":"2024-10-04T12:52:28Z","published":"2024-10-03T00:38:12Z","title":"L-CiteEval: Do Long-Context Models Truly Leverage Context for\n  Responding?","summary":"  Long-context models (LCMs) have made remarkable strides in recent years,\noffering users great convenience for handling tasks that involve long context,\nsuch as document summarization. As the community increasingly prioritizes the\nfaithfulness of generated results, merely ensuring the accuracy of LCM outputs\nis insufficient, as it is quite challenging for humans to verify the results\nfrom the extremely lengthy context. Yet, although some efforts have been made\nto assess whether LCMs respond truly based on the context, these works either\nare limited to specific tasks or heavily rely on external evaluation resources\nlike GPT4.In this work, we introduce L-CiteEval, a comprehensive multi-task\nbenchmark for long-context understanding with citations, aiming to evaluate\nboth the understanding capability and faithfulness of LCMs. L-CiteEval covers\n11 tasks from diverse domains, spanning context lengths from 8K to 48K, and\nprovides a fully automated evaluation suite. Through testing with 11\ncutting-edge closed-source and open-source LCMs, we find that although these\nmodels show minor differences in their generated results, open-source models\nsubstantially trail behind their closed-source counterparts in terms of\ncitation accuracy and recall. This suggests that current open-source LCMs are\nprone to responding based on their inherent knowledge rather than the given\ncontext, posing a significant risk to the user experience in practical\napplications. We also evaluate the RAG approach and observe that RAG can\nsignificantly improve the faithfulness of LCMs, albeit with a slight decrease\nin the generation quality. Furthermore, we discover a correlation between the\nattention mechanisms of LCMs and the citation generation process.\n","authors":["Zecheng Tang","Keyan Zhou","Juntao Li","Baibei Ji","Jianye Hou","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14122v2","updated":"2024-10-04T12:50:46Z","published":"2024-04-22T12:21:12Z","title":"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy\n  Data in Misaligned Languages Suffice?","summary":"  Traditionally, success in multilingual machine translation can be attributed\nto three key factors in training data: large volume, diverse translation\ndirections, and high quality. In the current practice of fine-tuning large\nlanguage models (LLMs) for translation, we revisit the importance of these\nfactors. We find that LLMs display strong translation capability after being\nfine-tuned on as few as 32 parallel sentences and that fine-tuning on a single\ntranslation direction enables translation in multiple directions. However, the\nchoice of direction is critical: fine-tuning LLMs with only English on the\ntarget side can lead to task misinterpretation, which hinders translation into\nnon-English languages. Problems also arise when noisy synthetic data is placed\non the target side, especially when the target language is well-represented in\nLLM pre-training. Yet interestingly, synthesized data in an under-represented\nlanguage has a less pronounced effect. Our findings suggest that when adapting\nLLMs to translation, the requirement on data quantity can be eased but careful\nconsiderations are still crucial to prevent an LLM from exploiting unintended\ndata biases.\n","authors":["Dawei Zhu","Pinzhen Chen","Miaoran Zhang","Barry Haddow","Xiaoyu Shen","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2404.14122v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03381v1","updated":"2024-10-04T12:48:32Z","published":"2024-10-04T12:48:32Z","title":"Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission\n  to the WMT24 General Translation Task","summary":"  This paper presents the submission of the \\'Arni Magnusson Institute's team\nto the WMT24 General translation task. We work on the English->Icelandic\ntranslation direction. Our system comprises four translation models and a\ngrammar correction model. For training our models we carefully curate our\ndatasets, aggressively filtering out sentence pairs that may detrimentally\naffect the quality of our system's output. Some of our data are collected from\nhuman translations and some are synthetically generated. A part of the\nsynthetic data is generated using an LLM, and we find that it increases the\ntranslation capability of our system significantly.\n","authors":["Atli Jasonarson","Hinrik Hafsteinsson","Bjarki Ármannsson","Steinþór Steingrímsson"],"pdf_url":"https://arxiv.org/pdf/2410.03381v1.pdf","comment":"WMT24 General Translation Task System Description Paper, 10 pages, 1\n  figure, 6 tables"},{"id":"http://arxiv.org/abs/2405.19874v2","updated":"2024-10-04T12:39:20Z","published":"2024-05-30T09:28:56Z","title":"Is In-Context Learning Sufficient for Instruction Following in LLMs?","summary":"  In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.\n","authors":["Hao Zhao","Maksym Andriushchenko","Francesco Croce","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2405.19874v2.pdf","comment":"Preprint. Code at https://github.com/tml-epfl/icl-alignment"},{"id":"http://arxiv.org/abs/2405.03279v3","updated":"2024-10-04T12:29:46Z","published":"2024-05-06T08:52:11Z","title":"Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous\n  Prompt Learning","summary":"  Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed.\n","authors":["Qizhou Chen","Taolin Zhang","Xiaofeng He","Dongyang Li","Chengyu Wang","Longtao Huang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2405.03279v3.pdf","comment":"16 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.03357v1","updated":"2024-10-04T12:24:02Z","published":"2024-10-04T12:24:02Z","title":"Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of\n  Meta-Learning and Joint Learning AMR Parsing","summary":"  Cross-lingual AMR parsing is the task of predicting AMR graphs in a target\nlanguage when training data is available only in a source language. Due to the\nsmall size of AMR training data and evaluation data, cross-lingual AMR parsing\nhas only been explored in a small set of languages such as English, Spanish,\nGerman, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),\nwho apply meta-learning to tackle cross-lingual syntactic parsing, we\ninvestigate the use of meta-learning for cross-lingual AMR parsing. We evaluate\nour models in $k$-shot scenarios (including 0-shot) and assess their\neffectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean\nand Croatian test sets are developed as part of our work, based on the existing\nThe Little Prince English AMR corpus, and made publicly available. We\nempirically study our method by comparing it to classical joint learning. Our\nfindings suggest that while the meta-learning model performs slightly better in\n0-shot evaluation for certain languages, the performance gain is minimal or\nabsent when $k$ is higher than 0.\n","authors":["Jeongwoo Kang","Maximin Coavoux","Cédric Lopez","Didier Schwab"],"pdf_url":"https://arxiv.org/pdf/2410.03357v1.pdf","comment":"to appear in Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03351v1","updated":"2024-10-04T12:17:08Z","published":"2024-10-04T12:17:08Z","title":"Generating Equivalent Representations of Code By A Self-Reflection\n  Approach","summary":"  Equivalent Representations (ERs) of code are textual representations that\npreserve the same semantics as the code itself, e.g., natural language comments\nand pseudocode. ERs play a critical role in software development and\nmaintenance. However, how to automatically generate ERs of code remains an open\nchallenge. In this paper, we propose a self-reflection approach to generating\nERs of code. It enables two Large Language Models (LLMs) to work mutually and\nproduce an ER through a reflection process. Depending on whether constraints on\nERs are applied, our approach generates ERs in both open and constrained\nsettings. We conduct a empirical study to generate ERs in two settings and\nobtain eight findings. (1) Generating ERs in the open setting. In the open\nsetting, we allow LLMs to represent code without any constraints, analyzing the\nresulting ERs and uncovering five key findings. These findings shed light on\nhow LLMs comprehend syntactic structures, APIs, and numerical computations in\ncode. (2) Generating ERs in the constrained setting. In the constrained\nsetting, we impose constraints on ERs, such as natural language comments,\npseudocode, and flowcharts. This allows our approach to address a range of\nsoftware engineering tasks. Based on our experiments, we have three findings\ndemonstrating that our approach can effectively generate ERs that adhere to\nspecific constraints, thus supporting various software engineering tasks. (3)\nFuture directions. We also discuss potential future research directions, such\nas deriving intermediate languages for code generation, exploring LLM-friendly\nrequirement descriptions, and further supporting software engineering tasks. We\nbelieve that this paper will spark discussions in research communities and\ninspire many follow-up studies.\n","authors":["Jia Li","Ge Li","Lecheng Wang","Hao Zhu","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2410.03351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10989v2","updated":"2024-10-04T12:13:42Z","published":"2024-09-17T08:44:20Z","title":"GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in\n  Machine Translation","summary":"  Gender bias in machine translation (MT) systems poses significant challenges\nthat often result in the reinforcement of harmful stereotypes. Especially in\nthe labour domain where frequently occupations are inaccurately associated with\nspecific genders, such biases perpetuate traditional gender stereotypes with a\nsignificant impact on society. Addressing these issues is crucial for ensuring\nequitable and accurate MT systems. This paper introduces a novel approach to\nstudying occupation-related gender bias through the creation of the GOSt-MT\n(Gender and Occupation Statistics for Machine Translation) Knowledge Graph.\nGOSt-MT integrates comprehensive gender statistics from real-world labour data\nand textual corpora used in MT training. This Knowledge Graph allows for a\ndetailed analysis of gender bias across English, French, and Greek,\nfacilitating the identification of persistent stereotypes and areas requiring\nintervention. By providing a structured framework for understanding how\noccupations are gendered in both labour markets and MT systems, GOSt-MT\ncontributes to efforts aimed at making MT systems more equitable and reducing\ngender biases in automated translations.\n","authors":["Orfeas Menis Mastromichalakis","Giorgos Filandrianos","Eva Tsouparopoulou","Dimitris Parsanoglou","Maria Symeonaki","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.10989v2.pdf","comment":"Accepted at the KG-STAR'24: Workshop on Knowledge Graphs for\n  Responsible AI co-located with the 33rd ACM CIKM Conference, October 25,\n  2024, Boise, Idaho"},{"id":"http://arxiv.org/abs/2408.13933v2","updated":"2024-10-04T12:03:03Z","published":"2024-08-25T20:41:22Z","title":"MobileQuant: Mobile-friendly Quantization for On-device Language Models","summary":"  Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU.\n","authors":["Fuwen Tan","Royson Lee","Łukasz Dudziak","Shell Xu Hu","Sourav Bhattacharya","Timothy Hospedales","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.13933v2.pdf","comment":"EMNLP 2024 Findings. Code and models available:\n  https://github.com/saic-fi/MobileQuant"},{"id":"http://arxiv.org/abs/2305.00450v3","updated":"2024-10-04T12:00:21Z","published":"2023-04-30T11:26:10Z","title":"SMILE: Single-turn to Multi-turn Inclusive Language Expansion via\n  ChatGPT for Mental Health Support","summary":"  Developing specialized dialogue systems for mental health support requires\nmulti-turn conversation data, which has recently garnered increasing attention.\nHowever, gathering and releasing large-scale, real-life multi-turn\nconversations that could facilitate advancements in mental health support\npresents challenges in data privacy protection and the time and cost involved\nin crowdsourcing. To address these challenges, we introduce SMILE, a\nsingle-turn to multi-turn inclusive language expansion technique that prompts\nChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work\nbegins by analyzing language transformation and validating the feasibility of\nour proposed method. We conduct a study on dialogue diversity, including\nlexical features, semantic features, and dialogue topics, demonstrating the\neffectiveness of our method. Further, we employ our method to generate a\nlarge-scale, lifelike, and diverse dialogue dataset named SMILECHAT, consisting\nof 55k dialogues. Finally, we utilize the collected corpus to develop a mental\nhealth chatbot, MeChat. To better assess the quality of SMILECHAT, we collect a\nsmall-scale real-life counseling dataset conducted by data anonymization. Both\nautomatic and human evaluations demonstrate significant improvements in our\ndialogue system and confirm that SMILECHAT is high-quality. Code, data, and\nmodel are publicly available at https://github.com/qiuhuachuan/smile.\n","authors":["Huachuan Qiu","Hongliang He","Shuai Zhang","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2305.00450v3.pdf","comment":"accepted to the EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03341v1","updated":"2024-10-04T11:57:32Z","published":"2024-10-04T11:57:32Z","title":"Zero-Shot Fact Verification via Natural Logic and Large Language Models","summary":"  The recent development of fact verification systems with natural logic has\nenhanced their explainability by aligning claims with evidence through\nset-theoretic operators, providing faithful justifications. Despite these\nadvancements, such systems often rely on a large amount of training data\nannotated with natural logic. To address this issue, we propose a zero-shot\nmethod that utilizes the generalization capabilities of instruction-tuned large\nlanguage models. To comprehensively assess the zero-shot capabilities of our\nmethod and other fact verification systems, we evaluate all models on both\nartificial and real-world claims, including multilingual datasets. We also\ncompare our method against other fact verification systems in two setups.\nFirst, in the zero-shot generalization setup, we demonstrate that our approach\noutperforms other systems that were not specifically trained on natural logic\ndata, achieving an average accuracy improvement of 8.96 points over the\nbest-performing baseline. Second, in the zero-shot transfer setup, we show that\ncurrent systems trained on natural logic data do not generalize well to other\ndomains, and our method outperforms these systems across all datasets with\nreal-world claims.\n","authors":["Marek Strong","Rami Aly","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.03341v1.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.11614v2","updated":"2024-10-04T11:46:20Z","published":"2024-06-17T15:00:35Z","title":"Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces","summary":"  The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.\n","authors":["Yihuai Hong","Lei Yu","Haiqin Yang","Shauli Ravfogel","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2406.11614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12163v2","updated":"2024-10-04T11:40:58Z","published":"2024-08-22T07:18:46Z","title":"Preference-Guided Reflective Sampling for Aligning Language Models","summary":"  Iterative data generation and model re-training can effectively align large\nlanguage models(LLMs) to human preferences. The process of data sampling is\ncrucial, as it significantly influences the success of policy improvement.\nRepeated random sampling is a widely used method that independently queries the\nmodel multiple times to generate outputs. In this work, we propose a more\neffective sampling method, named Preference-Guided Reflective Sampling (PRS).\nUnlike random sampling, PRS employs a tree-based generation framework to enable\nmore efficient sampling. It leverages adaptive self-refinement techniques to\nbetter explore the sampling space. By specifying user preferences in natural\nlanguage, PRS can further optimize response generation according to these\npreferences. As a result, PRS can align models to diverse user preferences. Our\nexperiments demonstrate that PRS generates higher-quality responses with\nsignificantly higher rewards. On AlpacaEval and Arena-Hard, PRS substantially\noutperforms repeated random sampling in best-of-$N$ sampling. Moreover, PRS\nshows strong performance when applied in iterative offline RL training.\n","authors":["Hai Ye","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2408.12163v2.pdf","comment":"EMNLP2024, main"},{"id":"http://arxiv.org/abs/2407.16222v2","updated":"2024-10-04T11:34:23Z","published":"2024-07-23T06:59:53Z","title":"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of\n  Multilingual Alignment","summary":"  Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.\n","authors":["Jiahuan Li","Shujian Huang","Aarron Ching","Xinyu Dai","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12593v2","updated":"2024-10-04T11:28:41Z","published":"2024-02-19T23:18:18Z","title":"Standardize: Aligning Language Models with Expert-Defined Standards for\n  Content Generation","summary":"  Domain experts across engineering, healthcare, and education follow strict\nstandards for producing quality content such as technical manuals, medication\ninstructions, and children's reading materials. However, current works in\ncontrollable text generation have yet to explore using these standards as\nreferences for control. Towards this end, we introduce Standardize, a\nretrieval-style in-context learning-based framework to guide large language\nmodels to align with expert-defined standards. Focusing on English language\nstandards in the education domain as a use case, we consider the Common\nEuropean Framework of Reference for Languages (CEFR) and Common Core Standards\n(CCS) for the task of open-ended content generation. Our findings show that\nmodels can gain a 45% to 100% increase in precise accuracy across open and\ncommercial LLMs evaluated, demonstrating that the use of knowledge artifacts\nextracted from standards and integrating them in the generation process can\neffectively guide models to produce better standard-aligned content.\n","authors":["Joseph Marvin Imperial","Gail Forey","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2402.12593v2.pdf","comment":"Camera-ready for EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2406.14654v2","updated":"2024-10-04T11:08:06Z","published":"2024-06-20T18:17:58Z","title":"Major Entity Identification: A Generalizable Alternative to Coreference\n  Resolution","summary":"  The limited generalization of coreference resolution (CR) models has been a\nmajor bottleneck in the task's broad application. Prior work has identified\nannotation differences, especially for mention detection, as one of the main\nreasons for the generalization gap and proposed using additional annotated\ntarget domain data. Rather than relying on this additional annotation, we\npropose an alternative referential task, Major Entity Identification (MEI),\nwhere we: (a) assume the target entities to be specified in the input, and (b)\nlimit the task to only the frequent entities. Through extensive experiments, we\ndemonstrate that MEI models generalize well across domains on multiple datasets\nwith supervised models and LLM-based few-shot prompting. Additionally, MEI fits\nthe classification framework, which enables the use of robust and intuitive\nclassification-based metrics. Finally, MEI is also of practical use as it\nallows a user to search for all mentions of a particular entity or a group of\nentities of interest.\n","authors":["Kawshik Manikantan","Shubham Toshniwal","Makarand Tapaswi","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2406.14654v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.02131v2","updated":"2024-10-04T11:05:18Z","published":"2024-10-03T01:24:09Z","title":"C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language\n  Pre-Training","summary":"  Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with their\naccompanying textual reports holds immense potential to enhance clinical\ndiagnostics through the combination of physiological data and qualitative\ninsights. However, this integration faces significant challenges due to\ninherent modality disparities and the scarcity of labeled data for robust\ncross-modal learning. To address these obstacles, we propose C-MELT, a novel\nframework that pre-trains ECG and text data using a contrastive masked\nauto-encoder architecture. C-MELT uniquely combines the strengths of generative\nwith enhanced discriminative capabilities to achieve robust cross-modal\nrepresentations. This is accomplished through masked modality modeling,\nspecialized loss functions, and an improved negative sampling strategy tailored\nfor cross-modal alignment. Extensive experiments on five public datasets across\ndiverse downstream tasks demonstrate that C-MELT significantly outperforms\nexisting methods, achieving 15% and 2% increases in linear probing and\nzero-shot performance over state-of-the-art models, respectively. These results\nhighlight the effectiveness of C-MELT, underscoring its potential to advance\nautomated clinical diagnostics through multi-modal representations.\n","authors":["Manh Pham","Aaqib Saeed","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03312v1","updated":"2024-10-04T10:50:18Z","published":"2024-10-04T10:50:18Z","title":"Context and System Fusion in Post-ASR Emotion Recognition with Large\n  Language Models","summary":"  Large language models (LLMs) have started to play a vital role in modelling\nspeech and text. To explore the best use of context and multiple systems'\noutputs for post-ASR speech emotion prediction, we study LLM prompting on a\nrecent task named GenSEC. Our techniques include ASR transcript ranking,\nvariable conversation context, and system output fusion. We show that the\nconversation context has diminishing returns and the metric used to select the\ntranscript for prediction is crucial. Finally, our best submission surpasses\nthe provided baseline by 20% in absolute accuracy.\n","authors":["Pavel Stepachev","Pinzhen Chen","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2410.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11368v2","updated":"2024-10-04T10:39:17Z","published":"2024-06-17T09:46:35Z","title":"Improving Quotation Attribution with Fictional Character Embeddings","summary":"  Humans naturally attribute utterances of direct speech to their speaker in\nliterary works. When attributing quotes, we process contextual information but\nalso access mental representations of characters that we build and revise\nthroughout the narrative. Recent methods to automatically attribute such\nutterances have explored simulating human logic with deterministic rules or\nlearning new implicit rules with neural networks when processing contextual\ninformation. However, these systems inherently lack \\textit{character}\nrepresentations, which often leads to errors in more challenging examples of\nattribution: anaphoric and implicit quotes. In this work, we propose to augment\na popular quotation attribution system, BookNLP, with character embeddings that\nencode global stylistic information of characters derived from an off-the-shelf\nstylometric model, Universal Authorship Representation (UAR). We create DramaCV\n(Code and data can be found at\nhttps://github.com/deezer/character_embeddings_qa ), a corpus of English drama\nplays from the 15th to 20th century that we automatically annotate for\nAuthorship Verification of fictional characters utterances, and release two\nversions of UAR trained on DramaCV, that are tailored for literary characters\nanalysis. Then, through an extensive evaluation on 28 novels, we show that\ncombining BookNLP's contextual information with our proposed global character\nembeddings improves the identification of speakers for anaphoric and implicit\nquotes, reaching state-of-the-art performance.\n","authors":["Gaspard Michel","Elena V. Epure","Romain Hennequin","Christophe Cerisara"],"pdf_url":"https://arxiv.org/pdf/2406.11368v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.12680v2","updated":"2024-10-04T10:30:44Z","published":"2024-06-18T14:51:54Z","title":"Measuring Psychological Depth in Language Models","summary":"  Evaluations of creative stories generated by large language models (LLMs)\noften focus on objective properties of the text, such as its style, coherence,\nand diversity. While these metrics are indispensable, they do not speak to a\nstory's subjective, psychological impact from a reader's perspective. We\nintroduce the Psychological Depth Scale (PDS), a novel framework rooted in\nliterary theory that measures an LLM's ability to produce authentic and\nnarratively complex stories that provoke emotion, empathy, and engagement. We\nempirically validate our framework by showing that humans can consistently\nevaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore\ntechniques for automating the PDS to easily scale future analyses. GPT-4o,\ncombined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an\naverage Spearman correlation of 0.51 with human judgment while Llama-3-70B with\nconstrained decoding scores as high as 0.68 for empathy. Finally, we compared\nthe depth of stories authored by both humans and LLMs. Surprisingly, GPT-4\nstories either surpassed or were statistically indistinguishable from\nhighly-rated human-written stories sourced from Reddit. By shifting the focus\nfrom text to reader, the Psychological Depth Scale is a validated, automated,\nand systematic means of measuring the capacity of LLMs to connect with humans\nthrough the stories they tell.\n","authors":["Fabrice Harel-Canada","Hanyu Zhou","Sreya Muppalla","Zeynep Yildiz","Miryung Kim","Amit Sahai","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2406.12680v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.04325v2","updated":"2024-10-04T10:23:56Z","published":"2024-05-07T13:55:11Z","title":"Deception in Reinforced Autonomous Agents","summary":"  We explore the ability of large language model (LLM)-based agents to engage\nin subtle deception such as strategically phrasing and intentionally\nmanipulating information to misguide and deceive other agents. This harmful\nbehavior can be hard to detect, unlike blatant lying or unintentional\nhallucination. We build an adversarial testbed mimicking a legislative\nenvironment where two LLMs play opposing roles: a corporate *lobbyist*\nproposing amendments to bills that benefit a specific company while evading a\n*critic* trying to detect this deception. We use real-world legislative bills\nmatched with potentially affected companies to ground these interactions. Our\nresults show that LLM lobbyists initially exhibit limited deception against\nstrong LLM critics which can be further improved through simple verbal\nreinforcement, significantly enhancing their deceptive capabilities, and\nincreasing deception rates by up to 40 points. This highlights the risk of\nautonomous agents manipulating other agents through seemingly neutral language\nto attain self-serving goals.\n","authors":["Atharvan Dogra","Krishna Pillutla","Ameet Deshpande","Ananya B Sai","John Nay","Tanmay Rajpurohit","Ashwin Kalyan","Balaraman Ravindran"],"pdf_url":"https://arxiv.org/pdf/2405.04325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03296v1","updated":"2024-10-04T10:14:12Z","published":"2024-10-04T10:14:12Z","title":"Comparing zero-shot self-explanations with human rationales in\n  multilingual text classification","summary":"  Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations that do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation by evaluating\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. For this, we\napply two text classification tasks: sentiment classification and forced labour\ndetection. Next to English, we further include Danish and Italian translations\nof the sentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\napply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our\nresults show that self-explanations align more closely with human annotations\ncompared to LRP, while maintaining a comparable level of faithfulness.\n","authors":["Stephanie Brandl","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2410.03296v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.03293v1","updated":"2024-10-04T10:06:55Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03278v1","updated":"2024-10-04T09:50:45Z","published":"2024-10-04T09:50:45Z","title":"What do Large Language Models Need for Machine Translation Evaluation?","summary":"  Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.\n","authors":["Shenbin Qian","Archchana Sindhujan","Minnie Kabra","Diptesh Kanojia","Constantin Orăsan","Tharindu Ranasinghe","Frédéric Blain"],"pdf_url":"https://arxiv.org/pdf/2410.03278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03277v1","updated":"2024-10-04T09:49:57Z","published":"2024-10-04T09:49:57Z","title":"A Multi-task Learning Framework for Evaluating Machine Translation of\n  Emotion-loaded User-generated Content","summary":"  Machine translation (MT) of user-generated content (UGC) poses unique\nchallenges, including handling slang, emotion, and literary devices like irony\nand sarcasm. Evaluating the quality of these translations is challenging as\ncurrent metrics do not focus on these ubiquitous features of UGC. To address\nthis issue, we utilize an existing emotion-related dataset that includes\nemotion labels and human-annotated translation errors based on\nMulti-dimensional Quality Metrics. We extend it with sentence-level evaluation\nscores and word-level labels, leading to a dataset suitable for sentence- and\nword-level translation evaluation and emotion classification, in a multi-task\nsetting. We propose a new architecture to perform these tasks concurrently,\nwith a novel combined loss function, which integrates different loss\nheuristics, like the Nash and Aligned losses. Our evaluation compares existing\nfine-tuning and multi-task learning approaches, assessing generalization with\nablative experiments over multiple datasets. Our approach achieves\nstate-of-the-art performance and we present a comprehensive analysis for MT\nevaluation of UGC.\n","authors":["Shenbin Qian","Constantin Orăsan","Diptesh Kanojia","Félix do Carmo"],"pdf_url":"https://arxiv.org/pdf/2410.03277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07237v3","updated":"2024-10-04T09:28:23Z","published":"2023-11-13T10:56:59Z","title":"In Search of the Long-Tail: Systematic Generation of Long-Tail\n  Inferential Knowledge via Logical Rule Guided Search","summary":"  To effectively use large language models (LLMs) for real-world queries, it is\nimperative that they generalize to the long-tail distribution, i.e. rare\nexamples where models exhibit low confidence. In this work, we take the first\nstep towards evaluating LLMs in the long-tail distribution of inferential\nknowledge. We exemplify long-tail evaluation on the Natural Language Inference\ntask. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic\nlong-tail data generation framework, to obtain factually-correct yet long-tail\ninferential statements. LINK uses variable-wise prompting grounded on symbolic\nrules to seek low-confidence statements while ensuring factual correctness. We\nthen use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail\ninferential knowledge dataset that contains 108K statements spanning four\ndomains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs\nshow significant performance drop (21% relative drop for GPT4) on long-tail\ndata as compared to on head distribution data, and smaller models show even\nmore generalization weakness. These results further underscore the necessity of\nlong-tail evaluation in developing generalizable LLMs.\n","authors":["Huihan Li","Yuting Ning","Zeyi Liao","Siyuan Wang","Xiang Lorraine Li","Ximing Lu","Wenting Zhao","Faeze Brahman","Yejin Choi","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2311.07237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02155v2","updated":"2024-10-04T09:27:20Z","published":"2024-10-03T02:34:31Z","title":"From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities","summary":"  Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Our method not only improves performance across various\nbenchmarks but also shows promising scalability, potentially paving the way for\nmore efficient and capable multimodal foundation models.\n","authors":["Wanpeng Zhang","Zilong Xie","Yicheng Feng","Yijiang Li","Xingrun Xing","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08495v2","updated":"2024-10-04T09:25:39Z","published":"2024-07-11T13:29:28Z","title":"Investigating LLMs as Voting Assistants via Contextual Augmentation: A\n  Case Study on the European Parliament Elections 2024","summary":"  In light of the recent 2024 European Parliament elections, we are\ninvestigating if LLMs can be used as Voting Advice Applications (VAAs). We\naudit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the\nstance of political parties based on the latest \"EU and I\" voting assistance\nquestionnaire. Furthermore, we explore alternatives to improve models'\nperformance by augmenting the input context via Retrieval-Augmented Generation\n(RAG) relying on web search, and Self-Reflection using staged conversations\nthat aim to re-collect relevant content from the model's internal memory. We\nfind that MIXTRAL is highly accurate with an 82% accuracy on average with a\nsignificant performance disparity across different political groups (50-95%).\nAugmenting the input context with expert-curated information can lead to a\nsignificant boost of approx. 9%, which remains an open challenge for automated\nRAG approaches, even considering curated content.\n","authors":["Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2407.08495v2.pdf","comment":"accepted to EMNLP 2024 as a short paper"},{"id":"http://arxiv.org/abs/2410.01524v2","updated":"2024-10-04T09:25:27Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03258v1","updated":"2024-10-04T09:24:55Z","published":"2024-10-04T09:24:55Z","title":"Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in\n  Finetuning Pretrained Language Models","summary":"  In this work, we show a fundamental limitation in vocabulary adaptation\napproaches that use Byte-Pair Encoding (BPE) tokenization scheme for\nfine-tuning pretrained language models (PLMs) to expert domains. Current\napproaches trivially append the target domain-specific vocabulary at the end of\nthe PLM vocabulary. This approach leads to a lower priority score and causes\nsub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a\ngiven text. To mitigate this issue, we propose AdaptBPE where the BPE\ntokenization initialization phase is modified to first perform the longest\nstring matching on the added (target) vocabulary before tokenizing at the\ncharacter level. We perform an extensive evaluation of AdaptBPE versus the\nstandard BPE over various classification and summarization tasks; AdaptBPE\nimproves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),\nrespectively. AdaptBPE for MEDVOC works particularly well when reference\nsummaries have high OOV concentration or are longer in length. We also conduct\na human evaluation, revealing that AdaptBPE generates more relevant and more\nfaithful summaries as compared to MEDVOC. We make our codebase publicly\navailable at https://github.com/gb-kgp/adaptbpe.\n","authors":["Gunjan Balde","Soumyadeep Roy","Mainack Mondal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2410.03258v1.pdf","comment":"11 pages. Accepted at EMNLP Findings 2024 (The 2024 Conference on\n  Empirical Methods in Natural Language Processing)"},{"id":"http://arxiv.org/abs/2403.20279v3","updated":"2024-10-04T09:19:07Z","published":"2024-03-29T16:49:24Z","title":"LUQ: Long-text Uncertainty Quantification for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. However, LLMs are also prone to generate nonfactual\ncontent. Uncertainty Quantification (UQ) is pivotal in enhancing our\nunderstanding of a model's confidence on its generation, thereby aiding in the\nmitigation of nonfactual outputs. Existing research on UQ predominantly targets\nshort text generation, typically yielding brief, word-limited responses.\nHowever, real-world applications frequently necessitate much longer responses.\nOur study first highlights the limitations of current UQ methods in handling\nlong text generation. We then introduce \\textsc{Luq} and its two variations, a\nseries of novel sampling-based UQ approaches specifically designed for long\ntext. Our findings reveal that \\textsc{Luq} outperforms existing baseline\nmethods in correlating with the model's factuality scores (negative coefficient\nof -0.85 observed for Gemini Pro). To further improve the factuality of LLM\nresponses, we propose \\textsc{Luq-Ensemble}, a method that ensembles responses\nfrom multiple models and selects the response with the lowest uncertainty. The\nensembling method greatly improves the response factuality upon the best\nstandalone LLM.\n","authors":["Caiqi Zhang","Fangyu Liu","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.20279v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03255v1","updated":"2024-10-04T09:18:54Z","published":"2024-10-04T09:18:54Z","title":"Towards a Benchmark for Large Language Models for Business Process\n  Management Tasks","summary":"  An increasing number of organizations are deploying Large Language Models\n(LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone\nto errors, ranging from inaccuracies to hallucinations. To objectively assess\nthe capabilities of existing LLMs, performance benchmarks are conducted.\nHowever, these benchmarks often do not translate to more specific real-world\ntasks. This paper addresses the gap in benchmarking LLM performance in the\nBusiness Process Management (BPM) domain. Currently, no BPM-specific benchmarks\nexist, creating uncertainty about the suitability of different LLMs for BPM\ntasks. This paper systematically compares LLM performance on four BPM tasks\nfocusing on small open-source models. The analysis aims to identify\ntask-specific performance variations, compare the effectiveness of open-source\nversus commercial models, and assess the impact of model size on BPM task\nperformance. This paper provides insights into the practical applications of\nLLMs in BPM, guiding organizations in selecting appropriate models for their\nspecific needs.\n","authors":["Kiran Busch","Henrik Leopold"],"pdf_url":"https://arxiv.org/pdf/2410.03255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03254v1","updated":"2024-10-04T09:17:09Z","published":"2024-10-04T09:17:09Z","title":"Are Expert-Level Language Models Expert-Level Annotators?","summary":"  Data annotation refers to the labeling or tagging of textual data with\nrelevant information. A large body of works have reported positive results on\nleveraging LLMs as an alternative to human annotators. However, existing\nstudies focus on classic NLP tasks, and the extent to which LLMs as data\nannotators perform in domains requiring expert knowledge remains underexplored.\nIn this work, we investigate comprehensive approaches across three highly\nspecialized domains and discuss practical suggestions from a cost-effectiveness\nperspective. To the best of our knowledge, we present the first systematic\nevaluation of LLMs as expert-level data annotators.\n","authors":["Yu-Min Tseng","Wei-Lin Chen","Chung-Chi Chen","Hsin-Hsi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03254v1.pdf","comment":"Accepted to WiML @ NeurIPS 2024 (extended version)"},{"id":"http://arxiv.org/abs/2410.02281v2","updated":"2024-10-04T09:16:02Z","published":"2024-10-03T08:03:40Z","title":"Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition","summary":"  The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.\n","authors":["Arthur Amalvy","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2410.02281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03249v1","updated":"2024-10-04T09:14:11Z","published":"2024-10-04T09:14:11Z","title":"How much can we forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we use experimental evidence and theoretical estimates to\nchallenge the common assumption that small-scale contamination renders\nbenchmark evaluations invalid. First, we experimentally quantify the magnitude\nof benchmark overfitting based on scaling along three dimensions: The number of\nmodel parameters (up to 1.6B), the number of times an example is seen (up to\n144), and the number of training tokens (up to 40B). We find that if model and\ndata follow the Chinchilla scaling laws, minor contamination indeed leads to\noverfitting. At the same time, even 144 times of contamination can be forgotten\nif the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. We then derive a simple theory of example\nforgetting via cumulative weight decay. It allows us to bound the number of\ngradient steps required to forget past data for any training run where we know\nthe hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,\nhave forgotten the data seen at the beginning of training. Experimentally, we\ndemonstrate that forgetting occurs faster than what is predicted by our bounds.\nTaken together, our results suggest that moderate amounts of contamination can\nbe forgotten at the end of realistically scaled training runs.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03240v1","updated":"2024-10-04T09:04:20Z","published":"2024-10-04T09:04:20Z","title":"Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken\n  Vocabulary?","summary":"  Word frequency is a key variable in psycholinguistics, useful for modeling\nhuman familiarity with words even in the era of large language models (LLMs).\nFrequency in film subtitles has proved to be a particularly good approximation\nof everyday language exposure. For many languages, however, film subtitles are\nnot easily available, or are overwhelmingly translated from English. We\ndemonstrate that frequencies extracted from carefully processed YouTube\nsubtitles provide an approximation comparable to, and often better than, the\nbest currently available resources. Moreover, they are available for languages\nfor which a high-quality subtitle or speech corpus does not exist. We use\nYouTube subtitles to construct frequency norms for five diverse languages,\nChinese, English, Indonesian, Japanese, and Spanish, and evaluate their\ncorrelation with lexical decision time, word familiarity, and lexical\ncomplexity. In addition to being strongly correlated with two psycholinguistic\nvariables, a simple linear regression on the new frequencies achieves a new\nhigh score on a lexical complexity prediction task in English and Japanese,\nsurpassing both models trained on film subtitle frequencies and the LLM GPT-4.\nOur code, the frequency lists, fastText word embeddings, and statistical\nlanguage models are freely available at https://github.com/naist-nlp/tubelex.\n","authors":["Adam Nohejl","Frederikus Hudi","Eunike Andriani Kardinata","Shintaro Ozaki","Maria Angelica Riera Machin","Hongyu Sun","Justin Vasselli","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.03240v1.pdf","comment":"Submitted for review to COLING 2025. 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.03234v1","updated":"2024-10-04T08:51:31Z","published":"2024-10-04T08:51:31Z","title":"Showing LLM-Generated Code Selectively Based on Confidence of LLMs","summary":"  Large Language Models (LLMs) have shown impressive abilities in code\ngeneration, but they may generate erroneous programs. Reading a program takes\nten times longer than writing it. Showing these erroneous programs to\ndevelopers will waste developers' energies and introduce security risks to\nsoftware.\n  To address the above limitations, we propose HonestCoder, a novel LLM-based\ncode generation approach. HonestCoder selectively shows the generated programs\nto developers based on LLMs' confidence. The confidence provides valuable\ninsights into the correctness of generated programs. To achieve this goal, we\npropose a novel approach to estimate LLMs' confidence in code generation. It\nestimates confidence by measuring the multi-modal similarity between\nLLMs-generated programs.\n  We collect and release a multilingual benchmark named TruthCodeBench, which\nconsists of 2,265 samples and covers two popular programming languages (i.e.,\nPython and Java). We apply HonestCoder to four popular LLMs (e.g.,\nDeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the\nexperiments, we obtain the following insights. (1) HonestCoder can effectively\nestimate LLMs' confidence and accurately determine the correctness of generated\nprograms. For example, HonestCoder outperforms the state-of-the-art baseline by\n27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of\nerroneous programs shown to developers. Compared to eight baselines, it can\nshow more correct programs and fewer erroneous programs to developers. (3)\nCompared to showing code indiscriminately, HonestCoder only adds slight time\noverhead (approximately 0.4 seconds per requirement). (4) We discuss future\ndirections to facilitate the application of LLMs in software development. We\nhope this work can motivate broad discussions about measuring the reliability\nof LLMs' outputs in performing code-related tasks.\n","authors":["Jia Li","Yuqi Zhu","Yongmin Li","Ge Li","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2410.03234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14247v2","updated":"2024-10-04T08:49:43Z","published":"2024-09-21T21:06:25Z","title":"Repairs in a Block World: A New Benchmark for Handling User Corrections\n  with Multi-Modal Language Models","summary":"  In dialogue, the addressee may initially misunderstand the speaker and\nrespond erroneously, often prompting the speaker to correct the\nmisunderstanding in the next turn with a Third Position Repair (TPR). The\nability to process and respond appropriately to such repair sequences is thus\ncrucial in conversational AI systems. In this paper, we first collect, analyse,\nand publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences\nin an instruction-following manipulation task that is, by design, rife with\nreferential ambiguity. We employ this dataset to evaluate several\nstate-of-the-art Vision and Language Models (VLM) across multiple settings,\nfocusing on their capability to process and accurately respond to TPRs and thus\nrecover from miscommunication. We find that, compared to humans, all models\nsignificantly underperform in this task. We then show that VLMs can benefit\nfrom specialised losses targeting relevant tokens during fine-tuning, achieving\nbetter performance and generalising better to new scenarios. Our results\nsuggest that these models are not yet ready to be deployed in multi-modal\ncollaborative settings where repairs are common, and highlight the need to\ndesign training regimes and objectives that facilitate learning from\ninteraction. Our code and data are available at\nwww.github.com/JChiyah/blockworld-repairs\n","authors":["Javier Chiyah-Garcia","Alessandro Suglia","Arash Eshghi"],"pdf_url":"https://arxiv.org/pdf/2409.14247v2.pdf","comment":"Accepted to EMNLP'24 Main (Upcoming). Data and code at\n  www.github.com/JChiyah/blockworld-repairs - for Bibtex see\n  https://raw.githubusercontent.com/JChiyah/blockworld-repairs/refs/heads/main/citation.bib"},{"id":"http://arxiv.org/abs/2409.19715v2","updated":"2024-10-04T08:48:56Z","published":"2024-09-29T14:14:25Z","title":"Coffee-Gym: An Environment for Evaluating and Improving Natural Language\n  Feedback on Erroneous Code","summary":"  This paper presents Coffee-Gym, a comprehensive RL environment for training\nmodels that provide feedback on code editing. Coffee-Gym includes two major\ncomponents: (1) Coffee, a dataset containing humans' code edit traces for\ncoding questions and machine-written feedback for editing erroneous code; (2)\nCoffeeEval, a reward function that faithfully reflects the helpfulness of\nfeedback by assessing the performance of the revised code in unit tests. With\nthem, Coffee-Gym addresses the unavailability of high-quality datasets for\ntraining feedback models with RL, and provides more accurate rewards than the\nSOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback\nmodels that outperform baselines in enhancing open-source code LLMs' code\nediting, making them comparable with closed-source LLMs. We make the dataset\nand the model checkpoint publicly available.\n","authors":["Hyungjoo Chae","Taeyoon Kwon","Seungjun Moon","Yongho Song","Dongjin Kang","Kai Tzu-iunn Ong","Beong-woo Kwak","Seonghyeon Bae","Seung-won Hwang","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2409.19715v2.pdf","comment":"EMNLP2024"},{"id":"http://arxiv.org/abs/2403.16820v2","updated":"2024-10-04T08:44:54Z","published":"2024-03-25T14:46:51Z","title":"Cross-lingual Contextualized Phrase Retrieval","summary":"  Phrase-level dense retrieval has shown many appealing characteristics in\ndownstream NLP tasks by leveraging the fine-grained information that phrases\noffer. In our work, we propose a new task formulation of dense retrieval,\ncross-lingual contextualized phrase retrieval, which aims to augment\ncross-lingual applications by addressing polysemy using context information.\nHowever, the lack of specific training data and models are the primary\nchallenges to achieve our goal. As a result, we extract pairs of cross-lingual\nphrases using word alignment information automatically induced from parallel\nsentences. Subsequently, we train our Cross-lingual Contextualized Phrase\nRetriever (CCPR) using contrastive learning, which encourages the hidden\nrepresentations of phrases with similar contexts and semantics to align\nclosely. Comprehensive experiments on both the cross-lingual phrase retrieval\ntask and a downstream task, i.e, machine translation, demonstrate the\neffectiveness of CCPR. On the phrase retrieval task, CCPR surpasses baselines\nby a significant margin, achieving a top-1 accuracy that is at least 13 points\nhigher. When utilizing CCPR to augment the large-language-model-based\ntranslator, it achieves average gains of 0.7 and 1.5 in BERTScore for\ntranslations from X=>En and vice versa, respectively, on WMT16 dataset. Our\ncode and data are available at \\url{https://github.com/ghrua/ccpr_release}.\n","authors":["Huayang Li","Deng Cai","Zhi Qu","Qu Cui","Hidetaka Kamigaito","Lemao Liu","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.16820v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03227v1","updated":"2024-10-04T08:29:12Z","published":"2024-10-04T08:29:12Z","title":"ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question\n  Answering","summary":"  The context window of large language models (LLMs) has been extended\nsignificantly in recent years. However, while the context length that the LLM\ncan process has grown, the capability of the model to accurately reason over\nthat context degrades noticeably. This occurs because modern LLMs often become\noverwhelmed by the vast amount of information in the context; when answering\nquestions, the model must identify and reason over relevant evidence sparsely\ndistributed throughout the text. To alleviate the challenge of long-context\nreasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason\nover relevant evidence collected during an intermediate retrieval step. We find\nthat modern LLMs struggle to accurately retrieve relevant facts and instead,\noften hallucinate \"retrieved facts\", resulting in flawed reasoning and the\nproduction of incorrect answers. To address these issues, we introduce ALR$^2$,\na method that augments the long-context reasoning capability of LLMs via an\nexplicit two-stage procedure, i.e., aligning LLMs with the objectives of both\nretrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating\nperformance degradation in long-context reasoning tasks. Through extensive\nexperiments on long-context QA benchmarks, we find our method to outperform\ncompetitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains\non the long-context versions of HotpotQA and SQuAD datasets, respectively.\n","authors":["Huayang Li","Pat Verga","Priyanka Sen","Bowen Yang","Vijay Viswanathan","Patrick Lewis","Taro Watanabe","Yixuan Su"],"pdf_url":"https://arxiv.org/pdf/2410.03227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v1","updated":"2024-10-04T08:26:06Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xioalei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.03223v1","updated":"2024-10-04T08:22:16Z","published":"2024-10-04T08:22:16Z","title":"Consultation on Industrial Machine Faults with Large language Models","summary":"  Industrial machine fault diagnosis is a critical component of operational\nefficiency and safety in manufacturing environments. Traditional methods rely\nheavily on expert knowledge and specific machine learning models, which can be\nlimited in their adaptability and require extensive labeled data. This paper\nintroduces a novel approach leveraging Large Language Models (LLMs),\nspecifically through a structured multi-round prompting technique, to improve\nfault diagnosis accuracy. By dynamically crafting prompts, our method enhances\nthe model's ability to synthesize information from diverse data sources,\nleading to improved contextual understanding and actionable recommendations.\nExperimental results demonstrate that our approach outperforms baseline models,\nachieving an accuracy of 91% in diagnosing various fault types. The findings\nunderscore the potential of LLMs in revolutionizing industrial fault\nconsultation practices, paving the way for more effective maintenance\nstrategies in complex environments.\n","authors":["Apiradee Boonmee","Kritsada Wongsuwan","Pimchanok Sukjai"],"pdf_url":"https://arxiv.org/pdf/2410.03223v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.03215v1","updated":"2024-10-04T08:02:43Z","published":"2024-10-04T08:02:43Z","title":"NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task","summary":"  In this paper, we describe our system for the WMT 24 shared task of\nLow-Resource Indic Language Translation. We consider eng $\\leftrightarrow$ {as,\nkha, lus, mni} as participating language pairs. In this shared task, we explore\nthe finetuning of a pre-trained model motivated by the pre-trained objective of\naligning embeddings closer by alignment augmentation \\cite{lin-etal-2020-pre}\nfor 22 scheduled Indian languages. Our primary system is based on\nlanguage-specific finetuning on a pre-trained model. We achieve chrF2 scores of\n50.6, 42.3, 54.9, and 66.3 on the official public test set for\neng$\\rightarrow$as, eng$\\rightarrow$kha, eng$\\rightarrow$lus,\neng$\\rightarrow$mni respectively. We also explore multilingual training\nwith/without language grouping and layer-freezing. Our code, models, and\ngenerated translations are available here:\nhttps://github.com/pramitsahoo/WMT2024-LRILT.\n","authors":["Pramit Sahoo","Maharaj Brahma","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2410.03215v1.pdf","comment":"WMT2024 INDICMT Shared Task"},{"id":"http://arxiv.org/abs/2406.20015v2","updated":"2024-10-04T07:51:29Z","published":"2024-06-28T16:03:30Z","title":"ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models","summary":"  Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community has yet\nto fully understand the hallucination issues within these models. To address\nthis challenge, we introduce a comprehensive diagnostic benchmark, ToolBH.\nSpecifically, we assess the LLM's hallucinations through two perspectives:\ndepth and breadth. In terms of depth, we propose a multi-level diagnostic\nprocess, including (1) solvability detection, (2) solution planning, and (3)\nmissing-tool analysis. For breadth, we consider three scenarios based on the\ncharacteristics of the toolset: missing necessary tools, potential tools, and\nlimited functionality tools. Furthermore, we developed seven tasks and\ncollected 700 evaluation samples through multiple rounds of manual annotation.\nThe results show the significant challenges presented by the ToolBH benchmark.\nThe current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores\nof 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger\nmodel parameters do not guarantee better performance; the training data and\nresponse strategies also play crucial roles in tool-enhanced LLM scenarios. Our\ndiagnostic analysis indicates that the primary reason for model errors lies in\nassessing task solvability. Additionally, open-weight models suffer from\nperformance drops with verbose replies, whereas proprietary models excel with\nlonger reasoning.\n","authors":["Yuxiang Zhang","Jing Chen","Junjie Wang","Yaxin Liu","Cheng Yang","Chufan Shi","Xinyu Zhu","Zihao Lin","Hanwen Wan","Yujiu Yang","Tetsuya Sakai","Tian Feng","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2406.20015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07867v3","updated":"2024-10-04T07:45:22Z","published":"2024-01-15T17:57:41Z","title":"Authorship Obfuscation in Multilingual Machine-Generated Text Detection","summary":"  High-quality text generation capability of recent Large Language Models\n(LLMs) causes concerns about their misuse (e.g., in massive generation/spread\nof disinformation). Machine-generated text (MGT) detection is important to cope\nwith such threats. However, it is susceptible to authorship obfuscation (AO)\nmethods, such as paraphrasing, which can cause MGTs to evade detection. So far,\nthis was evaluated only in monolingual settings. Thus, the susceptibility of\nrecently proposed multilingual detectors is still unknown. We fill this gap by\ncomprehensively benchmarking the performance of 10 well-known AO methods,\nattacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10\n$\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of\ndata augmentation on adversarial robustness using obfuscated texts. The results\nindicate that all tested AO methods can cause evasion of automated detection in\nall tested languages, where homoglyph attacks are especially successful.\nHowever, some of the AO methods severely damaged the text, making it no longer\nreadable or easily recognizable by humans (e.g., changed language, weird\ncharacters).\n","authors":["Dominik Macko","Robert Moro","Adaku Uchendu","Ivan Srba","Jason Samuel Lucas","Michiharu Yamashita","Nafis Irtiza Tripto","Dongwon Lee","Jakub Simko","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2401.07867v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.14672v2","updated":"2024-10-04T07:40:57Z","published":"2024-02-22T16:18:07Z","title":"Middleware for LLMs: Tools Are Instrumental for Language Agents in\n  Complex Environments","summary":"  The applications of large language models (LLMs) have expanded well beyond\nthe confines of text processing, signaling a new era where LLMs are envisioned\nas generalist agents capable of operating within complex environments. These\nenvironments are often highly expansive, making it impossible for the LLM to\nprocess them within its short-term memory. Motivated by recent research on\nextending the capabilities of LLMs with tools, we seek to investigate the\nintriguing potential of tools to augment LLMs in handling such complexity by\nintroducing a novel class of tools, termed middleware, to aid in the proactive\nexploration within these massive environments. Such specialized tools can serve\nas a middleware layer shielding the LLM from environmental complexity. In two\nrepresentative complex environments -- knowledge bases (KBs) and databases --\nwe demonstrate the significant potential of augmenting language agents with\ntools in complex environments. Notably, equipped with the middleware, GPT-4\nachieves 2.8X the performance of the best baseline in tasks requiring access to\ndatabase content and 2.2X in KB tasks. Our findings illuminate the path for\nadvancing language agents in real-world applications.\n","authors":["Yu Gu","Yiheng Shu","Hao Yu","Xiao Liu","Yuxiao Dong","Jie Tang","Jayanth Srinivasa","Hugo Latapie","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2402.14672v2.pdf","comment":"EMNLP'2024; 18 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.03203v1","updated":"2024-10-04T07:39:34Z","published":"2024-10-04T07:39:34Z","title":"Learning Semantic Structure through First-Order-Logic Translation","summary":"  In this paper, we study whether transformer-based language models can extract\npredicate argument structure from simple sentences. We firstly show that\nlanguage models sometimes confuse which predicates apply to which objects. To\nmitigate this, we explore two tasks: question answering (Q/A), and first order\nlogic (FOL) translation, and two regimes, prompting and finetuning. In FOL\ntranslation, we finetune several large language models on synthetic datasets\ndesigned to gauge their generalization abilities. For Q/A, we finetune encoder\nmodels like BERT and RoBERTa and use prompting for LLMs. The results show that\nFOL translation for LLMs is better suited to learn predicate argument\nstructure.\n","authors":["Akshay Chaturvedi","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2410.03203v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03198v1","updated":"2024-10-04T07:29:41Z","published":"2024-10-04T07:29:41Z","title":"PersoBench: Benchmarking Personalized Response Generation in Large\n  Language Models","summary":"  While large language models (LLMs) have exhibited impressive conversational\ncapabilities, their proficiency in delivering personalized responses remains\nunclear. Although recent benchmarks automatically evaluate persona consistency\nin role-playing contexts using LLM-based judgment, the evaluation of\npersonalization in response generation remains underexplored. To address this\ngap, we present a new benchmark, PersoBench, to evaluate the personalization\nability of LLMs in persona-aware dialogue generation within a zero-shot\nsetting. We assess the performance of three open-source and three closed-source\nLLMs using well-known datasets and a range of metrics. Our analysis, conducted\non three well-known persona-aware datasets, evaluates multiple dimensions of\nresponse quality, including fluency, diversity, coherence, and personalization,\nacross both standard and chain-of-thought prompting methods. Our findings\nreveal that while LLMs excel at generating fluent and diverse responses, they\nare far from satisfactory in delivering personalized and coherent responses\nconsidering both the conversation context and the provided personas. Our\nbenchmark implementation is available at\nhttps://github.com/salehafzoon/PersoBench.\n","authors":["Saleh Afzoon","Usman Naseem","Amin Beheshti","Zahra Jamali"],"pdf_url":"https://arxiv.org/pdf/2410.03198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03197v1","updated":"2024-10-04T07:29:35Z","published":"2024-10-04T07:29:35Z","title":"Cross-lingual Transfer for Automatic Question Generation by Learning\n  Interrogative Structures in Target Languages","summary":"  Automatic question generation (QG) serves a wide range of purposes, such as\naugmenting question-answering (QA) corpora, enhancing chatbot systems, and\ndeveloping educational materials. Despite its importance, most existing\ndatasets predominantly focus on English, resulting in a considerable gap in\ndata availability for other languages. Cross-lingual transfer for QG (XLT-QG)\naddresses this limitation by allowing models trained on high-resource language\ndatasets to generate questions in low-resource languages. In this paper, we\npropose a simple and efficient XLT-QG method that operates without the need for\nmonolingual, parallel, or labeled data in the target language, utilizing a\nsmall language model. Our model, trained solely on English QA datasets, learns\ninterrogative structures from a limited set of question exemplars, which are\nthen applied to generate questions in the target language. Experimental results\nshow that our method outperforms several XLT-QG baselines and achieves\nperformance comparable to GPT-3.5-turbo across different languages.\nAdditionally, the synthetic data generated by our model proves beneficial for\ntraining multilingual QA models. With significantly fewer parameters than large\nlanguage models and without requiring additional training for target languages,\nour approach offers an effective solution for QG and QA tasks across various\nlanguages.\n","authors":["Seonjeong Hwang","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.03197v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.15468v2","updated":"2024-10-04T07:29:29Z","published":"2024-06-15T05:35:47Z","title":"MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large\n  Language Models","summary":"  We propose MMLU-SR, a novel dataset designed to measure the true\ncomprehension abilities of Large Language Models (LLMs) by challenging their\nperformance in question-answering tasks with modified terms. We reasoned that\nan agent that \"truly\" understands a concept can still evaluate it when key\nterms are replaced by suitably defined alternate terms, and sought to\ndifferentiate such comprehension from mere text replacement. In our study, we\nmodified standardized test questions by replacing a key term with a dummy word\nalong with its definition. The key term could be in the context of questions,\nanswers, or both questions and answers. Notwithstanding the high scores\nachieved by recent popular LLMs on the MMLU leaderboard, we found a substantial\nreduction in model performance after such replacement, suggesting poor\ncomprehension. This new benchmark provides a rigorous benchmark for testing\ntrue model comprehension, and poses a challenge to the broader scientific\ncommunity.\n","authors":["Wentian Wang","Sarthak Jain","Paul Kantor","Jacob Feldman","Lazaros Gallos","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07900v3","updated":"2024-10-04T07:27:53Z","published":"2024-04-11T16:39:00Z","title":"High-Dimension Human Value Representation in Large Language Models","summary":"  The widespread application of Large Language Models (LLMs) across various\ntasks and fields has necessitated the alignment of these models with human\nvalues and preferences. Given various approaches of human value alignment,\nranging from Reinforcement Learning with Human Feedback (RLHF), to\nconstitutional learning, etc. there is an urgent need to understand the scope\nand nature of human values injected into these models before their release.\nThere is also a need for model alignment without a costly large scale human\nannotation effort. We propose UniVaR, a high-dimensional representation of\nhuman value distributions in LLMs, orthogonal to model architecture and\ntraining data. Trained from the value-relevant output of eight multilingual\nLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,\nChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the\ndistribution of human values embedded in different LLMs with different langauge\nsources. Through UniVaR, we explore how different LLMs prioritize various\nvalues in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling.\n","authors":["Samuel Cahyawijaya","Delong Chen","Yejin Bang","Leila Khalatbari","Bryan Wilie","Ziwei Ji","Etsuko Ishii","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2404.07900v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04280v2","updated":"2024-10-04T07:25:18Z","published":"2024-07-05T06:25:54Z","title":"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous\n  Speech","summary":"  Prevalent ungrammatical expressions and disfluencies in spontaneous speech\nfrom second language (L2) learners pose unique challenges to Automatic Speech\nRecognition (ASR) systems. However, few datasets are tailored to L2 learner\nspeech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours\nof audio and transcriptions of L2 learners' spontaneous speech. Our linguistic\nanalysis reveals that transcriptions in our dataset contain L2S (L2 learner's\nSpontaneous speech) features, consisting of ungrammatical expressions and\ndisfluencies (e.g., filler words, word repetitions, self-repairs, false\nstarts), significantly more than native speech datasets. Fine-tuning\nwhisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than\nvanilla whisper-small.en. Furthermore, our qualitative analysis indicates that\n54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S\nfeatures, with 48.1% of them being reduced in the fine-tuned model.\n","authors":["Haechan Kim","Junho Myung","Seoyoung Kim","Sungpah Lee","Dongyeop Kang","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2407.04280v2.pdf","comment":"Proceedings of Interspeech"},{"id":"http://arxiv.org/abs/2410.03194v1","updated":"2024-10-04T07:15:07Z","published":"2024-10-04T07:15:07Z","title":"Parallel Corpus Augmentation using Masked Language Models","summary":"  In this paper we propose a novel method of augmenting parallel text corpora\nwhich promises good quality and is also capable of producing many fold larger\ncorpora than the seed corpus we start with. We do not need any additional\nmonolingual corpora. We use Multi-Lingual Masked Language Model to mask and\npredict alternative words in context and we use Sentence Embeddings to check\nand select sentence pairs which are likely to be translations of each other. We\ncross check our method using metrics for MT Quality Estimation. We believe this\nmethod can greatly alleviate the data scarcity problem for all language pairs\nfor which a reasonable seed corpus is available.\n","authors":["Vibhuti Kumari","Narayana Murthy Kavi"],"pdf_url":"https://arxiv.org/pdf/2410.03194v1.pdf","comment":"21 Pages, 3 Figures. arXiv admin note: text overlap with\n  arXiv:2011.01536 by other authors"},{"id":"http://arxiv.org/abs/2410.02185v2","updated":"2024-10-04T07:00:03Z","published":"2024-10-03T04:01:14Z","title":"POSIX: A Prompt Sensitivity Index For Large Language Models","summary":"  Despite their remarkable capabilities, Large Language Models (LLMs) are found\nto be surprisingly sensitive to minor variations in prompts, often generating\nsignificantly divergent outputs in response to minor variations in the prompts,\nsuch as spelling errors, alteration of wording or the prompt template. However,\nwhile assessing the quality of an LLM, the focus often tends to be solely on\nits performance on downstream tasks, while very little to no attention is paid\nto prompt sensitivity. To fill this gap, we propose POSIX - a novel PrOmpt\nSensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering\na more comprehensive evaluation of LLM performance. The key idea behind POSIX\nis to capture the relative change in loglikelihood of a given response upon\nreplacing the corresponding prompt with a different intent-preserving prompt.\nWe provide thorough empirical evidence demonstrating the efficacy of POSIX in\ncapturing prompt sensitivity and subsequently use it to measure and thereby\ncompare prompt sensitivity of various open-source LLMs. We find that merely\nincreasing the parameter count or instruction tuning does not necessarily\nreduce prompt sensitivity whereas adding some few-shot exemplars, even just\none, almost always leads to significant decrease in prompt sensitivity. We also\nfind that alterations to prompt template lead to the highest sensitivity in the\ncase of MCQ type tasks, whereas paraphrasing results in the highest sensitivity\nin open-ended generation tasks. The code for reproducing our results is\nopen-sourced at https://github.com/kowndinya-renduchintala/POSIX.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2410.02185v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.03182v1","updated":"2024-10-04T06:45:48Z","published":"2024-10-04T06:45:48Z","title":"Generating bilingual example sentences with large language models as\n  lexicography assistants","summary":"  We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.\n","authors":["Raphael Merx","Ekaterina Vylomova","Kemal Kurniawan"],"pdf_url":"https://arxiv.org/pdf/2410.03182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15000v2","updated":"2024-10-04T06:44:10Z","published":"2024-02-22T22:28:46Z","title":"Divide-or-Conquer? Which Part Should You Distill Your LLM?","summary":"  Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.\n","authors":["Zhuofeng Wu","He Bai","Aonan Zhang","Jiatao Gu","VG Vinod Vydiswaran","Navdeep Jaitly","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15000v2.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03181v1","updated":"2024-10-04T06:38:38Z","published":"2024-10-04T06:38:38Z","title":"Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large\n  Language Models with Assigned Visual Personas","summary":"  This study is the first to explore whether multi-modal large language models\n(LLMs) can align their behaviors with visual personas, addressing a significant\ngap in the literature that predominantly focuses on text-based personas. We\ndeveloped a novel dataset of 5K fictional avatar images for assignment as\nvisual personas to LLMs, and analyzed their negotiation behaviors based on the\nvisual traits depicted in these images, with a particular focus on\naggressiveness. The results indicate that LLMs assess the aggressiveness of\nimages in a manner similar to humans and output more aggressive negotiation\nbehaviors when prompted with an aggressive visual persona. Interestingly, the\nLLM exhibited more aggressive negotiation behaviors when the opponent's image\nappeared less aggressive than their own, and less aggressive behaviors when the\nopponents image appeared more aggressive.\n","authors":["Seungjong Sun","Eungu Lee","Seo Yeon Baek","Seunghyun Hwang","Wonbyung Lee","Dongyan Nan","Bernard J. Jansen","Jang Hyun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.03181v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.06034v6","updated":"2024-10-04T06:32:07Z","published":"2024-01-11T16:48:00Z","title":"LinguAlchemy: Fusing Typological and Geographical Elements for Unseen\n  Language Generalization","summary":"  Pretrained language models (PLMs) have become remarkably adept at task and\nlanguage generalization. Nonetheless, they often fail when faced with unseen\nlanguages. In this work, we present LinguAlchemy, a regularization method that\nincorporates various linguistic information covering typological, geographical,\nand phylogenetic features to align PLMs representation to the corresponding\nlinguistic information on each language. Our LinguAlchemy significantly\nimproves the performance of mBERT and XLM-R on low-resource languages in\nmultiple downstream tasks such as intent classification, news classification,\nand semantic relatedness compared to fully finetuned models and displaying a\nhigh degree of unseen language generalization. We further introduce\nAlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the\nlinguistic regularization weights automatically, alleviating the need for\nhyperparameter search.\n","authors":["Muhammad Farid Adilazuarda","Samuel Cahyawijaya","Alham Fikri Aji","Genta Indra Winata","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2401.06034v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03000v2","updated":"2024-10-04T06:31:47Z","published":"2024-05-05T17:06:31Z","title":"MedAdapter: Efficient Test-Time Adaptation of Large Language Models\n  towards Medical Reasoning","summary":"  Despite their improved capabilities in generation and reasoning, adapting\nlarge language models (LLMs) to the biomedical domain remains challenging due\nto their immense size and corporate privacy. In this work, we propose\nMedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards\nbiomedical applications. Instead of fine-tuning the entire LLM, MedAdapter\neffectively adapts the original model by fine-tuning only a small BERT-sized\nadapter to rank candidate solutions generated by LLMs. Experiments demonstrate\nthat MedAdapter effectively adapts both white-box and black-box LLMs in\nbiomedical reasoning, achieving average performance improvements of 25.48% and\n11.31%, respectively, without requiring extensive computational resources or\nsharing data with third parties. MedAdapter also yields superior performance\nwhen combined with train-time adaptation, highlighting a flexible and\ncomplementary solution to existing adaptation methods. Faced with the\nchallenges of balancing model performance, computational resources, and data\nprivacy, MedAdapter provides an efficient, privacy-preserving, cost-effective,\nand transparent solution for adapting LLMs to the biomedical domain.\n","authors":["Wenqi Shi","Ran Xu","Yuchen Zhuang","Yue Yu","Haotian Sun","Hang Wu","Carl Yang","May D. Wang"],"pdf_url":"https://arxiv.org/pdf/2405.03000v2.pdf","comment":"Accepted in EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2406.12016v2","updated":"2024-10-04T06:26:20Z","published":"2024-06-17T18:33:44Z","title":"Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization","summary":"  Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.\n","authors":["Seungwoo Son","Wonpyo Park","Woohyun Han","Kyuyeun Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2406.12016v2.pdf","comment":"EMNLP 2024 Main (Long)"},{"id":"http://arxiv.org/abs/2406.14026v2","updated":"2024-10-04T06:18:15Z","published":"2024-06-20T06:46:23Z","title":"Demystifying Language Model Forgetting with Low-rank Example\n  Associations","summary":"  Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/\n","authors":["Xisen Jin","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2406.14026v2.pdf","comment":"9 pages; preprint"},{"id":"http://arxiv.org/abs/2409.15202v2","updated":"2024-10-04T06:09:15Z","published":"2024-09-23T16:49:47Z","title":"ASTE Transformer Modelling Dependencies in Aspect-Sentiment Triplet\n  Extraction","summary":"  Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of\naspect-based sentiment analysis that consists in extracting (aspect phrase,\nopinion phrase, sentiment polarity) triples from a given sentence. Recent\nstate-of-the-art methods approach this task by first extracting all possible\ntext spans from a given text, then filtering the potential aspect and opinion\nphrases with a classifier, and finally considering all their pairs with another\nclassifier that additionally assigns sentiment polarity to them. Although\nseveral variations of the above scheme have been proposed, the common feature\nis that the final result is constructed by a sequence of independent classifier\ndecisions. This hinders the exploitation of dependencies between extracted\nphrases and prevents the use of knowledge about the interrelationships between\nclassifier predictions to improve performance. In this paper, we propose a new\nASTE approach consisting of three transformer-inspired layers, which enables\nthe modelling of dependencies both between phrases and between the final\nclassifier decisions. Experimental results show that the method achieves higher\nperformance in terms of F1 measure than other methods studied on popular\nbenchmarks. In addition, we show that a simple pre-training technique further\nimproves the performance of the model.\n","authors":["Iwo Naglik","Mateusz Lango"],"pdf_url":"https://arxiv.org/pdf/2409.15202v2.pdf","comment":"The 2024 Conference on Empirical Methods in Natural Language\n  Processing, November 12-16, Miami, Florida 9 pages, appendix, diagrams"},{"id":"http://arxiv.org/abs/2410.03170v1","updated":"2024-10-04T06:05:17Z","published":"2024-10-04T06:05:17Z","title":"Autoregressive Large Language Models are Computationally Universal","summary":"  We show that autoregressive decoding of a transformer-based language model\ncan realize universal computation, without external intervention or\nmodification of the model's weights. Establishing this result requires\nunderstanding how a language model can process arbitrarily long inputs using a\nbounded context. For this purpose, we consider a generalization of\nautoregressive decoding where, given a long input, emitted tokens are appended\nto the end of the sequence as the context window advances. We first show that\nthe resulting system corresponds to a classical model of computation, a Lag\nsystem, that has long been known to be computationally universal. By leveraging\na new proof, we show that a universal Turing machine can be simulated by a Lag\nsystem with 2027 production rules. We then investigate whether an existing\nlarge language model can simulate the behaviour of such a universal Lag system.\nWe give an affirmative answer by showing that a single system-prompt can be\ndeveloped for gemini-1.5-pro-001 that drives the model, under deterministic\n(greedy) decoding, to correctly apply each of the 2027 production rules. We\nconclude that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with\nextended autoregressive (greedy) decoding is a general purpose computer.\n","authors":["Dale Schuurmans","Hanjun Dai","Francesco Zanini"],"pdf_url":"https://arxiv.org/pdf/2410.03170v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2410.03168v1","updated":"2024-10-04T06:01:27Z","published":"2024-10-04T06:01:27Z","title":"Can Watermarked LLMs be Identified by Users via Crafted Prompts?","summary":"  Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.\n","authors":["Aiwei Liu","Sheng Guan","Yiming Liu","Leyi Pan","Yifei Zhang","Liancheng Fang","Lijie Wen","Philip S. Yu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.03168v1.pdf","comment":"25 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.07128v3","updated":"2024-10-04T05:56:56Z","published":"2024-01-13T18:09:05Z","title":"EHRAgent: Code Empowers Large Language Models for Few-shot Complex\n  Tabular Reasoning on Electronic Health Records","summary":"  Large language models (LLMs) have demonstrated exceptional capabilities in\nplanning and tool utilization as autonomous agents, but few have been developed\nfor medical problem-solving. We propose EHRAgent, an LLM agent empowered with a\ncode interface, to autonomously generate and execute code for multi-tabular\nreasoning within electronic health records (EHRs). First, we formulate an EHR\nquestion-answering task into a tool-use planning process, efficiently\ndecomposing a complicated task into a sequence of manageable actions. By\nintegrating interactive coding and execution feedback, EHRAgent learns from\nerror messages and improves the originally generated code through iterations.\nFurthermore, we enhance the LLM agent by incorporating long-term memory, which\nallows EHRAgent to effectively select and build upon the most relevant\nsuccessful cases from past experiences. Experiments on three real-world\nmulti-tabular EHR datasets show that EHRAgent outperforms the strongest\nbaseline by up to 29.6% in success rate. EHRAgent leverages the emerging\nfew-shot learning capabilities of LLMs, enabling autonomous code generation and\nexecution to tackle complex clinical tasks with minimal demonstrations.\n","authors":["Wenqi Shi","Ran Xu","Yuchen Zhuang","Yue Yu","Jieyu Zhang","Hang Wu","Yuanda Zhu","Joyce Ho","Carl Yang","May D. Wang"],"pdf_url":"https://arxiv.org/pdf/2401.07128v3.pdf","comment":"Accepted in EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2403.15744v6","updated":"2024-10-04T05:56:18Z","published":"2024-03-23T07:16:23Z","title":"On the Fragility of Active Learners for Text Classification","summary":"  Active learning (AL) techniques optimally utilize a labeling budget by\niteratively selecting instances that are most valuable for learning. However,\nthey lack ``prerequisite checks'', i.e., there are no prescribed criteria to\npick an AL algorithm best suited for a dataset. A practitioner must pick a\ntechnique they \\emph{trust} would beat random sampling, based on prior reported\nresults, and hope that it is resilient to the many variables in their\nenvironment: dataset, labeling budget and prediction pipelines. The important\nquestions then are: how often on average, do we expect any AL technique to\nreliably beat the computationally cheap and easy-to-implement strategy of\nrandom sampling? Does it at least make sense to use AL in an ``Always ON'' mode\nin a prediction pipeline, so that while it might not always help, it never\nunder-performs random sampling? How much of a role does the prediction pipeline\nplay in AL's success?\n  We examine these questions in detail for the task of text classification\nusing pre-trained representations, which are ubiquitous today.\n  Our primary contribution here is a rigorous evaluation of AL techniques, old\nand new, across setups that vary wrt datasets, text representations and\nclassifiers. This unlocks multiple insights around warm-up times, i.e., number\nof labels before gains from AL are seen, viability of an ``Always ON'' mode and\nthe relative significance of different factors. Additionally, we release a\nframework for rigorous benchmarking of AL techniques for text classification.\n","authors":["Abhishek Ghose","Emma Thuong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15744v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.14187v2","updated":"2024-10-04T05:52:42Z","published":"2022-03-27T02:21:19Z","title":"Educational Question Generation of Children Storybooks via Question Type\n  Distribution Learning and Event-Centric Summarization","summary":"  Generating educational questions of fairytales or storybooks is vital for\nimproving children's literacy ability. However, it is challenging to generate\nquestions that capture the interesting aspects of a fairytale story with\neducational meaningfulness. In this paper, we propose a novel question\ngeneration method that first learns the question type distribution of an input\nstory paragraph, and then summarizes salient events which can be used to\ngenerate high-cognitive-demand questions. To train the event-centric\nsummarizer, we finetune a pre-trained transformer-based sequence-to-sequence\nmodel using silver samples composed by educational question-answer pairs. On a\nnewly proposed educational question answering dataset FairytaleQA, we show good\nperformance of our method on both automatic and human evaluation metrics. Our\nwork indicates the necessity of decomposing question type distribution learning\nand event-centric summary generation for educational question generation.\n","authors":["Zhenjie Zhao","Yufang Hou","Dakuo Wang","Mo Yu","Chengzhong Liu","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2203.14187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12168v3","updated":"2024-10-04T05:41:08Z","published":"2024-06-18T00:41:40Z","title":"BPO: Staying Close to the Behavior LLM Creates Better Online LLM\n  Alignment","summary":"  Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.\n","authors":["Wenda Xu","Jiachen Li","William Yang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2406.12168v3.pdf","comment":"Wenda Xu and Jiachen Li contributed equally. Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2311.09756v3","updated":"2024-10-04T05:39:32Z","published":"2023-11-16T10:30:26Z","title":"StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning","summary":"  Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v3.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.07087v2","updated":"2024-10-04T05:35:57Z","published":"2024-07-09T17:58:18Z","title":"CopyBench: Measuring Literal and Non-Literal Reproduction of\n  Copyright-Protected Text in Language Model Generation","summary":"  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2\\% to 10.5\\% and non-literal copying from\n2.3\\% to 5.9\\% when comparing Llama3-8B and 70B models, respectively. We\nfurther evaluate the effectiveness of current strategies for mitigating copying\nand show that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n","authors":["Tong Chen","Akari Asai","Niloofar Mireshghallah","Sewon Min","James Grimmelmann","Yejin Choi","Hannaneh Hajishirzi","Luke Zettlemoyer","Pang Wei Koh"],"pdf_url":"https://arxiv.org/pdf/2407.07087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06648v3","updated":"2024-10-04T05:33:51Z","published":"2023-12-11T18:57:35Z","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","summary":"  Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our experiments reveal that\nindexing a corpus by fine-grained units such as propositions significantly\noutperforms passage-level units in retrieval tasks. Moreover, constructing\nprompts with fine-grained retrieved units for retrieval-augmented language\nmodels improves the performance of downstream QA tasks given a specific\ncomputation budget.\n","authors":["Tong Chen","Hongwei Wang","Sihao Chen","Wenhao Yu","Kaixin Ma","Xinran Zhao","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03154v1","updated":"2024-10-04T05:29:51Z","published":"2024-10-04T05:29:51Z","title":"Exploring Learnability in Memory-Augmented Recurrent Neural Networks:\n  Precision, Stability, and Empirical Insights","summary":"  This study explores the learnability of memory-less and memory-augmented\nRNNs, which are theoretically equivalent to Pushdown Automata. Empirical\nresults show that these models often fail to generalize on longer sequences,\nrelying more on precision than mastering symbolic grammar. Experiments on fully\ntrained and component-frozen models reveal that freezing the memory component\nsignificantly improves performance, achieving state-of-the-art results on the\nPenn Treebank dataset (test perplexity reduced from 123.5 to 120.5). Models\nwith frozen memory retained up to 90% of initial performance on longer\nsequences, compared to a 60% drop in standard models. Theoretical analysis\nsuggests that freezing memory stabilizes temporal dependencies, leading to\nrobust convergence. These findings stress the need for stable memory designs\nand long-sequence evaluations to understand RNNs true learnability limits.\n","authors":["Shrabon Das","Ankur Mali"],"pdf_url":"https://arxiv.org/pdf/2410.03154v1.pdf","comment":"21 pages, 4 theorems, 5 tables"},{"id":"http://arxiv.org/abs/2403.04224v4","updated":"2024-10-04T05:29:18Z","published":"2024-03-07T04:54:56Z","title":"Aligners: Decoupling LLMs and Alignment","summary":"  Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We use the same synthetic data to train inspectors, binary\nmiss-alignment classification models to guide a \"squad\" of multiple aligners.\nOur empirical results demonstrate consistent improvements when applying aligner\nsquad to various LLMs, including chat-aligned models, across several\ninstruction-following and red-teaming datasets.\n","authors":["Lilian Ngweta","Mayank Agarwal","Subha Maity","Alex Gittens","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2403.04224v4.pdf","comment":"Short version accepted as a Tiny Paper at the International\n  Conference on Learning Representations (ICLR) 2024. Long version accepted to\n  the Conference on Empirical Methods in Natural Language Processing (EMNLP)\n  2024 Findings"},{"id":"http://arxiv.org/abs/2410.03151v1","updated":"2024-10-04T05:21:42Z","published":"2024-10-04T05:21:42Z","title":"Media Framing through the Lens of Event-Centric Narratives","summary":"  From a communications perspective, a frame defines the packaging of the\nlanguage used in such a way as to encourage certain interpretations and to\ndiscourage others. For example, a news article can frame immigration as either\na boost or a drain on the economy, and thus communicate very different\ninterpretations of the same phenomenon. In this work, we argue that to explain\nframing devices we have to look at the way narratives are constructed. As a\nfirst step in this direction, we propose a framework that extracts events and\ntheir relations to other events, and groups them into high-level narratives\nthat help explain frames in news articles. We show that our framework can be\nused to analyze framing in U.S. news for two different domains: immigration and\ngun control.\n","authors":["Rohan Das","Aditya Chandra","I-Ta Lee","Maria Leonor Pacheco"],"pdf_url":"https://arxiv.org/pdf/2410.03151v1.pdf","comment":"Accepted to the 6th Workshop on Narrative Understanding, co-located\n  with EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.11972v3","updated":"2024-10-04T05:20:18Z","published":"2024-04-18T07:59:53Z","title":"Aligning Language Models to Explicitly Handle Ambiguity","summary":"  In interactions between users and language model agents, user utterances\nfrequently exhibit ellipsis (omission of words or phrases) or imprecision (lack\nof exactness) to prioritize efficiency. This can lead to varying\ninterpretations of the same input based on different assumptions or background\nknowledge. It is thus crucial for agents to adeptly handle the inherent\nambiguity in queries to ensure reliability. However, even state-of-the-art\nlarge language models (LLMs) still face challenges in such scenarios, primarily\ndue to the following hurdles: (1) LLMs are not explicitly trained to deal with\nambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may\nvary depending on the possessed knowledge. To address these issues, we propose\nAlignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to\nmanage ambiguous queries by leveraging their own assessment of ambiguity (i.e.,\nperceived ambiguity). Experimental results on question-answering datasets\ndemonstrate that APA empowers LLMs to explicitly detect and manage ambiguous\nqueries while retaining the ability to answer clear questions. Furthermore, our\nfinding proves that APA excels beyond training with gold-standard labels,\nespecially in out-of-distribution scenarios. The data and code are available at\nhttps://github.com/heyjoonkim/APA.\n","authors":["Hyuhng Joon Kim","Youna Kim","Cheonbok Park","Junyeob Kim","Choonghyun Park","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2404.11972v3.pdf","comment":"EMNLP 2024 (main)"},{"id":"http://arxiv.org/abs/2404.08634v2","updated":"2024-10-04T05:14:48Z","published":"2024-04-12T17:53:34Z","title":"Inheritune: Training Smaller Yet More Attentive Language Models","summary":"  Large Language Models (LLMs) have achieved remarkable performance across\nvarious natural language processing tasks, primarily due to the transformer\narchitecture and its self-attention mechanism. However, we observe that in\nstandard decoder-style LLMs, attention matrices degenerate to single-column for\ndeeper layers. Layers in this state are unable to learn anything meaningful and\nmostly redundant; we refer to these as lazy layers. The goal of this paper is\nto train smaller models by eliminating this structural inefficiency without\ncompromising performance.\n  Motivated by this observation, we propose Inheritune, a simple yet effective\ntraining recipe for developing smaller, high-performing language models.\nSmaller models trained with Inheritune, inherit early transformer layers from a\nlarger pre-trained model, then retrain and progressively expand until they\nmatch or exceed the performance of the larger model. We demonstrate that\nInheritune enables the training of various sizes of GPT-2 models on datasets\nlike OpenWebText-9B and FineWeb_edu. Models trained with Inheritune, despite\nhaving significantly fewer layers, match or even surpass the performance of\ntheir larger counterparts. For instance, our 16-layer GPT-2 medium variant\nachieves comparable performance to the standard 24-layer GPT-2 medium model.\nCode is available at https://github.com/sanyalsunny111/LLM-Inheritune.\n","authors":["Sunny Sanyal","Ravid Shwartz-Ziv","Alexandros G. Dimakis","Sujay Sanghavi"],"pdf_url":"https://arxiv.org/pdf/2404.08634v2.pdf","comment":"25 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2406.09324v2","updated":"2024-10-04T05:14:30Z","published":"2024-06-13T17:01:40Z","title":"Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","summary":"  Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we evaluate the impact of\nvarious attack settings on LLM performance and provide a baseline benchmark for\njailbreak attacks, encouraging the adoption of a standardized evaluation\nframework. Specifically, we evaluate the eight key factors of implementing\njailbreak attacks on LLMs from both target-level and attack-level perspectives.\nWe further conduct seven representative jailbreak attacks on six defense\nmethods across two widely used datasets, encompassing approximately 354\nexperiments with about 55,000 GPU hours on A800-80G. Our experimental results\nhighlight the need for standardized benchmarking to evaluate these attacks on\ndefense-enhanced LLMs. Our code is available at\nhttps://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.\n","authors":["Zhao Xu","Fan Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09324v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.04819v3","updated":"2024-10-04T05:13:58Z","published":"2024-05-08T05:38:20Z","title":"DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's\n  Disease Questions with Scientific Literature","summary":"  Recent advancements in large language models (LLMs) have achieved promising\nperformances across various applications. Nonetheless, the ongoing challenge of\nintegrating long-tail knowledge continues to impede the seamless adoption of\nLLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic\nCo-Augmentation of LLMs and KG, to address this limitation and demonstrate its\nability on studying Alzheimer's Disease (AD), a specialized sub-field in\nbiomedicine and a global health priority. With a synergized framework of LLM\nand KG mutually enhancing each other, we first leverage LLM to construct an\nevolving AD-specific knowledge graph (KG) sourced from AD-related scientific\nliterature, and then we utilize a coarse-to-fine sampling method with a novel\nself-aware knowledge retrieval approach to select appropriate knowledge from\nthe KG to augment LLM inference capabilities. The experimental results,\nconducted on our constructed AD question answering (ADQA) benchmark, underscore\nthe efficacy of DALK. Additionally, we perform a series of detailed analyses\nthat can offer valuable insights and guidelines for the emerging topic of\nmutually enhancing KG and LLM. We will release the code and data at\nhttps://github.com/David-Li0406/DALK.\n","authors":["Dawei Li","Shu Yang","Zhen Tan","Jae Young Baik","Sukwon Yun","Joseph Lee","Aaron Chacko","Bojian Hou","Duy Duong-Tran","Ying Ding","Huan Liu","Li Shen","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.04819v3.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03147v1","updated":"2024-10-04T05:07:55Z","published":"2024-10-04T05:07:55Z","title":"Analysis and Detection of Differences in Spoken User Behaviors between\n  Autonomous and Wizard-of-Oz Systems","summary":"  This study examined users' behavioral differences in a large corpus of\nJapanese human-robot interactions, comparing interactions between a\ntele-operated robot and an autonomous dialogue system. We analyzed user spoken\nbehaviors in both attentive listening and job interview dialogue scenarios.\nResults revealed significant differences in metrics such as speech length,\nspeaking rate, fillers, backchannels, disfluencies, and laughter between\noperator-controlled and autonomous conditions. Furthermore, we developed\npredictive models to distinguish between operator and autonomous system\nconditions. Our models demonstrated higher accuracy and precision compared to\nthe baseline model, with several models also achieving a higher F1 score than\nthe baseline.\n","authors":["Mikey Elmers","Koji Inoue","Divesh Lala","Keiko Ochi","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.03147v1.pdf","comment":"Accepted and will be presented at the 27th conference of the Oriental\n  COCOSDA (O-COCOSDA 2024)"},{"id":"http://arxiv.org/abs/2401.15498v3","updated":"2024-10-04T05:02:38Z","published":"2024-01-27T20:26:03Z","title":"Do We Need Language-Specific Fact-Checking Models? The Case of Chinese","summary":"  This paper investigates the potential benefits of language-specific\nfact-checking models, focusing on the case of Chinese. We first demonstrate the\nlimitations of translation-based methods and multilingual large language models\n(e.g., GPT-4), highlighting the need for language-specific systems. We further\npropose a Chinese fact-checking system that can better retrieve evidence from a\ndocument by incorporating context information. To better analyze token-level\nbiases in different systems, we construct an adversarial dataset based on the\nCHEF dataset, where each instance has large word overlap with the original one\nbut holds the opposite veracity label. Experimental results on the CHEF dataset\nand our adversarial dataset show that our proposed method outperforms\ntranslation-based methods and multilingual LLMs and is more robust toward\nbiases, while there is still large room for improvement, emphasizing the\nimportance of language-specific fact-checking systems.\n","authors":["Caiqi Zhang","Zhijiang Guo","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2401.15498v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2311.06985v3","updated":"2024-10-04T05:00:24Z","published":"2023-11-12T23:14:43Z","title":"Large Language Models are In-context Teachers for Knowledge Reasoning","summary":"  In this work, we study in-context teaching (ICT), where a teacher provides\nin-context example rationales to teach a student to reason over unseen cases.\nHuman teachers are usually required to craft in-context demonstrations, which\nare costly and have high variance. We ask whether a large language model (LLM)\ncan serve as a more effective in-context teacher for itself or other LLMs,\ncompared to humans. Inspired by the Encoding Specificity Hypothesis from human\nepisodic memory, we hypothesize that in-context exemplars crafted by the\nteacher should match the training data of the student. This hypothesis\nmotivates us to propose Self-Explain where an LLM's self-elicited explanations\nare used as in-context demonstrations for prompting it as they are generalized\nfrom the model's training examples. Self-Explain is shown to significantly\noutperform using human-crafted exemplars and other baselines.\n  Furthermore, we reveal that for ICT, rationales from different teacher LLMs\nor human experts that more resemble the student LLM's self-explanations are\nbetter in-context demonstrations. This supports our encoding specificity\nhypothesis. We then propose Teach-Back that aligns a teacher LLM with the\nstudent to enhance the ICT performance. For example, Teach-Back enables a 7B\nmodel to teach the much larger GPT-3.5 in context, surpassing human teachers by\naround 5% in test accuracy on medical question answering.\n","authors":["Jiachen Zhao","Zonghai Yao","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.06985v3.pdf","comment":"EMNLP 24 Findings"},{"id":"http://arxiv.org/abs/2406.17169v2","updated":"2024-10-04T05:00:13Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v2.pdf","comment":"Accepted at EMNLP 2024 Main"}],"Sound":[{"id":"http://arxiv.org/abs/2410.03459v1","updated":"2024-10-04T14:18:31Z","published":"2024-10-04T14:18:31Z","title":"Generative Semantic Communication for Text-to-Speech Synthesis","summary":"  Semantic communication is a promising technology to improve communication\nefficiency by transmitting only the semantic information of the source data.\nHowever, traditional semantic communication methods primarily focus on data\nreconstruction tasks, which may not be efficient for emerging generative tasks\nsuch as text-to-speech (TTS) synthesis. To address this limitation, this paper\ndevelops a novel generative semantic communication framework for TTS synthesis,\nleveraging generative artificial intelligence technologies. Firstly, we utilize\na pre-trained large speech model called WavLM and the residual vector\nquantization method to construct two semantic knowledge bases (KBs) at the\ntransmitter and receiver, respectively. The KB at the transmitter enables\neffective semantic extraction, while the KB at the receiver facilitates\nlifelike speech synthesis. Then, we employ a transformer encoder and a\ndiffusion model to achieve efficient semantic coding without introducing\nsignificant communication overhead. Finally, numerical results demonstrate that\nour framework achieves much higher fidelity for the generated speech than four\nbaselines, in both cases with additive white Gaussian noise channel and\nRayleigh fading channel.\n","authors":["Jiahao Zheng","Jinke Ren","Peng Xu","Zhihao Yuan","Jie Xu","Fangxin Wang","Gui Gui","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2410.03459v1.pdf","comment":"The paper has been accepted by IEEE Globecom Workshop"},{"id":"http://arxiv.org/abs/2410.03427v1","updated":"2024-10-04T13:37:07Z","published":"2024-10-04T13:37:07Z","title":"Biodenoising: animal vocalization denoising without access to clean data","summary":"  Animal vocalization denoising is a task similar to human speech enhancement,\na well-studied field of research. In contrast to the latter, it is applied to a\nhigher diversity of sound production mechanisms and recording environments, and\nthis higher diversity is a challenge for existing models. Adding to the\nchallenge and in contrast to speech, we lack large and diverse datasets\ncomprising clean vocalizations. As a solution we use as training data\npseudo-clean targets, i.e. pre-denoised vocalizations, and segments of\nbackground noise without a vocalization. We propose a train set derived from\nbioacoustics datasets and repositories representing diverse species, acoustic\nenvironments, geographic regions. Additionally, we introduce a non-overlapping\nbenchmark set comprising clean vocalizations from different taxa and noise\nsamples. We show that that denoising models (demucs, CleanUNet) trained on\npseudo-clean targets obtained with speech enhancement models achieve\ncompetitive results on the benchmarking set. We publish data, code, libraries,\nand demos https://mariusmiron.com/research/biodenoising.\n","authors":["Marius Miron","Sara Keen","Jen-Yu Liu","Benjamin Hoffman","Masato Hagiwara","Olivier Pietquin","Felix Effenberger","Maddie Cusimano"],"pdf_url":"https://arxiv.org/pdf/2410.03427v1.pdf","comment":"5 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.02560v2","updated":"2024-10-04T13:25:38Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Iakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v2.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.03375v1","updated":"2024-10-04T12:40:45Z","published":"2024-10-04T12:40:45Z","title":"SoundSignature: What Type of Music Do You Like?","summary":"  SoundSignature is a music application that integrates a custom OpenAI\nAssistant to analyze users' favorite songs. The system incorporates\nstate-of-the-art Music Information Retrieval (MIR) Python packages to combine\nextracted acoustic/musical features with the assistant's extensive knowledge of\nthe artists and bands. Capitalizing on this combined knowledge, SoundSignature\nleverages semantic audio and principles from the emerging Internet of Sounds\n(IoS) ecosystem, integrating MIR with AI to provide users with personalized\ninsights into the acoustic properties of their music, akin to a musical\npreference personality report. Users can then interact with the chatbot to\nexplore deeper inquiries about the acoustic analyses performed and how they\nrelate to their musical taste. This interactivity transforms the application,\nacting not only as an informative resource about familiar and/or favorite\nsongs, but also as an educational platform that enables users to deepen their\nunderstanding of musical features, music theory, acoustic properties commonly\nused in signal processing, and the artists behind the music. Beyond general\nusability, the application also incorporates several well-established\nopen-source musician-specific tools, such as a chord recognition algorithm\n(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter\n(basic-pitch). These features allow users without coding skills to access\nadvanced, open-source music processing algorithms simply by interacting with\nthe chatbot (e.g., can you give me the stems of this song?). In this paper, we\nhighlight the application's innovative features and educational potential, and\npresent findings from a pilot user study that evaluates its efficacy and\nusability.\n","authors":["Brandon James Carone","Pablo Ripollés"],"pdf_url":"https://arxiv.org/pdf/2410.03375v1.pdf","comment":"10 pages, 1 figure, to be published in the 2024 International\n  Symposium on the IEEE Internet of Sounds Proceedings"},{"id":"http://arxiv.org/abs/2409.09408v2","updated":"2024-10-04T12:26:21Z","published":"2024-09-14T10:49:06Z","title":"Leveraging Self-Supervised Learning for Speaker Diarization","summary":"  End-to-end neural diarization has evolved considerably over the past few\nyears, but data scarcity is still a major obstacle for further improvements.\nSelf-supervised learning methods such as WavLM have shown promising performance\non several downstream tasks, but their application on speaker diarization is\nsomehow limited. In this work, we explore using WavLM to alleviate the problem\nof data scarcity for neural diarization training. We use the same pipeline as\nPyannote and improve the local end-to-end neural diarization with WavLM and\nConformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets\nshow that our method substantially outperforms the Pyannote baseline and\nachieves new state-of-the-art results on AMI and AISHELL-4, respectively. In\naddition, by analyzing the system performance under different data quantity\nscenarios, we show that WavLM representations are much more robust against data\nscarcity than filterbank features, enabling less data hungry training\nstrategies. Furthermore, we found that simulated data, usually used to train\nendto-end diarization models, does not help when using WavLM in our\nexperiments. Additionally, we also evaluate our model on the recent CHiME8\nNOTSOFAR-1 task where it achieves better performance than the Pyannote\nbaseline. Our source code is publicly available at\nhttps://github.com/BUTSpeechFIT/DiariZen.\n","authors":["Jiangyu Han","Federico Landini","Johan Rohdin","Anna Silnova","Mireia Diez","Lukas Burget"],"pdf_url":"https://arxiv.org/pdf/2409.09408v2.pdf","comment":"Submitted to ICASSP 2025; New results are updated but conclusions are\n  exactly the same as the original one"},{"id":"http://arxiv.org/abs/2410.03335v1","updated":"2024-10-04T11:40:53Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions, and calls the agent for audio generation. Consequently,\nAudio-Agent generates high-quality audio that is closely aligned with the\nprovided text or video while also supporting variable-length generation. For\nvideo-to-audio (VTA) tasks, most existing methods require training a timestamp\ndetector to synchronize video events with generated audio, a process that can\nbe tedious and time-consuming. We propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions to bridge video and audio modality. Thus our\nframework provides a comprehensive solution for both TTA and VTA tasks without\nsubstantial computational overhead in training.\n","authors":["Zixuan Wang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2410.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11262v2","updated":"2024-10-04T10:35:03Z","published":"2024-09-17T15:10:36Z","title":"The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound\n  Event Detection","summary":"  This paper presents a residential audio dataset to support sound event\ndetection research for smart home applications aimed at promoting wellbeing for\nolder adults. The dataset is constructed by deploying audio recording systems\nin the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic\ncharacteristics are documented through detailed floor plans and construction\nmaterial information to enable replication of the recording environments for AI\nmodel deployment. A novel automated speech removal pipeline is developed, using\npre-trained audio neural networks to detect and remove segments containing\nspoken voice, while preserving segments containing other sound events. The\nresulting dataset consists of privacy-compliant audio recordings that\naccurately capture the soundscapes and activities of daily living within\nresidential spaces. The paper details the dataset creation methodology, the\nspeech removal pipeline utilizing cascaded model architectures, and an analysis\nof the vocal label distribution to validate the speech removal process. This\ndataset enables the development and benchmarking of sound event detection\nmodels tailored specifically for in-home applications.\n","authors":["Gabriel Bibbó","Thomas Deacon","Arshdeep Singh","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2409.11262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03264v1","updated":"2024-10-04T09:33:34Z","published":"2024-10-04T09:33:34Z","title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for\n  Text-to-Music Retrieval","summary":"  Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.\n","authors":["SeungHeon Doh","Minhee Lee","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2410.03264v1.pdf","comment":"Accepted for publication at the IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.16449v2","updated":"2024-10-04T08:32:14Z","published":"2023-12-27T07:27:09Z","title":"Online Similarity-and-Independence-Aware Beamformer for Low-latency\n  Target Sound Extraction","summary":"  This study introduces an online target sound extraction (TSE) process using\nthe similarity-and-independence-aware beamformer (SIBF) derived from an\niterative batch algorithm. The study aimed to reduce latency while maintaining\nextraction accuracy. The SIBF, which is a linear method, provides more accurate\nestimates of the target than an approximate magnitude spectrogram reference.\nThe transition to an online algorithm reduces latency but presents challenges.\nFirst, contrary to the conventional assumption, deriving the online algorithm\nmay degrade accuracy as compared to the batch algorithm using a sliding window.\nSecond, conventional post-processing methods intended for scaling the estimated\ntarget may widen the accuracy gap between the two algorithms. This study adopts\nan approach that addresses these challenges and minimizes the accuracy gap\nduring post-processing. It proposes a novel scaling method based on the\nsingle-channel Wiener filter (SWF-based scaling). To further improve accuracy,\nthe study introduces a modified version of the time-frequency-varying variance\ngeneralized Gaussian distribution as a source model to represent the joint\nprobability between the target and reference. Experimental results using the\nCHiME-3 dataset demonstrate several key findings: 1) SWF-based scaling\neffectively eliminates the gap between the two algorithms and improves\naccuracy. 2) The new source model achieves optimal accuracy, corresponding to\nthe Laplacian model. 3) Our online SIBF outperforms conventional linear TSE\nmethods, including independent vector extraction and minimum mean square error\nbeamforming. These findings can contribute to the fields of beamforming and\nblind source separation.\n","authors":["Atsuo Hiroe"],"pdf_url":"https://arxiv.org/pdf/2312.16449v2.pdf","comment":"Re-submitted to IEEE Open Journal of Signal Processing"},{"id":"http://arxiv.org/abs/2407.04280v2","updated":"2024-10-04T07:25:18Z","published":"2024-07-05T06:25:54Z","title":"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous\n  Speech","summary":"  Prevalent ungrammatical expressions and disfluencies in spontaneous speech\nfrom second language (L2) learners pose unique challenges to Automatic Speech\nRecognition (ASR) systems. However, few datasets are tailored to L2 learner\nspeech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours\nof audio and transcriptions of L2 learners' spontaneous speech. Our linguistic\nanalysis reveals that transcriptions in our dataset contain L2S (L2 learner's\nSpontaneous speech) features, consisting of ungrammatical expressions and\ndisfluencies (e.g., filler words, word repetitions, self-repairs, false\nstarts), significantly more than native speech datasets. Fine-tuning\nwhisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than\nvanilla whisper-small.en. Furthermore, our qualitative analysis indicates that\n54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S\nfeatures, with 48.1% of them being reduced in the fine-tuned model.\n","authors":["Haechan Kim","Junho Myung","Seoyoung Kim","Sungpah Lee","Dongyeop Kang","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2407.04280v2.pdf","comment":"Proceedings of Interspeech"},{"id":"http://arxiv.org/abs/2410.03192v1","updated":"2024-10-04T07:10:25Z","published":"2024-10-04T07:10:25Z","title":"MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech","summary":"  Text-to-speech (TTS) systems that scale up the amount of training data have\nachieved significant improvements in zero-shot speech synthesis. However, these\nsystems have certain limitations: they require a large amount of training data,\nwhich increases costs, and often overlook prosody similarity. To address these\nissues, we propose MultiVerse, a zero-shot multi-task TTS system that is able\nto perform TTS or speech style transfer in zero-shot and cross-lingual\nconditions. MultiVerse requires much less training data than traditional\ndata-driven approaches. To ensure zero-shot performance even with limited data,\nwe leverage source-filter theory-based disentanglement, utilizing the prompt\nfor modeling filter-related and source-related representations. Additionally,\nto further enhance prosody similarity, we adopt a prosody modeling approach\ncombining prompt-based autoregressive and non-autoregressive methods.\nEvaluations demonstrate the remarkable zero-shot multi-task TTS performance of\nMultiVerse and show that MultiVerse not only achieves zero-shot TTS performance\ncomparable to data-driven TTS systems with much less data, but also\nsignificantly outperforms other zero-shot TTS systems trained with the same\nsmall amount of data. In particular, our novel prosody modeling technique\nsignificantly contributes to MultiVerse's ability to generate speech with high\nprosody similarity to the given prompts. Our samples are available at\nhttps://nc-ai.github.io/speech/publications/multiverse/index.html\n","authors":["Taejun Bak","Youngsik Eom","SeungJae Choi","Young-Sun Joo"],"pdf_url":"https://arxiv.org/pdf/2410.03192v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2405.12609v5","updated":"2024-10-04T07:07:15Z","published":"2024-05-21T09:04:48Z","title":"Mamba in Speech: Towards an Alternative to Self-Attention","summary":"  Transformer and its derivatives have achieved success in diverse tasks across\ncomputer vision, natural language processing, and speech processing. To reduce\nthe complexity of computations within the multi-head self-attention mechanism\nin Transformer, Selective State Space Models (i.e., Mamba) were proposed as an\nalternative. Mamba exhibited its effectiveness in natural language processing\nand computer vision tasks, but its superiority has rarely been investigated in\nspeech signal processing. This paper explores solutions for applying Mamba to\nspeech processing by discussing two typical speech processing tasks: speech\nrecognition, which requires semantic and sequential information, and speech\nenhancement, which focuses primarily on sequential patterns. The experimental\nresults show the superiority of bidirectional Mamba~(BiMamba) for speech\nprocessing to vanilla Mamba. Moreover, experiments demonstrate the\neffectiveness of BiMamba as an alternative to the self-attention module in\nTransformer and its derivates, particularly for the semantic-aware task. The\ncrucial technologies for transferring Mamba to speech are then summarized in\nablation studies and the discussion section to offer insights for future\nresearch.\n","authors":["Xiangyu Zhang","Qiquan Zhang","Hexin Liu","Tianyi Xiao","Xinyuan Qian","Beena Ahmed","Eliathamby Ambikairajah","Haizhou Li","Julien Epps"],"pdf_url":"https://arxiv.org/pdf/2405.12609v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03139v1","updated":"2024-10-04T04:26:11Z","published":"2024-10-04T04:26:11Z","title":"How does the teacher rate? Observations from the NeuroPiano dataset","summary":"  This paper provides a detailed analysis of the NeuroPiano dataset, which\ncomprise 104 audio recordings of student piano performances accompanied with\n2255 textual feedback and ratings given by professional pianists. We offer a\nstatistical overview of the dataset, focusing on the standardization of\nannotations and inter-annotator agreement across 12 evaluative questions\nconcerning performance quality. We also explore the predictive relationship\nbetween audio features and teacher ratings via machine learning, as well as\nannotations provided for text analysis of the responses.\n","authors":["Huan Zhang","Vincent Cheung","Hayato Nishioka","Simon Dixon","Shinichi Furuya"],"pdf_url":"https://arxiv.org/pdf/2410.03139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19283v2","updated":"2024-10-04T22:34:38Z","published":"2024-09-28T08:36:44Z","title":"Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for\n  Neural Codec Language Models","summary":"  Building upon advancements in Large Language Models (LLMs), the field of\naudio processing has seen increased interest in training audio generation tasks\nwith discrete audio token sequences. However, directly discretizing audio by\nneural audio codecs often results in sequences that fundamentally differ from\ntext sequences. Unlike text, where text token sequences are deterministic,\ndiscrete audio tokens can exhibit significant variability based on contextual\nfactors, while still producing perceptually identical audio segments. We refer\nto this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}.\nThis inconsistency can lead to a single audio segment being represented by\nmultiple divergent sequences, which creates confusion in neural codec language\nmodels and results in omissions and repetitions during speech generation. In\nthis paper, we quantitatively analyze the DRI phenomenon within popular audio\ntokenizers such as EnCodec. Our approach effectively mitigates the DRI\nphenomenon of the neural audio codec. Furthermore, extensive experiments on the\nneural codec language model over LibriTTS and large-scale MLS datases (44,000\nhours) demonstrate the effectiveness and generality of our method. The demo of\naudio samples is available\nonline~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.\n","authors":["Wenrui Liu","Zhifang Guo","Jin Xu","Yuanjun Lv","Yunfei Chu","Zhou Zhao","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.19283v2.pdf","comment":"e.g.: 15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.03930v1","updated":"2024-10-04T21:13:58Z","published":"2024-10-04T21:13:58Z","title":"Reverb: Open-Source ASR and Diarization from Rev","summary":"  Today, we are open-sourcing our core speech recognition and diarization\nmodels for non-commercial use. We are releasing both a full production pipeline\nfor developers as well as pared-down research models for experimentation. Rev\nhopes that these releases will spur research and innovation in the fast-moving\ndomain of voice technology. The speech recognition models released today\noutperform all existing open source speech recognition models across a variety\nof long-form speech recognition domains.\n","authors":["Nishchal Bhandari","Danny Chen","Miguel Ángel del Río Fernández","Natalie Delworth","Jennifer Drexler Fox","Migüel Jetté","Quinten McNamara","Corey Miller","Ondřej Novotný","Ján Profant","Nan Qin","Martin Ratajczak","Jean-Philippe Robichaud"],"pdf_url":"https://arxiv.org/pdf/2410.03930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03904v1","updated":"2024-10-04T20:12:35Z","published":"2024-10-04T20:12:35Z","title":"Did You Hear That? Introducing AADG: A Framework for Generating\n  Benchmark Data in Audio Anomaly Detection","summary":"  We introduce a novel, general-purpose audio generation framework specifically\ndesigned for anomaly detection and localization. Unlike existing datasets that\npredominantly focus on industrial and machine-related sounds, our framework\nfocuses a broader range of environments, particularly useful in real-world\nscenarios where only audio data are available, such as in video-derived or\ntelephonic audio. To generate such data, we propose a new method inspired by\nthe LLM-Modulo framework, which leverages large language models(LLMs) as world\nmodels to simulate such real-world scenarios. This tool is modular allowing a\nplug-and-play approach. It operates by first using LLMs to predict plausible\nreal-world scenarios. An LLM further extracts the constituent sounds, the order\nand the way in which these should be merged to create coherent wholes. Much\nlike the LLM-Modulo framework, we include rigorous verification of each output\nstage, ensuring the reliability of the generated data. The data produced using\nthe framework serves as a benchmark for anomaly detection applications,\npotentially enhancing the performance of models trained on audio data,\nparticularly in handling out-of-distribution cases. Our contributions thus fill\na critical void in audio anomaly detection resources and provide a scalable\ntool for generating diverse, realistic audio data.\n","authors":["Ksheeraja Raghavan","Samiran Gode","Ankit Shah","Surabhi Raghavan","Wolfram Burgard","Bhiksha Raj","Rita Singh"],"pdf_url":"https://arxiv.org/pdf/2410.03904v1.pdf","comment":"9 pages, under review"},{"id":"http://arxiv.org/abs/2410.03879v1","updated":"2024-10-04T19:22:35Z","published":"2024-10-04T19:22:35Z","title":"SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual\n  Data","summary":"  We present SONIQUE, a model for generating background music tailored to video\ncontent. Unlike traditional video-to-music generation approaches, which rely\nheavily on paired audio-visual datasets, SONIQUE leverages unpaired data,\ncombining royalty-free music and independent video sources. By utilizing large\nlanguage models (LLMs) for video understanding and converting visual\ndescriptions into musical tags, alongside a U-Net-based conditional diffusion\nmodel, SONIQUE enables customizable music generation. Users can control\nspecific aspects of the music, such as instruments, genres, tempo, and\nmelodies, ensuring the generated output fits their creative vision. SONIQUE is\nopen-source, with a demo available online.\n","authors":["Liqian Zhang","Magdalena Fuentes"],"pdf_url":"https://arxiv.org/pdf/2410.03879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00822v2","updated":"2024-10-04T18:30:06Z","published":"2024-10-01T16:06:02Z","title":"VHASR: A Multimodal Speech Recognition System With Vision Hotwords","summary":"  The image-based multimodal automatic speech recognition (ASR) model enhances\nspeech recognition performance by incorporating audio-related image. However,\nsome works suggest that introducing image information to model does not help\nimproving ASR performance. In this paper, we propose a novel approach\neffectively utilizing audio-related image information and set up VHASR, a\nmultimodal speech recognition system that uses vision as hotwords to strengthen\nthe model's speech recognition capability. Our system utilizes a dual-stream\narchitecture, which firstly transcribes the text on the two streams separately,\nand then combines the outputs. We evaluate the proposed model on four datasets:\nFlickr8k, ADE20k, COCO, and OpenImages. The experimental results show that\nVHASR can effectively utilize key information in images to enhance the model's\nspeech recognition ability. Its performance not only surpasses unimodal ASR,\nbut also achieves SOTA among existing image-based multimodal ASR.\n","authors":["Jiliang Hu","Zuchao Li","Ping Wang","Haojun Ai","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.00822v2.pdf","comment":"14 pages, 6 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03813v1","updated":"2024-10-04T13:53:15Z","published":"2024-10-04T13:53:15Z","title":"SOI: Scaling Down Computational Complexity by Estimating Partial States\n  of the Model","summary":"  Consumer electronics used to follow the miniaturization trend described by\nMoore's Law. Despite increased processing power in Microcontroller Units\n(MCUs), MCUs used in the smallest appliances are still not capable of running\neven moderately big, state-of-the-art artificial neural networks (ANNs)\nespecially in time-sensitive scenarios. In this work, we present a novel method\ncalled Scattered Online Inference (SOI) that aims to reduce the computational\ncomplexity of ANNs. SOI leverages the continuity and seasonality of time-series\ndata and model predictions, enabling extrapolation for processing speed\nimprovements, particularly in deeper layers. By applying compression, SOI\ngenerates more general inner partial states of ANN, allowing skipping full\nmodel recalculation at each inference.\n","authors":["Grzegorz Stefański","Paweł Daniluk","Artur Szumaczuk","Jakub Tkaczuk"],"pdf_url":"https://arxiv.org/pdf/2410.03813v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.03798v1","updated":"2024-10-04T04:34:24Z","published":"2024-10-04T04:34:24Z","title":"Self-Powered LLM Modality Expansion for Large Speech-Text Models","summary":"  Large language models (LLMs) exhibit remarkable performance across diverse\ntasks, indicating their potential for expansion into large speech-text models\n(LSMs) by integrating speech capabilities. Although unified speech-text\npre-training and multimodal data instruction-tuning offer considerable\nbenefits, these methods generally entail significant resource demands and tend\nto overfit specific tasks. This study aims to refine the use of speech datasets\nfor LSM training by addressing the limitations of vanilla instruction tuning.\nWe explore the instruction-following dynamics within LSMs, identifying a\ncritical issue termed speech anchor bias-a tendency for LSMs to over-rely on\nspeech inputs, mistakenly interpreting the entire speech modality as\ndirectives, thereby neglecting textual instructions. To counteract this bias,\nwe introduce a self-powered LSM that leverages augmented automatic speech\nrecognition data generated by the model itself for more effective instruction\ntuning. Our experiments across a range of speech-based tasks demonstrate that\nself-powered LSM mitigates speech anchor bias and improves the fusion of speech\nand text modalities in LSMs. Data, code and scripts are freely available at\nhttps://github.com/ytf-philp/Self-powered-LSM.\n","authors":["Tengfei Yu","Xuebo Liu","Zhiyi Hou","Liang Ding","Dacheng Tao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03798v1.pdf","comment":"Accepted to EMNLP 2024"}],"Speech Processing":[{"id":"http://arxiv.org/abs/2410.03459v1","updated":"2024-10-04T14:18:31Z","published":"2024-10-04T14:18:31Z","title":"Generative Semantic Communication for Text-to-Speech Synthesis","summary":"  Semantic communication is a promising technology to improve communication\nefficiency by transmitting only the semantic information of the source data.\nHowever, traditional semantic communication methods primarily focus on data\nreconstruction tasks, which may not be efficient for emerging generative tasks\nsuch as text-to-speech (TTS) synthesis. To address this limitation, this paper\ndevelops a novel generative semantic communication framework for TTS synthesis,\nleveraging generative artificial intelligence technologies. Firstly, we utilize\na pre-trained large speech model called WavLM and the residual vector\nquantization method to construct two semantic knowledge bases (KBs) at the\ntransmitter and receiver, respectively. The KB at the transmitter enables\neffective semantic extraction, while the KB at the receiver facilitates\nlifelike speech synthesis. Then, we employ a transformer encoder and a\ndiffusion model to achieve efficient semantic coding without introducing\nsignificant communication overhead. Finally, numerical results demonstrate that\nour framework achieves much higher fidelity for the generated speech than four\nbaselines, in both cases with additive white Gaussian noise channel and\nRayleigh fading channel.\n","authors":["Jiahao Zheng","Jinke Ren","Peng Xu","Zhihao Yuan","Jie Xu","Fangxin Wang","Gui Gui","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2410.03459v1.pdf","comment":"The paper has been accepted by IEEE Globecom Workshop"},{"id":"http://arxiv.org/abs/2410.03427v1","updated":"2024-10-04T13:37:07Z","published":"2024-10-04T13:37:07Z","title":"Biodenoising: animal vocalization denoising without access to clean data","summary":"  Animal vocalization denoising is a task similar to human speech enhancement,\na well-studied field of research. In contrast to the latter, it is applied to a\nhigher diversity of sound production mechanisms and recording environments, and\nthis higher diversity is a challenge for existing models. Adding to the\nchallenge and in contrast to speech, we lack large and diverse datasets\ncomprising clean vocalizations. As a solution we use as training data\npseudo-clean targets, i.e. pre-denoised vocalizations, and segments of\nbackground noise without a vocalization. We propose a train set derived from\nbioacoustics datasets and repositories representing diverse species, acoustic\nenvironments, geographic regions. Additionally, we introduce a non-overlapping\nbenchmark set comprising clean vocalizations from different taxa and noise\nsamples. We show that that denoising models (demucs, CleanUNet) trained on\npseudo-clean targets obtained with speech enhancement models achieve\ncompetitive results on the benchmarking set. We publish data, code, libraries,\nand demos https://mariusmiron.com/research/biodenoising.\n","authors":["Marius Miron","Sara Keen","Jen-Yu Liu","Benjamin Hoffman","Masato Hagiwara","Olivier Pietquin","Felix Effenberger","Maddie Cusimano"],"pdf_url":"https://arxiv.org/pdf/2410.03427v1.pdf","comment":"5 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.02560v2","updated":"2024-10-04T13:25:38Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Iakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v2.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.03375v1","updated":"2024-10-04T12:40:45Z","published":"2024-10-04T12:40:45Z","title":"SoundSignature: What Type of Music Do You Like?","summary":"  SoundSignature is a music application that integrates a custom OpenAI\nAssistant to analyze users' favorite songs. The system incorporates\nstate-of-the-art Music Information Retrieval (MIR) Python packages to combine\nextracted acoustic/musical features with the assistant's extensive knowledge of\nthe artists and bands. Capitalizing on this combined knowledge, SoundSignature\nleverages semantic audio and principles from the emerging Internet of Sounds\n(IoS) ecosystem, integrating MIR with AI to provide users with personalized\ninsights into the acoustic properties of their music, akin to a musical\npreference personality report. Users can then interact with the chatbot to\nexplore deeper inquiries about the acoustic analyses performed and how they\nrelate to their musical taste. This interactivity transforms the application,\nacting not only as an informative resource about familiar and/or favorite\nsongs, but also as an educational platform that enables users to deepen their\nunderstanding of musical features, music theory, acoustic properties commonly\nused in signal processing, and the artists behind the music. Beyond general\nusability, the application also incorporates several well-established\nopen-source musician-specific tools, such as a chord recognition algorithm\n(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter\n(basic-pitch). These features allow users without coding skills to access\nadvanced, open-source music processing algorithms simply by interacting with\nthe chatbot (e.g., can you give me the stems of this song?). In this paper, we\nhighlight the application's innovative features and educational potential, and\npresent findings from a pilot user study that evaluates its efficacy and\nusability.\n","authors":["Brandon James Carone","Pablo Ripollés"],"pdf_url":"https://arxiv.org/pdf/2410.03375v1.pdf","comment":"10 pages, 1 figure, to be published in the 2024 International\n  Symposium on the IEEE Internet of Sounds Proceedings"},{"id":"http://arxiv.org/abs/2409.09408v2","updated":"2024-10-04T12:26:21Z","published":"2024-09-14T10:49:06Z","title":"Leveraging Self-Supervised Learning for Speaker Diarization","summary":"  End-to-end neural diarization has evolved considerably over the past few\nyears, but data scarcity is still a major obstacle for further improvements.\nSelf-supervised learning methods such as WavLM have shown promising performance\non several downstream tasks, but their application on speaker diarization is\nsomehow limited. In this work, we explore using WavLM to alleviate the problem\nof data scarcity for neural diarization training. We use the same pipeline as\nPyannote and improve the local end-to-end neural diarization with WavLM and\nConformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets\nshow that our method substantially outperforms the Pyannote baseline and\nachieves new state-of-the-art results on AMI and AISHELL-4, respectively. In\naddition, by analyzing the system performance under different data quantity\nscenarios, we show that WavLM representations are much more robust against data\nscarcity than filterbank features, enabling less data hungry training\nstrategies. Furthermore, we found that simulated data, usually used to train\nendto-end diarization models, does not help when using WavLM in our\nexperiments. Additionally, we also evaluate our model on the recent CHiME8\nNOTSOFAR-1 task where it achieves better performance than the Pyannote\nbaseline. Our source code is publicly available at\nhttps://github.com/BUTSpeechFIT/DiariZen.\n","authors":["Jiangyu Han","Federico Landini","Johan Rohdin","Anna Silnova","Mireia Diez","Lukas Burget"],"pdf_url":"https://arxiv.org/pdf/2409.09408v2.pdf","comment":"Submitted to ICASSP 2025; New results are updated but conclusions are\n  exactly the same as the original one"},{"id":"http://arxiv.org/abs/2410.03335v1","updated":"2024-10-04T11:40:53Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions, and calls the agent for audio generation. Consequently,\nAudio-Agent generates high-quality audio that is closely aligned with the\nprovided text or video while also supporting variable-length generation. For\nvideo-to-audio (VTA) tasks, most existing methods require training a timestamp\ndetector to synchronize video events with generated audio, a process that can\nbe tedious and time-consuming. We propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions to bridge video and audio modality. Thus our\nframework provides a comprehensive solution for both TTA and VTA tasks without\nsubstantial computational overhead in training.\n","authors":["Zixuan Wang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2410.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03312v1","updated":"2024-10-04T10:50:18Z","published":"2024-10-04T10:50:18Z","title":"Context and System Fusion in Post-ASR Emotion Recognition with Large\n  Language Models","summary":"  Large language models (LLMs) have started to play a vital role in modelling\nspeech and text. To explore the best use of context and multiple systems'\noutputs for post-ASR speech emotion prediction, we study LLM prompting on a\nrecent task named GenSEC. Our techniques include ASR transcript ranking,\nvariable conversation context, and system output fusion. We show that the\nconversation context has diminishing returns and the metric used to select the\ntranscript for prediction is crucial. Finally, our best submission surpasses\nthe provided baseline by 20% in absolute accuracy.\n","authors":["Pavel Stepachev","Pinzhen Chen","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2410.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11262v2","updated":"2024-10-04T10:35:03Z","published":"2024-09-17T15:10:36Z","title":"The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound\n  Event Detection","summary":"  This paper presents a residential audio dataset to support sound event\ndetection research for smart home applications aimed at promoting wellbeing for\nolder adults. The dataset is constructed by deploying audio recording systems\nin the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic\ncharacteristics are documented through detailed floor plans and construction\nmaterial information to enable replication of the recording environments for AI\nmodel deployment. A novel automated speech removal pipeline is developed, using\npre-trained audio neural networks to detect and remove segments containing\nspoken voice, while preserving segments containing other sound events. The\nresulting dataset consists of privacy-compliant audio recordings that\naccurately capture the soundscapes and activities of daily living within\nresidential spaces. The paper details the dataset creation methodology, the\nspeech removal pipeline utilizing cascaded model architectures, and an analysis\nof the vocal label distribution to validate the speech removal process. This\ndataset enables the development and benchmarking of sound event detection\nmodels tailored specifically for in-home applications.\n","authors":["Gabriel Bibbó","Thomas Deacon","Arshdeep Singh","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2409.11262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03298v1","updated":"2024-10-04T10:21:15Z","published":"2024-10-04T10:21:15Z","title":"Textless Streaming Speech-to-Speech Translation using Semantic Speech\n  Tokens","summary":"  Cascaded speech-to-speech translation systems often suffer from the error\naccumulation problem and high latency, which is a result of cascaded modules\nwhose inference delays accumulate. In this paper, we propose a transducer-based\nspeech translation model that outputs discrete speech tokens in a low-latency\nstreaming fashion. This approach eliminates the need for generating text output\nfirst, followed by machine translation (MT) and text-to-speech (TTS) systems.\nThe produced speech tokens can be directly used to generate a speech signal\nwith low latency by utilizing an acoustic language model (LM) to obtain\nacoustic tokens and an audio codec model to retrieve the waveform. Experimental\nresults show that the proposed method outperforms other existing approaches and\nachieves state-of-the-art results for streaming translation in terms of BLEU,\naverage latency, and BLASER 2.0 scores for multiple language pairs using the\nCVSS-C dataset as a benchmark.\n","authors":["Jinzheng Zhao","Niko Moritz","Egor Lakomkin","Ruiming Xie","Zhiping Xiu","Katerina Zmolikova","Zeeshan Ahmed","Yashesh Gaur","Duc Le","Christian Fuegen"],"pdf_url":"https://arxiv.org/pdf/2410.03298v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.03280v1","updated":"2024-10-04T09:53:16Z","published":"2024-10-04T09:53:16Z","title":"Manikin-Recorded Cardiopulmonary Sounds Dataset Using Digital\n  Stethoscope","summary":"  Heart and lung sounds are crucial for healthcare monitoring. Recent\nimprovements in stethoscope technology have made it possible to capture patient\nsounds with enhanced precision. In this dataset, we used a digital stethoscope\nto capture both heart and lung sounds, including individual and mixed\nrecordings. To our knowledge, this is the first dataset to offer both separate\nand mixed cardiorespiratory sounds. The recordings were collected from a\nclinical manikin, a patient simulator designed to replicate human physiological\nconditions, generating clean heart and lung sounds at different body locations.\nThis dataset includes both normal sounds and various abnormalities (i.e.,\nmurmur, atrial fibrillation, tachycardia, atrioventricular block, third and\nfourth heart sound, wheezing, crackles, rhonchi, pleural rub, and gurgling\nsounds). The dataset includes audio recordings of chest examinations performed\nat different anatomical locations, as determined by specialist nurses. Each\nrecording has been enhanced using frequency filters to highlight specific sound\ntypes. This dataset is useful for applications in artificial intelligence, such\nas automated cardiopulmonary disease detection, sound classification,\nunsupervised separation techniques, and deep learning algorithms related to\naudio signal processing.\n","authors":["Yasaman Torabi","Shahram Shirani","James P. Reilly"],"pdf_url":"https://arxiv.org/pdf/2410.03280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03264v1","updated":"2024-10-04T09:33:34Z","published":"2024-10-04T09:33:34Z","title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for\n  Text-to-Music Retrieval","summary":"  Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.\n","authors":["SeungHeon Doh","Minhee Lee","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2410.03264v1.pdf","comment":"Accepted for publication at the IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.16449v2","updated":"2024-10-04T08:32:14Z","published":"2023-12-27T07:27:09Z","title":"Online Similarity-and-Independence-Aware Beamformer for Low-latency\n  Target Sound Extraction","summary":"  This study introduces an online target sound extraction (TSE) process using\nthe similarity-and-independence-aware beamformer (SIBF) derived from an\niterative batch algorithm. The study aimed to reduce latency while maintaining\nextraction accuracy. The SIBF, which is a linear method, provides more accurate\nestimates of the target than an approximate magnitude spectrogram reference.\nThe transition to an online algorithm reduces latency but presents challenges.\nFirst, contrary to the conventional assumption, deriving the online algorithm\nmay degrade accuracy as compared to the batch algorithm using a sliding window.\nSecond, conventional post-processing methods intended for scaling the estimated\ntarget may widen the accuracy gap between the two algorithms. This study adopts\nan approach that addresses these challenges and minimizes the accuracy gap\nduring post-processing. It proposes a novel scaling method based on the\nsingle-channel Wiener filter (SWF-based scaling). To further improve accuracy,\nthe study introduces a modified version of the time-frequency-varying variance\ngeneralized Gaussian distribution as a source model to represent the joint\nprobability between the target and reference. Experimental results using the\nCHiME-3 dataset demonstrate several key findings: 1) SWF-based scaling\neffectively eliminates the gap between the two algorithms and improves\naccuracy. 2) The new source model achieves optimal accuracy, corresponding to\nthe Laplacian model. 3) Our online SIBF outperforms conventional linear TSE\nmethods, including independent vector extraction and minimum mean square error\nbeamforming. These findings can contribute to the fields of beamforming and\nblind source separation.\n","authors":["Atsuo Hiroe"],"pdf_url":"https://arxiv.org/pdf/2312.16449v2.pdf","comment":"Re-submitted to IEEE Open Journal of Signal Processing"},{"id":"http://arxiv.org/abs/2407.04280v2","updated":"2024-10-04T07:25:18Z","published":"2024-07-05T06:25:54Z","title":"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous\n  Speech","summary":"  Prevalent ungrammatical expressions and disfluencies in spontaneous speech\nfrom second language (L2) learners pose unique challenges to Automatic Speech\nRecognition (ASR) systems. However, few datasets are tailored to L2 learner\nspeech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours\nof audio and transcriptions of L2 learners' spontaneous speech. Our linguistic\nanalysis reveals that transcriptions in our dataset contain L2S (L2 learner's\nSpontaneous speech) features, consisting of ungrammatical expressions and\ndisfluencies (e.g., filler words, word repetitions, self-repairs, false\nstarts), significantly more than native speech datasets. Fine-tuning\nwhisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than\nvanilla whisper-small.en. Furthermore, our qualitative analysis indicates that\n54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S\nfeatures, with 48.1% of them being reduced in the fine-tuned model.\n","authors":["Haechan Kim","Junho Myung","Seoyoung Kim","Sungpah Lee","Dongyeop Kang","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2407.04280v2.pdf","comment":"Proceedings of Interspeech"},{"id":"http://arxiv.org/abs/2410.03192v1","updated":"2024-10-04T07:10:25Z","published":"2024-10-04T07:10:25Z","title":"MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech","summary":"  Text-to-speech (TTS) systems that scale up the amount of training data have\nachieved significant improvements in zero-shot speech synthesis. However, these\nsystems have certain limitations: they require a large amount of training data,\nwhich increases costs, and often overlook prosody similarity. To address these\nissues, we propose MultiVerse, a zero-shot multi-task TTS system that is able\nto perform TTS or speech style transfer in zero-shot and cross-lingual\nconditions. MultiVerse requires much less training data than traditional\ndata-driven approaches. To ensure zero-shot performance even with limited data,\nwe leverage source-filter theory-based disentanglement, utilizing the prompt\nfor modeling filter-related and source-related representations. Additionally,\nto further enhance prosody similarity, we adopt a prosody modeling approach\ncombining prompt-based autoregressive and non-autoregressive methods.\nEvaluations demonstrate the remarkable zero-shot multi-task TTS performance of\nMultiVerse and show that MultiVerse not only achieves zero-shot TTS performance\ncomparable to data-driven TTS systems with much less data, but also\nsignificantly outperforms other zero-shot TTS systems trained with the same\nsmall amount of data. In particular, our novel prosody modeling technique\nsignificantly contributes to MultiVerse's ability to generate speech with high\nprosody similarity to the given prompts. Our samples are available at\nhttps://nc-ai.github.io/speech/publications/multiverse/index.html\n","authors":["Taejun Bak","Youngsik Eom","SeungJae Choi","Young-Sun Joo"],"pdf_url":"https://arxiv.org/pdf/2410.03192v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2405.12609v5","updated":"2024-10-04T07:07:15Z","published":"2024-05-21T09:04:48Z","title":"Mamba in Speech: Towards an Alternative to Self-Attention","summary":"  Transformer and its derivatives have achieved success in diverse tasks across\ncomputer vision, natural language processing, and speech processing. To reduce\nthe complexity of computations within the multi-head self-attention mechanism\nin Transformer, Selective State Space Models (i.e., Mamba) were proposed as an\nalternative. Mamba exhibited its effectiveness in natural language processing\nand computer vision tasks, but its superiority has rarely been investigated in\nspeech signal processing. This paper explores solutions for applying Mamba to\nspeech processing by discussing two typical speech processing tasks: speech\nrecognition, which requires semantic and sequential information, and speech\nenhancement, which focuses primarily on sequential patterns. The experimental\nresults show the superiority of bidirectional Mamba~(BiMamba) for speech\nprocessing to vanilla Mamba. Moreover, experiments demonstrate the\neffectiveness of BiMamba as an alternative to the self-attention module in\nTransformer and its derivates, particularly for the semantic-aware task. The\ncrucial technologies for transferring Mamba to speech are then summarized in\nablation studies and the discussion section to offer insights for future\nresearch.\n","authors":["Xiangyu Zhang","Qiquan Zhang","Hexin Liu","Tianyi Xiao","Xinyuan Qian","Beena Ahmed","Eliathamby Ambikairajah","Haizhou Li","Julien Epps"],"pdf_url":"https://arxiv.org/pdf/2405.12609v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03139v1","updated":"2024-10-04T04:26:11Z","published":"2024-10-04T04:26:11Z","title":"How does the teacher rate? Observations from the NeuroPiano dataset","summary":"  This paper provides a detailed analysis of the NeuroPiano dataset, which\ncomprise 104 audio recordings of student piano performances accompanied with\n2255 textual feedback and ratings given by professional pianists. We offer a\nstatistical overview of the dataset, focusing on the standardization of\nannotations and inter-annotator agreement across 12 evaluative questions\nconcerning performance quality. We also explore the predictive relationship\nbetween audio features and teacher ratings via machine learning, as well as\nannotations provided for text analysis of the responses.\n","authors":["Huan Zhang","Vincent Cheung","Hayato Nishioka","Simon Dixon","Shinichi Furuya"],"pdf_url":"https://arxiv.org/pdf/2410.03139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19283v2","updated":"2024-10-04T22:34:38Z","published":"2024-09-28T08:36:44Z","title":"Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for\n  Neural Codec Language Models","summary":"  Building upon advancements in Large Language Models (LLMs), the field of\naudio processing has seen increased interest in training audio generation tasks\nwith discrete audio token sequences. However, directly discretizing audio by\nneural audio codecs often results in sequences that fundamentally differ from\ntext sequences. Unlike text, where text token sequences are deterministic,\ndiscrete audio tokens can exhibit significant variability based on contextual\nfactors, while still producing perceptually identical audio segments. We refer\nto this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}.\nThis inconsistency can lead to a single audio segment being represented by\nmultiple divergent sequences, which creates confusion in neural codec language\nmodels and results in omissions and repetitions during speech generation. In\nthis paper, we quantitatively analyze the DRI phenomenon within popular audio\ntokenizers such as EnCodec. Our approach effectively mitigates the DRI\nphenomenon of the neural audio codec. Furthermore, extensive experiments on the\nneural codec language model over LibriTTS and large-scale MLS datases (44,000\nhours) demonstrate the effectiveness and generality of our method. The demo of\naudio samples is available\nonline~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.\n","authors":["Wenrui Liu","Zhifang Guo","Jin Xu","Yuanjun Lv","Yunfei Chu","Zhou Zhao","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.19283v2.pdf","comment":"e.g.: 15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.03930v1","updated":"2024-10-04T21:13:58Z","published":"2024-10-04T21:13:58Z","title":"Reverb: Open-Source ASR and Diarization from Rev","summary":"  Today, we are open-sourcing our core speech recognition and diarization\nmodels for non-commercial use. We are releasing both a full production pipeline\nfor developers as well as pared-down research models for experimentation. Rev\nhopes that these releases will spur research and innovation in the fast-moving\ndomain of voice technology. The speech recognition models released today\noutperform all existing open source speech recognition models across a variety\nof long-form speech recognition domains.\n","authors":["Nishchal Bhandari","Danny Chen","Miguel Ángel del Río Fernández","Natalie Delworth","Jennifer Drexler Fox","Migüel Jetté","Quinten McNamara","Corey Miller","Ondřej Novotný","Ján Profant","Nan Qin","Martin Ratajczak","Jean-Philippe Robichaud"],"pdf_url":"https://arxiv.org/pdf/2410.03930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03904v1","updated":"2024-10-04T20:12:35Z","published":"2024-10-04T20:12:35Z","title":"Did You Hear That? Introducing AADG: A Framework for Generating\n  Benchmark Data in Audio Anomaly Detection","summary":"  We introduce a novel, general-purpose audio generation framework specifically\ndesigned for anomaly detection and localization. Unlike existing datasets that\npredominantly focus on industrial and machine-related sounds, our framework\nfocuses a broader range of environments, particularly useful in real-world\nscenarios where only audio data are available, such as in video-derived or\ntelephonic audio. To generate such data, we propose a new method inspired by\nthe LLM-Modulo framework, which leverages large language models(LLMs) as world\nmodels to simulate such real-world scenarios. This tool is modular allowing a\nplug-and-play approach. It operates by first using LLMs to predict plausible\nreal-world scenarios. An LLM further extracts the constituent sounds, the order\nand the way in which these should be merged to create coherent wholes. Much\nlike the LLM-Modulo framework, we include rigorous verification of each output\nstage, ensuring the reliability of the generated data. The data produced using\nthe framework serves as a benchmark for anomaly detection applications,\npotentially enhancing the performance of models trained on audio data,\nparticularly in handling out-of-distribution cases. Our contributions thus fill\na critical void in audio anomaly detection resources and provide a scalable\ntool for generating diverse, realistic audio data.\n","authors":["Ksheeraja Raghavan","Samiran Gode","Ankit Shah","Surabhi Raghavan","Wolfram Burgard","Bhiksha Raj","Rita Singh"],"pdf_url":"https://arxiv.org/pdf/2410.03904v1.pdf","comment":"9 pages, under review"},{"id":"http://arxiv.org/abs/2410.03879v1","updated":"2024-10-04T19:22:35Z","published":"2024-10-04T19:22:35Z","title":"SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual\n  Data","summary":"  We present SONIQUE, a model for generating background music tailored to video\ncontent. Unlike traditional video-to-music generation approaches, which rely\nheavily on paired audio-visual datasets, SONIQUE leverages unpaired data,\ncombining royalty-free music and independent video sources. By utilizing large\nlanguage models (LLMs) for video understanding and converting visual\ndescriptions into musical tags, alongside a U-Net-based conditional diffusion\nmodel, SONIQUE enables customizable music generation. Users can control\nspecific aspects of the music, such as instruments, genres, tempo, and\nmelodies, ensuring the generated output fits their creative vision. SONIQUE is\nopen-source, with a demo available online.\n","authors":["Liqian Zhang","Magdalena Fuentes"],"pdf_url":"https://arxiv.org/pdf/2410.03879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00822v2","updated":"2024-10-04T18:30:06Z","published":"2024-10-01T16:06:02Z","title":"VHASR: A Multimodal Speech Recognition System With Vision Hotwords","summary":"  The image-based multimodal automatic speech recognition (ASR) model enhances\nspeech recognition performance by incorporating audio-related image. However,\nsome works suggest that introducing image information to model does not help\nimproving ASR performance. In this paper, we propose a novel approach\neffectively utilizing audio-related image information and set up VHASR, a\nmultimodal speech recognition system that uses vision as hotwords to strengthen\nthe model's speech recognition capability. Our system utilizes a dual-stream\narchitecture, which firstly transcribes the text on the two streams separately,\nand then combines the outputs. We evaluate the proposed model on four datasets:\nFlickr8k, ADE20k, COCO, and OpenImages. The experimental results show that\nVHASR can effectively utilize key information in images to enhance the model's\nspeech recognition ability. Its performance not only surpasses unimodal ASR,\nbut also achieves SOTA among existing image-based multimodal ASR.\n","authors":["Jiliang Hu","Zuchao Li","Ping Wang","Haojun Ai","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.00822v2.pdf","comment":"14 pages, 6 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03813v1","updated":"2024-10-04T13:53:15Z","published":"2024-10-04T13:53:15Z","title":"SOI: Scaling Down Computational Complexity by Estimating Partial States\n  of the Model","summary":"  Consumer electronics used to follow the miniaturization trend described by\nMoore's Law. Despite increased processing power in Microcontroller Units\n(MCUs), MCUs used in the smallest appliances are still not capable of running\neven moderately big, state-of-the-art artificial neural networks (ANNs)\nespecially in time-sensitive scenarios. In this work, we present a novel method\ncalled Scattered Online Inference (SOI) that aims to reduce the computational\ncomplexity of ANNs. SOI leverages the continuity and seasonality of time-series\ndata and model predictions, enabling extrapolation for processing speed\nimprovements, particularly in deeper layers. By applying compression, SOI\ngenerates more general inner partial states of ANN, allowing skipping full\nmodel recalculation at each inference.\n","authors":["Grzegorz Stefański","Paweł Daniluk","Artur Szumaczuk","Jakub Tkaczuk"],"pdf_url":"https://arxiv.org/pdf/2410.03813v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.03798v1","updated":"2024-10-04T04:34:24Z","published":"2024-10-04T04:34:24Z","title":"Self-Powered LLM Modality Expansion for Large Speech-Text Models","summary":"  Large language models (LLMs) exhibit remarkable performance across diverse\ntasks, indicating their potential for expansion into large speech-text models\n(LSMs) by integrating speech capabilities. Although unified speech-text\npre-training and multimodal data instruction-tuning offer considerable\nbenefits, these methods generally entail significant resource demands and tend\nto overfit specific tasks. This study aims to refine the use of speech datasets\nfor LSM training by addressing the limitations of vanilla instruction tuning.\nWe explore the instruction-following dynamics within LSMs, identifying a\ncritical issue termed speech anchor bias-a tendency for LSMs to over-rely on\nspeech inputs, mistakenly interpreting the entire speech modality as\ndirectives, thereby neglecting textual instructions. To counteract this bias,\nwe introduce a self-powered LSM that leverages augmented automatic speech\nrecognition data generated by the model itself for more effective instruction\ntuning. Our experiments across a range of speech-based tasks demonstrate that\nself-powered LSM mitigates speech anchor bias and improves the fusion of speech\nand text modalities in LSMs. Data, code and scripts are freely available at\nhttps://github.com/ytf-philp/Self-powered-LSM.\n","authors":["Tengfei Yu","Xuebo Liu","Zhiyi Hou","Liang Ding","Dacheng Tao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03798v1.pdf","comment":"Accepted to EMNLP 2024"}]},"2024-10-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.05269v1","updated":"2024-10-07T17:59:58Z","published":"2024-10-07T17:59:58Z","title":"Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models","summary":"  Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.\n","authors":["Fei Wang","Ninareh Mehrabi","Palash Goyal","Rahul Gupta","Kai-Wei Chang","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2410.05269v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/"},{"id":"http://arxiv.org/abs/2410.05267v1","updated":"2024-10-07T17:59:48Z","published":"2024-10-07T17:59:48Z","title":"Grounding Partially-Defined Events in Multimodal Data","summary":"  How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.\n","authors":["Kate Sanders","Reno Kriz","David Etter","Hannah Recknor","Alexander Martin","Cameron Carpenter","Jingyang Lin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.05267v1.pdf","comment":"Preprint; 9 pages; 2024 EMNLP Findings"},{"id":"http://arxiv.org/abs/2406.11839v2","updated":"2024-10-07T17:59:42Z","published":"2024-06-17T17:59:58Z","title":"mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models","summary":"  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n","authors":["Fei Wang","Wenxuan Zhou","James Y. Huang","Nan Xu","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11839v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO"},{"id":"http://arxiv.org/abs/2410.05265v1","updated":"2024-10-07T17:59:35Z","published":"2024-10-07T17:59:35Z","title":"PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs","summary":"  Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.\n","authors":["Mengzhao Chen","Yi Liu","Jiahao Wang","Yi Bin","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05265v1.pdf","comment":"A PTQ method to significantly boost the performance of static\n  activation quantization"},{"id":"http://arxiv.org/abs/2410.05262v1","updated":"2024-10-07T17:58:47Z","published":"2024-10-07T17:58:47Z","title":"TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles","summary":"  As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"\n","authors":["Qingchen Yu","Shichao Song","Ke Fang","Yunfeng Shi","Zifan Zheng","Hanyu Wang","Simin Niu","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.05262v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2410.05258v1","updated":"2024-10-07T17:57:38Z","published":"2024-10-07T17:57:38Z","title":"Differential Transformer","summary":"  Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.\n","authors":["Tianzhu Ye","Li Dong","Yuqing Xia","Yutao Sun","Yi Zhu","Gao Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.05258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05254v1","updated":"2024-10-07T17:55:35Z","published":"2024-10-07T17:55:35Z","title":"GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments","summary":"  Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.\n","authors":["Eilam Shapira","Omer Madmon","Itamar Reinman","Samuel Joseph Amouyal","Roi Reichart","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2410.05254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05252v1","updated":"2024-10-07T17:55:10Z","published":"2024-10-07T17:55:10Z","title":"Causal Micro-Narratives","summary":"  We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.\n","authors":["Mourad Heddaya","Qingcheng Zeng","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2410.05252v1.pdf","comment":"Accepted to EMNLP 2024 Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2410.05248v1","updated":"2024-10-07T17:52:21Z","published":"2024-10-07T17:52:21Z","title":"SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe","summary":"  To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.\n","authors":["Yuxin Xiao","Shujian Zhang","Wenxuan Zhou","Marzyeh Ghassemi","Sanqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.05248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17975v2","updated":"2024-10-07T17:49:13Z","published":"2024-06-25T23:12:07Z","title":"SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)","summary":"  Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.\n","authors":["Matthieu Meeus","Igor Shilov","Shubham Jain","Manuel Faysse","Marek Rei","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2406.17975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05243v1","updated":"2024-10-07T17:47:50Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly take\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01574v5","updated":"2024-10-07T17:46:08Z","published":"2024-06-03T17:53:00Z","title":"MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark (Published at NeurIPS 2024 Track Datasets and\n  Benchmarks)","summary":"  In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.\n","authors":["Yubo Wang","Xueguang Ma","Ge Zhang","Yuansheng Ni","Abhranil Chandra","Shiguang Guo","Weiming Ren","Aaran Arulraj","Xuan He","Ziyan Jiang","Tianle Li","Max Ku","Kai Wang","Alex Zhuang","Rongqi Fan","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01574v5.pdf","comment":"This version has been accepted and published at NeurIPS 2024 Track\n  Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2410.05239v1","updated":"2024-10-07T17:42:53Z","published":"2024-10-07T17:42:53Z","title":"TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models","summary":"  Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.\n","authors":["Rabin Adhikari","Safal Thapaliya","Manish Dhakal","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2410.05239v1.pdf","comment":"Accepted at ACCV 2024 (oral presentation)"},{"id":"http://arxiv.org/abs/2410.05235v1","updated":"2024-10-07T17:41:45Z","published":"2024-10-07T17:41:45Z","title":"CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with\n  Explanatory Argumentative Structures","summary":"  Explaining Artificial Intelligence (AI) decisions is a major challenge\nnowadays in AI, in particular when applied to sensitive scenarios like medicine\nand law. However, the need to explain the rationale behind decisions is a main\nissue also for human-based deliberation as it is important to justify\n\\textit{why} a certain decision has been taken. Resident medical doctors for\ninstance are required not only to provide a (possibly correct) diagnosis, but\nalso to explain how they reached a certain conclusion. Developing new tools to\naid residents to train their explanation skills is therefore a central\nobjective of AI in education. In this paper, we follow this direction, and we\npresent, to the best of our knowledge, the first multilingual dataset for\nMedical Question Answering where correct and incorrect diagnoses for a clinical\ncase are enriched with a natural language explanation written by doctors. These\nexplanations have been manually annotated with argument components (i.e.,\npremise, claim) and argument relations (i.e., attack, support), resulting in\nthe Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases\nin four languages (English, Spanish, French, Italian) with explanations, where\nwe annotated 5021 claims, 2313 premises, 2431 support relations, and 1106\nattack relations. We conclude by showing how competitive baselines perform over\nthis challenging dataset for the argument mining task.\n","authors":["katerina Sviridova","Anar Yeginbergen","Ainara Estarrona","Elena Cabrio","Serena Villata","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.05235v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.05224v1","updated":"2024-10-07T17:29:40Z","published":"2024-10-07T17:29:40Z","title":"Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates","summary":"  Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.\n","authors":["Avanika Narayan","Mayee F. Chen","Kush Bhatia","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2410.05224v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2410.05222v1","updated":"2024-10-07T17:26:31Z","published":"2024-10-07T17:26:31Z","title":"Precise Model Benchmarking with Only a Few Observations","summary":"  How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.\n","authors":["Riccardo Fogliato","Pratik Patil","Nil-Jana Akpinar","Mathew Monfort"],"pdf_url":"https://arxiv.org/pdf/2410.05222v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.15877v3","updated":"2024-10-07T17:23:30Z","published":"2024-06-22T15:52:04Z","title":"BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions","summary":"  Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.\n","authors":["Terry Yue Zhuo","Minh Chien Vu","Jenny Chim","Han Hu","Wenhao Yu","Ratnadira Widyasari","Imam Nur Bani Yusuf","Haolan Zhan","Junda He","Indraneil Paul","Simon Brunner","Chen Gong","Thong Hoang","Armel Randy Zebaze","Xiaoheng Hong","Wen-Ding Li","Jean Kaddour","Ming Xu","Zhihan Zhang","Prateek Yadav","Naman Jain","Alex Gu","Zhoujun Cheng","Jiawei Liu","Qian Liu","Zijian Wang","David Lo","Binyuan Hui","Niklas Muennighoff","Daniel Fried","Xiaoning Du","Harm de Vries","Leandro Von Werra"],"pdf_url":"https://arxiv.org/pdf/2406.15877v3.pdf","comment":"44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)"},{"id":"http://arxiv.org/abs/2410.05218v1","updated":"2024-10-07T17:22:56Z","published":"2024-10-07T17:22:56Z","title":"Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories","summary":"  Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.\n","authors":["Toni J. B. Liu","Nicolas Boullé","Raphaël Sarfati","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.05218v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2309.02233v3","updated":"2024-10-07T17:21:45Z","published":"2023-09-05T13:39:38Z","title":"Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)","summary":"  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.\n","authors":["Yubo Wang","Xueguang Ma","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.02233v3.pdf","comment":"This version has been accepted and published at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.05210v1","updated":"2024-10-07T17:16:20Z","published":"2024-10-07T17:16:20Z","title":"Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality","summary":"  In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.\n","authors":["Youngtaek Oh","Jae Won Cho","Dong-Jin Kim","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.05210v1.pdf","comment":"EMNLP 2024 (Long, Main). Project page:\n  https://ytaek-oh.github.io/fsc-clip"},{"id":"http://arxiv.org/abs/2406.06369v4","updated":"2024-10-07T17:13:45Z","published":"2024-06-10T15:30:13Z","title":"Annotation alignment: Comparing LLM and human annotations of\n  conversational safety","summary":"  Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.\n","authors":["Rajiv Movva","Pang Wei Koh","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2406.06369v4.pdf","comment":"EMNLP 2024 (Main). Main text contains 6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.05206v1","updated":"2024-10-07T17:09:03Z","published":"2024-10-07T17:09:03Z","title":"Studying and Mitigating Biases in Sign Language Understanding Models","summary":"  Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.\n","authors":["Katherine Atwell","Danielle Bragg","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.05206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05193v1","updated":"2024-10-07T16:50:47Z","published":"2024-10-07T16:50:47Z","title":"RevisEval: Improving LLM-as-a-Judge via Response-Adapted References","summary":"  With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.\n","authors":["Qiyuan Zhang","Yufei Wang","Tiezheng YU","Yuxin Jiang","Chuhan Wu","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Fuyuan Lyu","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05192v1","updated":"2024-10-07T16:49:39Z","published":"2024-10-07T16:49:39Z","title":"Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss\n  Landscape Perspective","summary":"  Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\nat any time with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.\n","authors":["Kaiyue Wen","Zhiyuan Li","Jason Wang","David Hall","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05192v1.pdf","comment":"45 pages,13 figures"},{"id":"http://arxiv.org/abs/2410.02525v2","updated":"2024-10-07T16:46:05Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00099v4","updated":"2024-10-07T16:45:42Z","published":"2024-04-30T18:00:02Z","title":"Creative Beam Search: LLM-as-a-Judge For Improving Response Generation","summary":"  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2405.00099v4.pdf","comment":"Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)"},{"id":"http://arxiv.org/abs/2410.05183v1","updated":"2024-10-07T16:42:10Z","published":"2024-10-07T16:42:10Z","title":"Beyond Correlation: Interpretable Evaluation of Machine Translation\n  Metrics","summary":"  Machine Translation (MT) evaluation metrics assess translation quality\nautomatically. Recently, researchers have employed MT metrics for various new\nuse cases, such as data filtering and translation re-ranking. However, most MT\nmetrics return assessments as scalar scores that are difficult to interpret,\nposing a challenge to making informed design choices. Moreover, MT metrics'\ncapabilities have historically been evaluated using correlation with human\njudgment, which, despite its efficacy, falls short of providing intuitive\ninsights into metric performance, especially in terms of new metric use cases.\nTo address these issues, we introduce an interpretable evaluation framework for\nMT metrics. Within this framework, we evaluate metrics in two scenarios that\nserve as proxies for the data filtering and translation re-ranking use cases.\nFurthermore, by measuring the performance of MT metrics using Precision,\nRecall, and F-score, we offer clearer insights into their capabilities than\ncorrelation with human judgments. Finally, we raise concerns regarding the\nreliability of manually curated data following the Direct Assessments+Scalar\nQuality Metrics (DA+SQM) guidelines, reporting a notably low agreement with\nMultidimensional Quality Metrics (MQM) annotations.\n","authors":["Stefano Perrella","Lorenzo Proietti","Pere-Lluís Huguet Cabot","Edoardo Barba","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05183v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference. 26 pages"},{"id":"http://arxiv.org/abs/2410.05180v1","updated":"2024-10-07T16:40:21Z","published":"2024-10-07T16:40:21Z","title":"Enhancing Equity in Large Language Models for Medical Applications","summary":"  Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups.\n","authors":["Yuelyu Ji","Wenhe Ma","Sonish Sivarajkumar","Hang Zhang","Eugene Mathew Sadhu","Zhuochun Li","Xizhi Wu","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02381v2","updated":"2024-10-07T16:39:24Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.14901v2","updated":"2024-10-07T16:38:35Z","published":"2024-02-22T18:09:33Z","title":"A Usage-centric Take on Intent Understanding in E-Commerce","summary":"  Identifying and understanding user intents is a pivotal task for E-Commerce.\nDespite its essential role in product recommendation and business user\nprofiling analysis, intent understanding has not been consistently defined or\naccurately benchmarked. In this paper, we focus on predicative user intents as\n\"how a customer uses a product\", and pose intent understanding as a natural\nlanguage reasoning task, independent of product ontologies. We identify two\nweaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:\ncategory-rigidity and property-ambiguity. They limit its ability to strongly\nalign user intents with products having the most desirable property, and to\nrecommend useful products across diverse categories. Following these\nobservations, we introduce a Product Recovery Benchmark featuring a novel\nevaluation framework and an example dataset. We further validate the above\nFolkScope weaknesses on this benchmark. Our code and dataset are available at\nhttps://github.com/stayones/Usgae-Centric-Intent-Understanding.\n","authors":["Wendi Zhou","Tianyi Li","Pavlos Vougiouklis","Mark Steedman","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2402.14901v2.pdf","comment":"Acepted by EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2310.09675v2","updated":"2024-10-07T16:28:52Z","published":"2023-10-14T22:24:26Z","title":"Efficient Model-Agnostic Multi-Group Equivariant Networks","summary":"  Constructing model-agnostic group equivariant networks, such as equitune\n(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be\ncomputationally expensive for large product groups. We address this problem by\nproviding efficient model-agnostic equivariant designs for two related\nproblems: one where the network has multiple inputs each with potentially\ndifferent groups acting on them, and another where there is a single input but\nthe group acting on it is a large product group. For the first design, we\ninitially consider a linear model and characterize the entire equivariant space\nthat satisfies this constraint. This characterization gives rise to a novel\nfusion layer between different channels that satisfies an invariance-symmetry\n(IS) constraint, which we call an IS layer. We then extend this design beyond\nlinear models, similar to equitune, consisting of equivariant and IS layers. We\nalso show that the IS layer is a universal approximator of invariant-symmetric\nfunctions. Inspired by the first design, we use the notion of the IS property\nto design a second efficient model-agnostic equivariant design for large\nproduct groups acting on a single input. For the first design, we provide\nexperiments on multi-image classification where each view is transformed\nindependently with transformations such as rotations. We find equivariant\nmodels are robust to such transformations and perform competitively otherwise.\nFor the second design, we consider three applications: language\ncompositionality on the SCAN dataset to product groups; fairness in natural\nlanguage generation from GPT-2 to address intersectionality; and robust\nzero-shot image classification with CLIP. Overall, our methods are simple and\ngeneral, competitive with equitune and its variants, while also being\ncomputationally more efficient.\n","authors":["Razan Baltaji","Sourya Basu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.09675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v2","updated":"2024-10-07T16:26:00Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05168v1","updated":"2024-10-07T16:25:39Z","published":"2024-10-07T16:25:39Z","title":"ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation","summary":"  Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.05168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02902v2","updated":"2024-10-07T16:25:04Z","published":"2024-10-03T18:48:38Z","title":"Better Instruction-Following Through Minimum Bayes Risk","summary":"  General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.\n","authors":["Ian Wu","Patrick Fernandes","Amanda Bertsch","Seungone Kim","Sina Pakazad","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.02902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05165v1","updated":"2024-10-07T16:23:36Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05162v1","updated":"2024-10-07T16:14:47Z","published":"2024-10-07T16:14:47Z","title":"Deciphering the Interplay of Parametric and Non-parametric Memory in\n  Retrieval-augmented Language Models","summary":"  Generative language models often struggle with specialized or less-discussed\nknowledge. A potential solution is found in Retrieval-Augmented Generation\n(RAG) models which act like retrieving information before generating responses.\nIn this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides\nbetween what it already knows (parametric) and what it retrieves\n(non-parametric). We use causal mediation analysis and controlled experiments\nto examine how internal representations influence information processing. Our\nfindings disentangle the effects of parametric knowledge and the retrieved\ncontext. They indicate that in cases where the model can choose between both\ntypes of information (parametric and non-parametric), it relies more on the\ncontext than the parametric knowledge. Furthermore, the analysis investigates\nthe computations involved in \\emph{how} the model uses the information from the\ncontext. We find that multiple mechanisms are active within the model and can\nbe detected with mediation analysis: first, the decision of \\emph{whether the\ncontext is relevant}, and second, how the encoder computes output\nrepresentations to support copying when relevant.\n","authors":["Mehrdad Farahani","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2410.05162v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05160v1","updated":"2024-10-07T16:14:05Z","published":"2024-10-07T16:14:05Z","title":"VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks","summary":"  Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.\n","authors":["Ziyan Jiang","Rui Meng","Xinyi Yang","Semih Yavuz","Yingbo Zhou","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05160v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2405.14577v2","updated":"2024-10-07T16:01:49Z","published":"2024-05-23T13:51:55Z","title":"Representation noising effectively prevents harmful fine-tuning on LLMs","summary":"  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","Łukasz Bartoszcze","David Atanasov","Robie Gonzales","Subhabrata Majumdar","Carsten Maple","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2405.14577v2.pdf","comment":"Published in NeurIPs 2024"},{"id":"http://arxiv.org/abs/2311.09090v4","updated":"2024-10-07T16:01:06Z","published":"2023-11-15T16:35:59Z","title":"Social Bias Probing: Fairness Benchmarking for Language Models","summary":"  While the impact of social biases in language models has been recognized,\nprior methods for bias evaluation have been limited to binary association tests\non small datasets, limiting our understanding of bias complexities. This paper\nproposes a novel framework for probing language models for social biases by\nassessing disparate treatment, which involves treating individuals differently\naccording to their affiliation with a sensitive demographic group. We curate\nSoFa, a large-scale benchmark designed to address the limitations of existing\nfairness collections. SoFa expands the analysis beyond the binary comparison of\nstereotypical versus anti-stereotypical identities to include a diverse range\nof identities and stereotypes. Comparing our methodology with existing\nbenchmarks, we reveal that biases within language models are more nuanced than\nacknowledged, indicating a broader scope of encoded biases than previously\nrecognized. Benchmarking LMs on SoFa, we expose how identities expressing\ndifferent religions lead to the most pronounced disparate treatments across all\nmodels. Finally, our findings indicate that real-life adversities faced by\nvarious groups such as women and people with disabilities are mirrored in the\nbehavior of these models.\n","authors":["Marta Marchiori Manerba","Karolina Stańczak","Riccardo Guidotti","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2311.09090v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05146v1","updated":"2024-10-07T15:58:03Z","published":"2024-10-07T15:58:03Z","title":"CTC-GMM: CTC guided modality matching for fast and accurate streaming\n  speech translation","summary":"  Models for streaming speech translation (ST) can achieve high accuracy and\nlow latency if they're developed with vast amounts of paired audio in the\nsource language and written text in the target language. Yet, these text labels\nfor the target language are often pseudo labels due to the prohibitive cost of\nmanual ST data labeling. In this paper, we introduce a methodology named\nConnectionist Temporal Classification guided modality matching (CTC-GMM) that\nenhances the streaming ST model by leveraging extensive machine translation\n(MT) text data. This technique employs CTC to compress the speech sequence into\na compact embedding sequence that matches the corresponding text sequence,\nallowing us to utilize matched {source-target} language text pairs from the MT\ncorpora to refine the streaming ST model further. Our evaluations with FLEURS\nand CoVoST2 show that the CTC-GMM approach can increase translation accuracy\nrelatively by 13.9% and 6.4% respectively, while also boosting decoding speed\nby 59.7% on GPU.\n","authors":["Rui Zhao","Jinyu Li","Ruchao Fan","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2410.05146v1.pdf","comment":"Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)"},{"id":"http://arxiv.org/abs/2407.10930v2","updated":"2024-10-07T15:52:48Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.00126v2","updated":"2024-10-07T15:44:45Z","published":"2024-02-29T21:05:37Z","title":"FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition","summary":"  Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.\n","authors":["Xiaoqiang Wang","Lingfei Wu","Tengfei Ma","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00126v2.pdf","comment":"Accepted at EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2404.12132v2","updated":"2024-10-07T15:28:18Z","published":"2024-04-18T12:33:57Z","title":"Non-Invasive Suicide Risk Prediction Through Speech Analysis","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17385v2","updated":"2024-10-07T15:15:18Z","published":"2024-06-25T09:04:21Z","title":"Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance","summary":"  Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.\n","authors":["Manon Reusens","Philipp Borchert","Jochen De Weerdt","Bart Baesens"],"pdf_url":"https://arxiv.org/pdf/2406.17385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05257v2","updated":"2024-10-07T15:14:26Z","published":"2024-09-09T00:40:47Z","title":"UPCS: Unbiased Persona Construction for Dialogue Generation","summary":"  Narrative systems, such as dialogue and storytelling systems, often utilize\npersona profiles to enhance personalized interactions. Existing persona\nprofiles frequently exhibit biases, posing risks to system integrity and\nfairness. To address this, we introduce the UPCS framework, which categorizes\ncharacter descriptions into eight dimensions, including bias mitigation\nstrategies. Experimental results demonstrate UPCS's superiority in accuracy,\ndiversity, bias elimination, and user satisfaction, marking a significant\nadvancement in persona construction for reliable narrative systems.\n","authors":["Kuiyun Chen","Yanbin Wei"],"pdf_url":"https://arxiv.org/pdf/2409.05257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11472v3","updated":"2024-10-07T15:11:12Z","published":"2022-05-23T17:14:32Z","title":"Diversity Over Size: On the Effect of Sample and Topic Sizes for\n  Topic-Dependent Argument Mining Datasets","summary":"  The task of Argument Mining, that is extracting and classifying argument\ncomponents for a specific topic from large document sources, is an inherently\ndifficult task for machine learning models and humans alike, as large Argument\nMining datasets are rare and recognition of argument components requires expert\nknowledge. The task becomes even more difficult if it also involves stance\ndetection of retrieved arguments. In this work, we investigate the effect of\nArgument Mining dataset composition in few- and zero-shot settings. Our\nfindings show that, while fine-tuning is mandatory to achieve acceptable model\nperformance, using carefully composed training samples and reducing the\ntraining sample size by up to almost 90% can still yield 95% of the maximum\nperformance. This gain is consistent across three Argument Mining tasks on\nthree different datasets. We also publish a new dataset for future\nbenchmarking.\n","authors":["Benjamin Schiller","Johannes Daxenberger","Andreas Waldis","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2205.11472v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00161v2","updated":"2024-10-07T15:07:09Z","published":"2024-09-30T19:09:13Z","title":"KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head","summary":"  Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.\n","authors":["Isaac Rehg"],"pdf_url":"https://arxiv.org/pdf/2410.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15929v2","updated":"2024-10-07T15:01:48Z","published":"2024-02-24T23:16:57Z","title":"Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs","summary":"  Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.\n","authors":["Isha Chaudhary","Vedaant V. Jain","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2402.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05102v1","updated":"2024-10-07T15:01:29Z","published":"2024-10-07T15:01:29Z","title":"SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks","summary":"  Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.\n","authors":["Fenia Christopoulou","Ronald Cardenas","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05102v1.pdf","comment":"20 papges, 9 figures, 5 tables. Under Review"},{"id":"http://arxiv.org/abs/2406.16078v2","updated":"2024-10-07T15:01:26Z","published":"2024-06-23T11:11:46Z","title":"First Heuristic Then Rational: Dynamic Use of Heuristics in Language\n  Model Reasoning","summary":"  Multi-step reasoning instruction, such as chain-of-thought prompting, is\nwidely adopted to explore better language models (LMs) performance. We report\non the systematic strategy that LMs employ in such a multi-step reasoning\nprocess. Our controlled experiments reveal that LMs rely more heavily on\nheuristics, such as lexical overlap, in the earlier stages of reasoning, where\nmore reasoning steps remain to reach a goal. Conversely, their reliance on\nheuristics decreases as LMs progress closer to the final answer through\nmultiple reasoning steps. This suggests that LMs can backtrack only a limited\nnumber of future steps and dynamically combine heuristic strategies with\nrationale ones in tasks involving multi-step reasoning.\n","authors":["Yoichi Aoki","Keito Kudo","Tatsuki Kuribayashi","Shusaku Sone","Masaya Taniguchi","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2406.16078v2.pdf","comment":"This paper is accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05099v1","updated":"2024-10-07T14:55:20Z","published":"2024-10-07T14:55:20Z","title":"Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances","summary":"  Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.\n","authors":["Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2410.05099v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2410.02707v2","updated":"2024-10-07T14:46:11Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16671v7","updated":"2024-10-07T14:44:44Z","published":"2024-02-26T15:47:01Z","title":"StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding","summary":"  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.\n","authors":["Alex Zhuang","Ge Zhang","Tianyu Zheng","Xinrun Du","Junjie Wang","Weiming Ren","Stephen W. Huang","Jie Fu","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16671v7.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05085v1","updated":"2024-10-07T14:39:45Z","published":"2024-10-07T14:39:45Z","title":"Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification","summary":"  Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.\n","authors":["Jeremie Bogaert","Marie-Catherine de Marneffe","Antonin Descampe","Louis Escouflaire","Cedrick Fairon","Francois-Xavier Standaert"],"pdf_url":"https://arxiv.org/pdf/2410.05085v1.pdf","comment":"This paper is a faithful translation of a paper which was\n  peer-reviewed and published in the French journal Traitement Automatique des\n  Langues, n. 64"},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05080v1","updated":"2024-10-07T14:33:50Z","published":"2024-10-07T14:33:50Z","title":"ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery","summary":"  The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.\n","authors":["Ziru Chen","Shijie Chen","Yuting Ning","Qianheng Zhang","Boshi Wang","Botao Yu","Yifei Li","Zeyi Liao","Chen Wei","Zitong Lu","Vishal Dey","Mingyi Xue","Frazier N. Baker","Benjamin Burns","Daniel Adu-Ampratwum","Xuhui Huang","Xia Ning","Song Gao","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05080v1.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2410.05077v1","updated":"2024-10-07T14:31:43Z","published":"2024-10-07T14:31:43Z","title":"ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering","summary":"  Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.\n","authors":["Francesco Maria Molfese","Simone Conia","Riccardo Orlando","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05077v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.05076v1","updated":"2024-10-07T14:30:27Z","published":"2024-10-07T14:30:27Z","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention","summary":"  Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.\n","authors":["Lijie Yang","Zhihao Zhang","Zhuofu Chen","Zikun Li","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2410.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12034v2","updated":"2024-10-07T14:27:56Z","published":"2024-06-17T19:06:54Z","title":"Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts","summary":"  We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.\n","authors":["Junmo Kang","Leonid Karlinsky","Hongyin Luo","Zhen Wang","Jacob Hansen","James Glass","David Cox","Rameswar Panda","Rogerio Feris","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2406.12034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02325v2","updated":"2024-10-07T14:26:18Z","published":"2021-12-04T13:18:12Z","title":"A Russian Jeopardy! Data Set for Question-Answering Systems","summary":"  Question answering (QA) is one of the most common NLP tasks that relates to\nnamed entity recognition, fact extraction, semantic search and some other\nfields. In industry, it is much appreciated in chatbots and corporate\ninformation systems. It is also a challenging task that attracted the attention\nof a very general audience at the quiz show Jeopardy! In this article we\ndescribe a Jeopardy!-like Russian QA data set collected from the official\nRussian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like\nquestions with 29,375 from the Russian analogue of Jeopardy! - \"Own Game\". We\nobserve its linguistic features and the related QA-task. We conclude about\nperspectives of a QA competition based on the data set collected from this\ndatabase.\n","authors":["Elena Mikhalkova"],"pdf_url":"https://arxiv.org/pdf/2112.02325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19745v2","updated":"2024-10-07T14:17:44Z","published":"2024-09-29T15:40:54Z","title":"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead","summary":"  Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability.\n","authors":["Tao Tan","Yining Qian","Ang Lv","Hongzhan Lin","Songhao Wu","Yongbo Wang","Feng Wang","Jingtong Wu","Xin Lu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19745v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.05052v1","updated":"2024-10-07T14:09:58Z","published":"2024-10-07T14:09:58Z","title":"Initialization of Large Language Models via Reparameterization to\n  Mitigate Loss Spikes","summary":"  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a\nfundamental issue in the pre-training of large language models. This paper\nsupposes that the non-uniformity of the norm of the parameters is one of the\ncauses of loss spikes. Here, in training of neural networks, the scale of the\ngradients is required to be kept constant throughout the layers to avoid the\nvanishing and exploding gradients problem. However, to meet these requirements\nin the Transformer model, the norm of the model parameters must be non-uniform,\nand thus, parameters whose norm is smaller are more sensitive to the parameter\nupdate. To address this issue, we propose a novel technique, weight scaling as\nreparameterization (WeSaR). WeSaR introduces a gate parameter per parameter\nmatrix and adjusts it to the value satisfying the requirements. Because of the\ngate parameter, WeSaR sets the norm of the original parameters uniformly, which\nresults in stable training. Experimental results with the Transformer decoders\nconsisting of 130 million, 1.3 billion, and 13 billion parameters showed that\nWeSaR stabilizes and accelerates training and that it outperformed compared\nmethods including popular initialization methods.\n","authors":["Kosuke Nishida","Kyosuke Nishida","Kuniko Saito"],"pdf_url":"https://arxiv.org/pdf/2410.05052v1.pdf","comment":"EMNLP2024 accepted"},{"id":"http://arxiv.org/abs/2406.12058v4","updated":"2024-10-07T14:08:13Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v4.pdf","comment":"Accepted in BlackboxNLP @ EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05047v1","updated":"2024-10-07T14:01:20Z","published":"2024-10-07T14:01:20Z","title":"A test suite of prompt injection attacks for LLM-based machine\n  translation","summary":"  LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.\n","authors":["Antonio Valerio Miceli-Barone","Zhifan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05046v1","updated":"2024-10-07T14:00:18Z","published":"2024-10-07T14:00:18Z","title":"Named Clinical Entity Recognition Benchmark","summary":"  This technical report introduces a Named Clinical Entity Recognition\nBenchmark for evaluating language models in healthcare, addressing the crucial\nnatural language processing (NLP) task of extracting structured information\nfrom clinical narratives to support applications like automated coding,\nclinical trial cohort identification, and clinical decision support.\n  The leaderboard provides a standardized platform for assessing diverse\nlanguage models, including encoder and decoder architectures, on their ability\nto identify and classify clinical entities across multiple medical domains. A\ncurated collection of openly available clinical datasets is utilized,\nencompassing entities such as diseases, symptoms, medications, procedures, and\nlaboratory measurements. Importantly, these entities are standardized according\nto the Observational Medical Outcomes Partnership (OMOP) Common Data Model,\nensuring consistency and interoperability across different healthcare systems\nand datasets, and a comprehensive evaluation of model performance. Performance\nof models is primarily assessed using the F1-score, and it is complemented by\nvarious assessment modes to provide comprehensive insights into model\nperformance. The report also includes a brief analysis of models evaluated to\ndate, highlighting observed trends and limitations.\n  By establishing this benchmarking framework, the leaderboard aims to promote\ntransparency, facilitate comparative analyses, and drive innovation in clinical\nentity recognition tasks, addressing the need for robust evaluation methods in\nhealthcare NLP.\n","authors":["Wadood M Abdul","Marco AF Pimentel","Muhammad Umar Salman","Tathagata Raha","Clément Christophe","Praveen K Kanithi","Nasir Hayat","Ronnie Rajan","Shadab Khan"],"pdf_url":"https://arxiv.org/pdf/2410.05046v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05045v1","updated":"2024-10-07T14:00:08Z","published":"2024-10-07T14:00:08Z","title":"Can LLMs plan paths with extra hints from solvers?","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.\n","authors":["Erik Wu","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2410.05045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05021v1","updated":"2024-10-07T13:24:24Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15160v2","updated":"2024-10-07T13:19:53Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","authors":["Gilad Yehudai","Haim Kaplan","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2407.15160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05018v1","updated":"2024-10-07T13:19:08Z","published":"2024-10-07T13:19:08Z","title":"On the Biased Assessment of Expert Finding Systems","summary":"  In large organisations, identifying experts on a given topic is crucial in\nleveraging the internal knowledge spread across teams and departments.\nSo-called enterprise expert retrieval systems automatically discover and\nstructure employees' expertise based on the vast amount of heterogeneous data\navailable about them and the work they perform. Evaluating these systems\nrequires comprehensive ground truth expert annotations, which are hard to\nobtain. Therefore, the annotation process typically relies on automated\nrecommendations of knowledge areas to validate. This case study provides an\nanalysis of how these recommendations can impact the evaluation of expert\nfinding systems. We demonstrate on a popular benchmark that system-validated\nannotations lead to overestimated performance of traditional term-based\nretrieval models and even invalidate comparisons with more recent neural\nmethods. We also augment knowledge areas with synonyms to uncover a strong bias\ntowards literal mentions of their constituent words. Finally, we propose\nconstraints to the annotation process to prevent these biased evaluations, and\nshow that this still allows annotation suggestions of high utility. These\nfindings should inform benchmark creation or selection for expert finding, to\nguarantee meaningful comparison of methods.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Chris Develder","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2410.05018v1.pdf","comment":"Accepted to the 4th Workshop on Recommender Systems for Human\n  Resources (RecSys in HR 2024) as part of RecSys 2024"},{"id":"http://arxiv.org/abs/2402.18376v2","updated":"2024-10-07T13:17:03Z","published":"2024-02-28T14:52:15Z","title":"Tokenization Is More Than Compression","summary":"  Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n","authors":["Craig W. Schmidt","Varshini Reddy","Haoran Zhang","Alec Alameddine","Omri Uzan","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2402.18376v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05006v1","updated":"2024-10-07T13:05:26Z","published":"2024-10-07T13:05:26Z","title":"SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness","summary":"  Accurately modeling the relationships between skills is a crucial part of\nhuman resources processes such as recruitment and employee development. Yet, no\nbenchmarks exist to evaluate such methods directly. We construct and release\nSkillMatch, a benchmark for the task of skill relatedness, based on expert\nknowledge mining from millions of job ads. Additionally, we propose a scalable\nself-supervised learning technique to adapt a Sentence-BERT model based on\nskill co-occurrence in job ads. This new method greatly surpasses traditional\nmodels for skill relatedness as measured on SkillMatch. By releasing SkillMatch\npublicly, we aim to contribute a foundation for research towards increased\naccuracy and transparency of skill-based recommendation systems.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Thomas Demeester","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2410.05006v1.pdf","comment":"Accepted to the International workshop on AI for Human Resources and\n  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2406.04866v2","updated":"2024-10-07T12:32:24Z","published":"2024-06-07T12:01:59Z","title":"ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question\n  Answering","summary":"  We introduce ComplexTempQA, a large-scale dataset consisting of over 100\nmillion question-answer pairs designed to tackle the challenges in temporal\nquestion answering. ComplexTempQA significantly surpasses existing benchmarks\nlike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from\nWikipedia and Wikidata, the dataset covers questions spanning over two decades\nand offers an unmatched breadth of topics. We introduce a unique taxonomy that\ncategorizes questions as attributes, comparisons, and counting questions, each\nrevolving around events, entities, and time periods. One standout feature of\nComplexTempQA is the high complexity of its questions, which demand effective\ncapabilities for answering such as across-time comparison, temporal\naggregation, and multi-hop reasoning involving temporal event ordering and\nentity recognition. Additionally, each question is accompanied by detailed\nmetadata, including specific time scopes, allowing for comprehensive evaluation\nand enhancement of the temporal reasoning abilities of large language models.\nComplexTempQA serves both as a testing ground for developing sophisticated AI\nmodels and as a foundation for advancing research in question answering,\ninformation retrieval, and language understanding.\n","authors":["Raphael Gruber","Abdelrahman Abdallah","Michael Färber","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2406.04866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04981v1","updated":"2024-10-07T12:22:06Z","published":"2024-10-07T12:22:06Z","title":"On the Rigour of Scientific Writing: Criteria, Analysis, and Insights","summary":"  Rigour is crucial for scientific research as it ensures the reproducibility\nand validity of results and findings. Despite its importance, little work\nexists on modelling rigour computationally, and there is a lack of analysis on\nwhether these criteria can effectively signal or measure the rigour of\nscientific papers in practice. In this paper, we introduce a bottom-up,\ndata-driven framework to automatically identify and define rigour criteria and\nassess their relevance in scientific writing. Our framework includes rigour\nkeyword extraction, detailed rigour definition generation, and salient criteria\nidentification. Furthermore, our framework is domain-agnostic and can be\ntailored to the evaluation of scientific rigour for different areas,\naccommodating the distinct salient criteria across fields. We conducted\ncomprehensive experiments based on datasets collected from two high impact\nvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the\neffectiveness of our framework in modelling rigour. In addition, we analyse\nlinguistic patterns of rigour, revealing that framing certainty is crucial for\nenhancing the perception of scientific rigour, while suggestion certainty and\nprobability uncertainty diminish it.\n","authors":["Joseph James","Chenghao Xiao","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04981v1.pdf","comment":"Accepted Findings at EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.02987v2","updated":"2024-10-07T12:11:58Z","published":"2024-02-05T13:18:42Z","title":"Reconstruct Your Previous Conversations! Comprehensively Investigating\n  Privacy Leakage Risks in Conversations with GPT Models","summary":"  Significant advancements have recently been made in large language models\nrepresented by GPT models. Users frequently have multi-round private\nconversations with cloud-hosted GPT models for task optimization. Yet, this\noperational paradigm introduces additional attack surfaces, particularly in\ncustom GPTs and hijacked chat sessions. In this paper, we introduce a\nstraightforward yet potent Conversation Reconstruction Attack. This attack\ntargets the contents of previous conversations between GPT models and benign\nusers, i.e., the benign users' input contents during their interaction with GPT\nmodels. The adversary could induce GPT models to leak such contents by querying\nthem with designed malicious prompts. Our comprehensive examination of privacy\nrisks during the interactions with GPT models under this attack reveals GPT-4's\nconsiderable resilience. We present two advanced attacks targeting improved\nreconstruction of past conversations, demonstrating significant privacy leakage\nacross all models under these advanced techniques. Evaluating various defense\nmechanisms, we find them ineffective against these attacks. Our findings\nhighlight the ease with which privacy can be compromised in interactions with\nGPT models, urging the community to safeguard against potential abuses of these\nmodels' capabilities.\n","authors":["Junjie Chu","Zeyang Sha","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02987v2.pdf","comment":"Accepted in EMNLP 2024. 14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.19339v2","updated":"2024-10-07T12:05:55Z","published":"2024-09-28T12:49:16Z","title":"Visual Question Decomposition on Multimodal Large Language Models","summary":"  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n","authors":["Haowei Zhang","Jianzhe Liu","Zhen Han","Shuo Chen","Bailan He","Volker Tresp","Zhiqiang Xu","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2409.19339v2.pdf","comment":"Accepted to EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.04962v1","updated":"2024-10-07T12:01:32Z","published":"2024-10-07T12:01:32Z","title":"Activation Scaling for Steering and Interpreting Language Models","summary":"  Given the prompt \"Rome is in\", can we steer a language model to flip its\nprediction of an incorrect token \"France\" to a correct token \"Italy\" by only\nmultiplying a few relevant activation vectors with scalars? We argue that\nsuccessfully intervening on a model is a prerequisite for interpreting its\ninternal workings. Concretely, we establish a three-term objective: a\nsuccessful intervention should flip the correct with the wrong token and vice\nversa (effectiveness), and leave other tokens unaffected (faithfulness), all\nwhile being sparse (minimality). Using gradient-based optimization, this\nobjective lets us learn (and later evaluate) a specific kind of efficient and\ninterpretable intervention: activation scaling only modifies the signed\nmagnitude of activation vectors to strengthen, weaken, or reverse the steering\ndirections already encoded in the model. On synthetic tasks, this intervention\nperforms comparably with steering vectors in terms of effectiveness and\nfaithfulness, but is much more minimal allowing us to pinpoint interpretable\nmodel components. We evaluate activation scaling from different angles, compare\nperformance on different datasets, and make activation scalars a learnable\nfunction of the activation vectors themselves to generalize to varying-length\nprompts.\n","authors":["Niklas Stoehr","Kevin Du","Vésteinn Snæbjarnarson","Robert West","Ryan Cotterell","Aaron Schein"],"pdf_url":"https://arxiv.org/pdf/2410.04962v1.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.17023v2","updated":"2024-10-07T11:59:37Z","published":"2024-07-24T06:06:07Z","title":"DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models","summary":"  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.\n","authors":["Sara Vera Marjanović","Haeun Yu","Pepa Atanasova","Maria Maistro","Christina Lioma","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2407.17023v2.pdf","comment":"15 pages, 6 figures, Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.04185v2","updated":"2024-10-07T11:54:11Z","published":"2024-09-06T11:01:55Z","title":"Residual Stream Analysis with Multi-Layer SAEs","summary":"  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, SAEs are\nusually trained separately on each transformer layer, making it difficult to\nuse them to study how information flows across layers. To solve this problem,\nwe introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer. Given that the residual\nstream is understood to preserve information across layers, we expected MLSAE\nlatents to `switch on' at a token position and remain active at later layers.\nInterestingly, we find that individual latents are often active at a single\nlayer for a given token or prompt, but this layer may differ for different\ntokens or prompts. We quantify these phenomena by defining a distribution over\nlayers and considering its variance. We find that the variance of the\ndistributions of latent activations over layers is about two orders of\nmagnitude greater when aggregating over tokens compared with a single token.\nFor larger underlying models, the degree to which latents are active at\nmultiple layers increases, which is consistent with the fact that the residual\nstream activation vectors at adjacent layers become more similar. Finally, we\nrelax the assumption that the residual stream basis is the same at every layer\nby applying pre-trained tuned-lens transformations, but our findings remain\nqualitatively similar. Our results represent a new approach to understanding\nhow representations change as they flow through transformers. We release our\ncode to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.\n","authors":["Tim Lawson","Lucy Farnik","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2409.04185v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2407.10805v4","updated":"2024-10-07T11:52:50Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Cehao Yang","Jiaxin Mao","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14883v2","updated":"2024-10-07T11:37:44Z","published":"2024-04-23T10:09:46Z","title":"Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans","summary":"  Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.\n","authors":["Vittoria Dentella","Fritz Guenther","Evelina Leivada"],"pdf_url":"https://arxiv.org/pdf/2404.14883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04925v1","updated":"2024-10-07T11:17:05Z","published":"2024-10-07T11:17:05Z","title":"Intent Classification for Bank Chatbots through LLM Fine-Tuning","summary":"  This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.\n","authors":["Bibiána Lajčinová","Patrik Valábek","Michal Spišiak"],"pdf_url":"https://arxiv.org/pdf/2410.04925v1.pdf","comment":"7 pages, no figures"},{"id":"http://arxiv.org/abs/2401.05930v4","updated":"2024-10-07T09:58:48Z","published":"2024-01-11T14:09:09Z","title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully","summary":"  Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks.\n","authors":["Jushi Kai","Tianhang Zhang","Hai Hu","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2401.05930v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.04878v1","updated":"2024-10-07T09:57:59Z","published":"2024-10-07T09:57:59Z","title":"Leveraging Grammar Induction for Language Understanding and Generation","summary":"  Grammar induction has made significant progress in recent years. However, it\nis not clear how the application of induced grammar could enhance practical\nperformance in downstream tasks. In this work, we introduce an unsupervised\ngrammar induction method for language understanding and generation. We\nconstruct a grammar parser to induce constituency structures and dependency\nrelations, which is simultaneously trained on downstream tasks without\nadditional syntax annotations. The induced grammar features are subsequently\nincorporated into Transformer as a syntactic mask to guide self-attention. We\nevaluate and apply our method to multiple machine translation tasks and natural\nlanguage understanding tasks. Our method demonstrates superior performance\ncompared to the original Transformer and other models enhanced with external\nparsers. Experimental results indicate that our method is effective in both\nfrom-scratch and pre-trained scenarios. Additionally, our research highlights\nthe contribution of explicitly modeling the grammatical structure of texts to\nneural network models.\n","authors":["Jushi Kai","Shengyuan Hou","Yusheng Huang","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04878v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.15625v2","updated":"2024-10-07T09:49:08Z","published":"2024-08-28T08:25:22Z","title":"CBF-LLM: Safe Control for LLM Alignment","summary":"  This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.\n","authors":["Yuya Miyaoka","Masaki Inoue"],"pdf_url":"https://arxiv.org/pdf/2408.15625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15489v2","updated":"2024-10-07T08:55:15Z","published":"2024-07-22T09:16:30Z","title":"A Comparison of Language Modeling and Translation as Multilingual\n  Pretraining Objectives","summary":"  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing best practices in\npretraining has, therefore, become a major focus of NLP research, especially\nsince insights gained from monolingual English models may not necessarily apply\nto more complex multilingual models. One significant caveat of the current\nstate of the art is that different works are rarely comparable: they often\ndiscuss different parameter counts, training data, and evaluation methodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pretraining objective\nunder the right conditions. We make our code, data, and model weights available\nat \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n","authors":["Zihao Li","Shaoxiong Ji","Timothee Mickus","Vincent Segonne","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2407.15489v2.pdf","comment":"Proceedings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.04838v1","updated":"2024-10-07T08:53:00Z","published":"2024-10-07T08:53:00Z","title":"Rationale-Aware Answer Verification by Pairwise Self-Evaluation","summary":"  Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.\n","authors":["Akira Kawabata","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2410.04838v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.00545v2","updated":"2024-10-07T08:52:39Z","published":"2024-10-01T09:38:34Z","title":"What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine\n  Translation with a Human-centered Study","summary":"  Gender bias in machine translation (MT) is recognized as an issue that can\nharm people and society. And yet, advancements in the field rarely involve\npeople, the final MT users, or inform how they might be impacted by biased\ntechnologies. Current evaluations are often restricted to automatic methods,\nwhich offer an opaque estimate of what the downstream impact of gender\ndisparities might be. We conduct an extensive human-centered study to examine\nif and to what extent bias in MT brings harms with tangible costs, such as\nquality of service gaps across women and men. To this aim, we collect\nbehavioral data from 90 participants, who post-edited MT outputs to ensure\ncorrect gender translation. Across multiple datasets, languages, and types of\nusers, our study shows that feminine post-editing demands significantly more\ntechnical and temporal effort, also corresponding to higher financial costs.\nExisting bias measurements, however, fail to reflect the found disparities. Our\nfindings advocate for human-centered approaches that can inform the societal\nimpact of bias.\n","authors":["Beatrice Savoldi","Sara Papi","Matteo Negri","Ana Guerberof","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2410.00545v2.pdf","comment":"Accepted ad EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.06551v2","updated":"2024-10-07T08:49:49Z","published":"2024-07-09T05:16:22Z","title":"OffsetBias: Leveraging Debiased Data for Tuning Evaluators","summary":"  Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public.\n","authors":["Junsoo Park","Seungyeon Jwa","Meiying Ren","Daeyoung Kim","Sanghyuk Choi"],"pdf_url":"https://arxiv.org/pdf/2407.06551v2.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2408.08313v2","updated":"2024-10-07T08:44:35Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v2.pdf","comment":"Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)"},{"id":"http://arxiv.org/abs/2410.04834v1","updated":"2024-10-07T08:44:04Z","published":"2024-10-07T08:44:04Z","title":"As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss","summary":"  Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.\n","authors":["Xin Mao","Feng-Lin Li","Huimin Xu","Wei Zhang","Wang Chen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2410.04834v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.02298v2","updated":"2024-10-07T08:40:35Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.04819v1","updated":"2024-10-07T08:13:16Z","published":"2024-10-07T08:13:16Z","title":"MINER: Mining the Underlying Pattern of Modality-Specific Neurons in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have significantly\nadvanced, integrating more modalities into diverse applications. However, the\nlack of explainability remains a major barrier to their use in scenarios\nrequiring decision transparency. Current neuron-level explanation paradigms\nmainly focus on knowledge localization or language- and domain-specific\nanalyses, leaving the exploration of multimodality largely unaddressed. To\ntackle these challenges, we propose MINER, a transferable framework for mining\nmodality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)\nmodality separation, (2) importance score calculation, (3) importance score\naggregation, (4) modality-specific neuron selection. Extensive experiments\nacross six benchmarks and two representative MLLMs show that (I) deactivating\nONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for\nQwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly\nconverge in the lower layers, (III) MSNs influence how key information from\nvarious modalities converges to the last token, (IV) two intriguing phenomena\nworth further investigation, i.e., semantic probing and semantic telomeres. The\nsource code is available at this URL.\n","authors":["Kaichen Huang","Jiahao Huo","Yibo Yan","Kun Wang","Yutao Yue","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00016v3","updated":"2024-10-07T07:54:37Z","published":"2024-07-28T15:45:08Z","title":"Towards a Universal Method for Meaningful Signal Detection","summary":"  It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.\n","authors":["Louis Mahon"],"pdf_url":"https://arxiv.org/pdf/2408.00016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14399v2","updated":"2024-10-07T07:49:27Z","published":"2024-09-22T11:35:59Z","title":"Beyond Persuasion: Towards Conversational Recommender System with\n  Credible Explanations","summary":"  With the aid of large language models, current conversational recommender\nsystem (CRS) has gaining strong abilities to persuade users to accept\nrecommended items. While these CRSs are highly persuasive, they can mislead\nusers by incorporating incredible information in their explanations, ultimately\ndamaging the long-term trust between users and the CRS. To address this, we\npropose a simple yet effective method, called PC-CRS, to enhance the\ncredibility of CRS's explanations during persuasion. It guides the explanation\ngeneration through our proposed credibility-aware persuasive strategies and\nthen gradually refines explanations via post-hoc self-reflection. Experimental\nresults demonstrate the efficacy of PC-CRS in promoting persuasive and credible\nexplanations. Further analysis reveals the reason behind current methods\nproducing incredible explanations and the potential of credible explanations to\nimprove recommendation accuracy.\n","authors":["Peixin Qin","Chen Huang","Yang Deng","Wenqiang Lei","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14399v2.pdf","comment":"Findings of EMNLP 2024. Our code is available at\n  https://github.com/mumen798/PC-CRS"},{"id":"http://arxiv.org/abs/2407.01449v3","updated":"2024-10-07T07:46:00Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.04808v1","updated":"2024-10-07T07:41:48Z","published":"2024-10-07T07:41:48Z","title":"LPZero: Language Model Zero-cost Proxy Search from Zero","summary":"  In spite of the outstanding performance, Neural Architecture Search (NAS) is\ncriticized for massive computation. Recently, Zero-shot NAS has emerged as a\npromising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce\ncomputational demands. Despite this, existing ZC proxies heavily rely on expert\nknowledge and incur significant trial-and-error costs. Particularly in NLP\ntasks, most existing ZC proxies fail to surpass the performance of the naive\nbaseline. To address these challenges, we introduce a novel framework,\n\\textbf{LPZero}, which is the first to automatically design ZC proxies for\nvarious tasks, achieving higher ranking consistency than human-designed\nproxies. Specifically, we model the ZC proxy as a symbolic equation and\nincorporate a unified proxy search space that encompasses existing ZC proxies,\nwhich are composed of a predefined set of mathematical symbols. To\nheuristically search for the best ZC proxy, LPZero incorporates genetic\nprogramming to find the optimal symbolic composition. We propose a\n\\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates\nunpromising proxies, thereby mitigating the risk of proxy degradation.\nExtensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's\nsuperior ranking ability and performance on downstream tasks compared to\ncurrent approaches.\n","authors":["Peijie Dong","Lujun Li","Xiang Liu","Zhenheng Tang","Xuebo Liu","Qiang Wang","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2410.04808v1.pdf","comment":"8 pages, 7 figures, 10 appendix"},{"id":"http://arxiv.org/abs/2410.04798v1","updated":"2024-10-07T07:21:49Z","published":"2024-10-07T07:21:49Z","title":"DAPE V2: Process Attention Score as Feature Map for Length Extrapolation","summary":"  The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Jing Xiong","Jiankai Sun","Jingyao Li","Minbin Huang","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.04798v1.pdf","comment":"Tech Report. arXiv admin note: text overlap with arXiv:2405.14722"},{"id":"http://arxiv.org/abs/2410.04795v1","updated":"2024-10-07T07:14:37Z","published":"2024-10-07T07:14:37Z","title":"Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models","summary":"  The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs.\n","authors":["Dahyun Kim","Sukyung Lee","Yungi Kim","Attapol Rutherford","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.04795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"},{"id":"http://arxiv.org/abs/2410.04790v1","updated":"2024-10-07T07:02:09Z","published":"2024-10-07T07:02:09Z","title":"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA","summary":"  In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods.\n","authors":["Xinyu Wang","Yanzheng Xiang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2410.04790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07284v4","updated":"2024-10-07T06:54:30Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v4.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2410.04784v1","updated":"2024-10-07T06:49:41Z","published":"2024-10-07T06:49:41Z","title":"Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge","summary":"  Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data.\n","authors":["Jiahuan Li","Yiqing Cao","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04784v1.pdf","comment":"accepted by EMNLP 2024, main conference"},{"id":"http://arxiv.org/abs/2408.01084v2","updated":"2024-10-07T06:11:46Z","published":"2024-08-02T08:03:38Z","title":"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts","summary":"  When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge the gap between\nexternal knowledge and the LLMs' parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLMs\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.\n","authors":["Youna Kim","Hyuhng Joon Kim","Cheonbok Park","Choonghyun Park","Hyunsoo Cho","Junyeob Kim","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01084v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.05356v2","updated":"2024-10-07T05:29:01Z","published":"2024-09-09T06:28:47Z","title":"IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS","summary":"  Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.\n","authors":["Ashwin Sankar","Srija Anand","Praveen Srinivasa Varadhan","Sherry Thomas","Mehak Singal","Shridhar Kumar","Deovrat Mehendale","Aditi Krishana","Giri Raju","Mitesh Khapra"],"pdf_url":"https://arxiv.org/pdf/2409.05356v2.pdf","comment":"Accepted to NeurIPS 2024 Datasets and Benchmarks track"},{"id":"http://arxiv.org/abs/2407.17467v2","updated":"2024-10-07T05:16:25Z","published":"2024-07-24T17:59:02Z","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","summary":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","authors":["Jiawei Gu","Zacc Yang","Chuanghao Ding","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17467v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.04753v1","updated":"2024-10-07T05:14:18Z","published":"2024-10-07T05:14:18Z","title":"ImProver: Agent-Based Automated Proof Optimization","summary":"  Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.\n","authors":["Riyaz Ahuja","Jeremy Avigad","Prasad Tetali","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2410.04753v1.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.04752v1","updated":"2024-10-07T05:07:48Z","published":"2024-10-07T05:07:48Z","title":"Document-level Causal Relation Extraction with Knowledge-guided Binary\n  Question Answering","summary":"  As an essential task in information extraction (IE), Event-Event Causal\nRelation Extraction (ECRE) aims to identify and classify the causal\nrelationships between event mentions in natural language texts. However,\nexisting research on ECRE has highlighted two critical challenges, including\nthe lack of document-level modeling and causal hallucinations. In this paper,\nwe propose a Knowledge-guided binary Question Answering (KnowQA) method with\nevent structures for ECRE, consisting of two stages: Event Structure\nConstruction and Binary Question Answering. We conduct extensive experiments\nunder both zero-shot and fine-tuning settings with large language models (LLMs)\non the MECI and MAVEN-ERE datasets. Experimental results demonstrate the\nusefulness of event structures on document-level ECRE and the effectiveness of\nKnowQA by achieving state-of-the-art on the MECI dataset. We observe not only\nthe effectiveness but also the high generalizability and low inconsistency of\nour method, particularly when with complete event structures after fine-tuning\nthe models.\n","authors":["Zimu Wang","Lei Xia","Wei Wang","Xinya Du"],"pdf_url":"https://arxiv.org/pdf/2410.04752v1.pdf","comment":"Accepted at Findings of EMNLP 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2410.04751v1","updated":"2024-10-07T05:07:01Z","published":"2024-10-07T05:07:01Z","title":"Intriguing Properties of Large Language and Vision Models","summary":"  Recently, large language and vision models (LLVMs) have received significant\nattention and development efforts due to their remarkable generalization\nperformance across a wide range of tasks requiring perception and cognitive\nabilities. A key factor behind their success is their simple architecture,\nwhich consists of a vision encoder, a projector, and a large language model\n(LLM). Despite their achievements in advanced reasoning tasks, their\nperformance on fundamental perception-related tasks (e.g., MMVP) remains\nsurprisingly low. This discrepancy raises the question of how LLVMs truly\nperceive images and exploit the advantages of the vision encoder. To address\nthis, we systematically investigate this question regarding several aspects:\npermutation invariance, robustness, math reasoning, alignment preserving and\nimportance, by evaluating the most common LLVM's families (i.e., LLaVA) across\n10 evaluation benchmarks. Our extensive experiments reveal several intriguing\nproperties of current LLVMs: (1) they internally process the image in a global\nmanner, even when the order of visual patch sequences is randomly permuted; (2)\nthey are sometimes able to solve math problems without fully perceiving\ndetailed numerical information; (3) the cross-modal alignment is overfitted to\ncomplex reasoning tasks, thereby, causing them to lose some of the original\nperceptual capabilities of their vision encoder; (4) the representation space\nin the lower layers (<25%) plays a crucial role in determining performance and\nenhancing visual understanding. Lastly, based on the above observations, we\nsuggest potential future directions for building better LLVMs and constructing\nmore challenging evaluation benchmarks.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Yechan Hwang","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.04751v1.pdf","comment":"Code is available in https://github.com/passing2961/IP-LLVM"},{"id":"http://arxiv.org/abs/2405.14722v3","updated":"2024-10-07T04:35:58Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2304.09797v6","updated":"2024-10-07T04:28:04Z","published":"2023-04-19T16:29:48Z","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models","summary":"  The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted extensive and comprehensive experiments on seven benchmarks. The\nresults show that PHP significantly improves accuracy while remaining highly\nefficient. For instance, with text-davinci-003, we observed a 4.2% improvement\non GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction\nin sample paths with self-consistency. With GPT-4 and PHP, we achieve\nstate-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),\nAQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).\n","authors":["Chuanyang Zheng","Zhengying Liu","Enze Xie","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2304.09797v6.pdf","comment":"Accepted to ICML AI4MATH 2024"},{"id":"http://arxiv.org/abs/2403.19270v2","updated":"2024-10-07T04:21:15Z","published":"2024-03-28T09:56:04Z","title":"sDPO: Don't Use Your Data All at Once","summary":"  As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.\n","authors":["Dahyun Kim","Yungi Kim","Wonho Song","Hyeonwoo Kim","Yunsu Kim","Sanghoon Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2403.19270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13803v3","updated":"2024-10-07T04:18:40Z","published":"2024-05-22T16:30:24Z","title":"\"I Like Sunnie More Than I Expected!\": Exploring User Expectation and\n  Perception of an Anthropomorphic LLM-based Conversational Agent for\n  Well-Being Support","summary":"  The human-computer interaction (HCI) research community has a longstanding\ninterest in exploring the mismatch between users' actual experiences and\nexpectation toward new technologies, for instance, large language models\n(LLMs). In this study, we compared users' (N = 38) initial expectations against\ntheir post-interaction perceptions of two LLM-powered mental well-being\nintervention activity recommendation systems. Both systems have a built-in LLM\nto recommend a personalized well-being intervention activity, but one system\n(Sunnie) has an anthropomorphic conversational interaction design via elements\nsuch as appearance, persona, and natural conversation. Results showed that user\nengagement was high with both systems, and both systems exceeded users'\nexpectations along the utility dimension, highlighting AI's potential to offer\nuseful intervention activity recommendations. In addition, Sunnie further\noutperformed the non-anthropomorphic baseline system in relational warmth.\nThese findings suggest that anthropomorphic conversational interaction design\nmay be particularly effective in fostering warmth in mental health support\ncontexts.\n","authors":["Siyi Wu","Julie Y. A. Cachia","Feixue Han","Bingsheng Yao","Tianyi Xie","Xuan Zhao","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13803v3.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2408.07930v3","updated":"2024-10-07T04:17:18Z","published":"2024-08-15T04:57:55Z","title":"MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and\n  Iterative Sub-SQL Refinement for Text-to-SQL","summary":"  Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider.\n","authors":["Wenxuan Xie","Gaochen Wu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07930v3.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.04739v1","updated":"2024-10-07T04:15:02Z","published":"2024-10-07T04:15:02Z","title":"TableRAG: Million-Token Table Understanding with Language Models","summary":"  Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.\n","authors":["Si-An Chen","Lesly Miculicich","Julian Martin Eisenschlos","Zifeng Wang","Zilong Wang","Yanfei Chen","Yasuhisa Fujii","Hsuan-Tien Lin","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04739v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.04734v1","updated":"2024-10-07T04:00:22Z","published":"2024-10-07T04:00:22Z","title":"TLDR: Token-Level Detective Reward Model for Large Vision Language\n  Models","summary":"  Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. Finally, we show that TLDR models can significantly speed up human\nannotation by 3 times to acquire a broader range of high-quality vision\nlanguage data.\n","authors":["Deqing Fu","Tong Xiao","Rui Wang","Wang Zhu","Pengchuan Zhang","Guan Pang","Robin Jia","Lawrence Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04734v1.pdf","comment":"Work done at Meta"},{"id":"http://arxiv.org/abs/2409.06927v2","updated":"2024-10-07T03:56:35Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v2.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.17169v3","updated":"2024-10-07T03:48:18Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v3.pdf","comment":"Accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.04731v1","updated":"2024-10-07T03:47:34Z","published":"2024-10-07T03:47:34Z","title":"Efficient transformer with reinforced position embedding for language\n  models","summary":"  In this paper, we propose an efficient transformer architecture that uses\nreinforced positional embedding to obtain superior performance with half the\nnumber of encoder decoder layers. We demonstrate that concatenating positional\nencoding with trainable token embeddings, normalizing columns in the token\nembedding matrix, and using the normalized token embedding matrix as the value\nof the attention layer improve the training and validation loss and the\ntraining time in an encoder-decoder Transformer model for a Portuguese-English\ntranslation task with 10 epochs or 12 hours of training across 10 trials. Our\nmethod, with roughly a threefold parameter reduction compared to the baseline\nmodel, yields a mean training loss of 1.21, a mean validation loss of 1.51, and\nan average training time of 1352.27 seconds per epoch, surpassing the baseline\nmodel with the same embedding dimension that employs addition of positional\nencoding and token embeddings, which achieves a mean training loss of 1.96, a\nvalidation loss of 2.18, and an average training time of 4297.79 seconds per\nepoch. Additionally, we evaluated our proposed architecture and the baseline\nacross 14 diverse translation datasets from TensorFlow. The results indicate\nthat our method consistently achieves lower or comparable training and\nvalidation losses, suggesting enhanced learning efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2410.04731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16997v2","updated":"2024-10-07T03:41:57Z","published":"2024-07-24T04:39:24Z","title":"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective","summary":"  This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.\n","authors":["Yujian Liu","Yang Zhang","Tommi Jaakkola","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2407.16997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05621v2","updated":"2024-10-07T03:41:27Z","published":"2023-12-09T17:38:39Z","title":"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching","summary":"  Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.\n","authors":["Zhenting Qi","Xiaoyu Tan","Shaojie Shi","Chao Qu","Yinghui Xu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2312.05621v2.pdf","comment":"Accepted by EMNLP 2023 (Industry Track), Oral Presentation"},{"id":"http://arxiv.org/abs/2410.04727v1","updated":"2024-10-07T03:38:27Z","published":"2024-10-07T03:38:27Z","title":"Forgetting Curve: A Reliable Method for Evaluating Memorization\n  Capability for Long-context Models","summary":"  Numerous recent works target to extend effective context length for language\nmodels and various methods, tasks and benchmarks exist to measure model's\neffective memorization length. However, through thorough investigations, we\nfind limitations for currently existing evaluations on model's memorization\ncapability. We provide an extensive survey for limitations in this work and\npropose a new method called forgetting curve to measure the memorization\ncapability of long-context models. We show that forgetting curve has the\nadvantage of being robust to the tested corpus and the experimental settings,\nof not relying on prompts and can be applied to any model size.\n  We apply our forgetting curve to a large variety of models involving both\ntransformer and RNN/SSM based architectures. Our measurement provides empirical\nevidence for the effectiveness of transformer extension techniques while raises\nquestions for the effective length of RNN/SSM based models. We also examine the\ndifference between our measurement and existing benchmarks as well as popular\nmetrics for various models. Our code and results can be found at\nhttps://github.com/1azybug/ForgettingCurve.\n","authors":["Xinyu Liu","Runsong Zhao","Pengcheng Huang","Chunyang Xiao","Bei Li","Jingang Wang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.04727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17141v3","updated":"2024-10-07T03:19:16Z","published":"2024-03-25T19:28:10Z","title":"MetaAligner: Towards Generalizable Multi-Objective Alignment of Language\n  Models","summary":"  Recent advancements in large language models (LLMs) focus on aligning to\nheterogeneous human expectations and values via multi-objective preference\nalignment. However, existing methods are dependent on the policy model\nparameters, which require high-cost repetition of their alignment algorithms\nfor each new policy model, and they cannot expand to unseen objectives due to\ntheir static alignment objectives. In this work, we propose Meta-Objective\nAligner (MetaAligner), the first policy-agnostic and generalizable method for\nmulti-objective preference alignment. MetaAligner models multi-objective\nalignment into three stages: (1) dynamic objectives reformulation algorithm\nreorganizes traditional alignment datasets to supervise the model on performing\nflexible alignment across different objectives; (2) conditional weak-to-strong\ncorrection paradigm aligns the weak outputs of fixed policy models to approach\nstrong outputs with higher preferences in the corresponding alignment\nobjectives, enabling plug-and-play inferences on any policy models, which\nsignificantly reduces training costs and facilitates alignment on close-source\npolicy models; (3) generalizable inference method flexibly adjusts target\nobjectives by updating their text descriptions in the prompts, facilitating\ngeneralizable alignment to unseen objectives. Experimental results show that\nMetaAligner achieves significant and balanced improvements in multi-objective\nalignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU\ntraining hours compared to previous alignment methods. The model also\neffectively aligns unseen objectives, marking the first step towards\ngeneralizable multi-objective preference alignment.\n","authors":["Kailai Yang","Zhiwei Liu","Qianqian Xie","Jimin Huang","Tianlin Zhang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2403.17141v3.pdf","comment":"Accepted by NeurIPS 2024 main track"},{"id":"http://arxiv.org/abs/2410.04717v1","updated":"2024-10-07T03:15:11Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04715v1","updated":"2024-10-07T03:13:06Z","published":"2024-10-07T03:13:06Z","title":"Rule-based Data Selection for Large Language Models","summary":"  The quality of training data significantly impacts the performance of large\nlanguage models (LLMs). There are increasing studies using LLMs to rate and\nselect data based on several human-crafted metrics (rules). However, these\nconventional rule-based approaches often depend too heavily on human\nheuristics, lack effective metrics for assessing rules, and exhibit limited\nadaptability to new tasks. In our study, we introduce an innovative rule-based\nframework that utilizes the orthogonality of score vectors associated with\nrules as a novel metric for rule evaluations. Our approach includes an\nautomated pipeline that first uses LLMs to generate a diverse set of rules,\nencompassing various rating dimensions to evaluate data quality. Then it rates\na batch of data based on these rules and uses the determinantal point process\n(DPP) from random matrix theory to select the most orthogonal score vectors,\nthereby identifying a set of independent rules. These rules are subsequently\nused to evaluate all data, selecting samples with the highest average scores\nfor downstream tasks such as LLM training. We verify the effectiveness of our\nmethod through two experimental setups: 1) comparisons with ground truth\nratings and 2) benchmarking LLMs trained with the chosen data. Our\ncomprehensive experiments cover a range of scenarios, including general\npre-training and domain-specific fine-tuning in areas such as IMDB, Medical,\nMath, and Code. The outcomes demonstrate that our DPP-based rule rating method\nconsistently outperforms other approaches, including rule-free rating, uniform\nsampling, importance resampling, and QuRating, in terms of both rating\nprecision and model performance.\n","authors":["Xiaomin Li","Mingye Gao","Zhiwei Zhang","Chang Yue","Hong Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12327v3","updated":"2024-10-07T03:08:12Z","published":"2024-07-17T05:53:20Z","title":"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models","summary":"  Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\n\\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.\n","authors":["Ayush Kaushal","Tejas Vaidhya","Arnab Kumar Mondal","Tejas Pandey","Aaryan Bhagat","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2407.12327v3.pdf","comment":"42 pages, 21 figures, and 13 tables"},{"id":"http://arxiv.org/abs/2405.15984v3","updated":"2024-10-07T03:07:58Z","published":"2024-05-24T23:56:36Z","title":"Evaluating and Safeguarding the Adversarial Robustness of\n  Retrieval-Based In-Context Learning","summary":"  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl\n","authors":["Simon Yu","Jie He","Pasquale Minervini","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2405.15984v3.pdf","comment":"COLM 2024, 30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05328v2","updated":"2024-10-07T03:06:34Z","published":"2024-06-08T02:59:52Z","title":"FacLens: Transferable Probe for Foreseeing Non-Factuality in Large\n  Language Models","summary":"  Despite advancements in large language models (LLMs), non-factual responses\nremain prevalent. Unlike extensive studies on post-hoc detection of such\nresponses, this work studies non-factuality prediction (NFP), aiming to predict\nwhether an LLM will generate a non-factual response to a question before the\ngeneration process. Previous efforts on NFP have demonstrated LLMs' awareness\nof their internal knowledge, but they still face challenges in efficiency and\ntransferability. In this work, we propose a lightweight NFP model named\nFactuality Lens (FacLens), which effectively probes hidden representations of\nquestions for the NFP task. Besides, we discover that hidden question\nrepresentations sourced from different LLMs exhibit similar NFP patterns, which\nenables the transferability of FacLens across LLMs to reduce development costs.\nExtensive experiments highlight FacLens's superiority in both effectiveness and\nefficiency.\n","authors":["Yanling Wang","Haoyang Li","Hao Zou","Jing Zhang","Xinlei He","Qi Li","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2406.05328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v2","updated":"2024-10-07T03:01:01Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xiaolei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.20267v4","updated":"2024-10-07T02:53:44Z","published":"2024-05-30T17:19:19Z","title":"Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and\n  Committee Discussions","summary":"  As LLMs continuously evolve, there is an urgent need for a reliable\nevaluation method that delivers trustworthy results promptly. Currently, static\nbenchmarks suffer from inflexibility and unreliability, leading users to prefer\nhuman voting platforms like Chatbot Arena. However, human evaluations require\nsignificant manual effort. To address this, we propose the Auto-Arena, an\ninnovative framework that automates the entire evaluation process using\nLLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM\ncandidates engage in a multi-round peer battle based on individual questions,\naiming at revealing their true performance differences. Finally, a committee of\nLLM judges collaboratively discusses and decides the winner, reducing bias and\nenhancing fairness. During the peer battles, we observe intriguing scenarios\nwhere the LLM candidates display competitive behaviors and even learn from the\nopponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena\nshows a 92.14% correlation with human preferences, surpassing all previous\nexpert-annotated benchmarks without any manual efforts. As a result, Auto-Arena\noffers a promising alternative to current human evaluation platforms for\nevaluating LLMs automatically.\n","authors":["Ruochen Zhao","Wenxuan Zhang","Yew Ken Chia","Weiwen Xu","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2405.20267v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04707v1","updated":"2024-10-07T02:52:30Z","published":"2024-10-07T02:52:30Z","title":"Learning How Hard to Think: Input-Adaptive Allocation of LM Computation","summary":"  Computationally intensive decoding procedures--including search, reranking,\nand self-critique--can improve the quality of language model (LM) outputs in\nproblems spanning code generation, numerical reasoning, and dialog. Existing\nwork typically applies the same decoding procedure for every input to an LM.\nBut not all inputs require the same amount of computation to process. Can we\nallocate decoding computation adaptively, using more resources to answer\nquestions whose answers will be harder to compute? We present an approach that\npredicts the distribution of rewards given an input and computation budget,\nthen allocates additional computation to inputs for which it is predicted to be\nmost useful. We apply this approach in two decoding procedures: first, an\nadaptive best-of-k procedure that dynamically selects the number of samples to\ngenerate as input to a reranker; second, a routing procedure that dynamically\nresponds to a query using a decoding procedure that is expensive but accurate,\nor one that is cheaper but less capable. Across a suite of programming,\nmathematics, and dialog tasks, we show that accurate computation-allocation\nprocedures can be learned, and reduce computation by up to 50% at no cost to\nresponse quality, or improve quality by up to 10% at a fixed computational\nbudget.\n","authors":["Mehul Damani","Idan Shenfeld","Andi Peng","Andreea Bobu","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.04707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04704v1","updated":"2024-10-07T02:48:18Z","published":"2024-10-07T02:48:18Z","title":"Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model","summary":"  Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.\n","authors":["Kai Lia","Masato Akagia","Yongwei Lib","Masashi Unokia"],"pdf_url":"https://arxiv.org/pdf/2410.04704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00943v2","updated":"2024-10-07T02:47:36Z","published":"2024-04-01T06:03:39Z","title":"Evalverse: Unified and Accessible Library for Large Language Model\n  Evaluation","summary":"  This paper introduces Evalverse, a novel library that streamlines the\nevaluation of Large Language Models (LLMs) by unifying disparate evaluation\ntools into a single, user-friendly framework. Evalverse enables individuals\nwith limited knowledge of artificial intelligence to easily request LLM\nevaluations and receive detailed reports, facilitated by an integration with\ncommunication platforms like Slack. Thus, Evalverse serves as a powerful tool\nfor the comprehensive assessment of LLMs, offering both researchers and\npractitioners a centralized and easily accessible evaluation framework.\nFinally, we also provide a demo video for Evalverse, showcasing its\ncapabilities and implementation in a two-minute format.\n","authors":["Jihoo Kim","Wonho Song","Dahyun Kim","Yunsu Kim","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.00943v2.pdf","comment":"Accepted to EMNLP 2024 Demo Track"},{"id":"http://arxiv.org/abs/2410.04699v1","updated":"2024-10-07T02:30:18Z","published":"2024-10-07T02:30:18Z","title":"The LLM Effect: Are Humans Truly Using LLMs, or Are They Being\n  Influenced By Them Instead?","summary":"  Large Language Models (LLMs) have shown capabilities close to human\nperformance in various analytical tasks, leading researchers to use them for\ntime and labor-intensive analyses. However, their capability to handle highly\nspecialized and open-ended tasks in domains like policy studies remains in\nquestion. This paper investigates the efficiency and accuracy of LLMs in\nspecialized tasks through a structured user study focusing on Human-LLM\npartnership. The study, conducted in two stages-Topic Discovery and Topic\nAssignment-integrates LLMs with expert annotators to observe the impact of LLM\nsuggestions on what is usually human-only analysis. Results indicate that\nLLM-generated topic lists have significant overlap with human generated topic\nlists, with minor hiccups in missing document-specific topics. However, LLM\nsuggestions may significantly improve task completion speed, but at the same\ntime introduce anchoring bias, potentially affecting the depth and nuance of\nthe analysis, raising a critical question about the trade-off between increased\nefficiency and the risk of biased analysis.\n","authors":["Alexander S. Choi","Syeda Sabrina Akter","JP Singh","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.04699v1.pdf","comment":"Accepted to EMNLP Main 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2410.04698v1","updated":"2024-10-07T02:30:07Z","published":"2024-10-07T02:30:07Z","title":"MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning\n  in LLMs","summary":"  Recent large language models (LLMs) have demonstrated versatile capabilities\nin long-context scenarios. Although some recent benchmarks have been developed\nto evaluate the long-context capabilities of LLMs, there is a lack of\nbenchmarks evaluating the mathematical reasoning abilities of LLMs over long\ncontexts, which is crucial for LLMs' application in real-world scenarios. In\nthis paper, we introduce MathHay, an automated benchmark designed to assess the\nlong-context mathematical reasoning capabilities of LLMs. Unlike previous\nbenchmarks like Needle in a Haystack, which focus primarily on information\nretrieval within long texts, MathHay demands models with both\ninformation-seeking and complex mathematical reasoning abilities. We conduct\nextensive experiments on MathHay to assess the long-context mathematical\nreasoning abilities of eight top-performing LLMs. Even the best-performing\nmodel, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over\nlong contexts, achieving only 51.26% accuracy at 128K tokens. This highlights\nthe significant room for improvement on the MathHay benchmark.\n","authors":["Lei Wang","Shan Dong","Yuhui Xu","Hanze Dong","Yalu Wang","Amrita Saha","Ee-Peng Lim","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.04698v1.pdf","comment":"Work-in-Progress"},{"id":"http://arxiv.org/abs/2401.15884v3","updated":"2024-10-07T02:19:21Z","published":"2024-01-29T04:36:39Z","title":"Corrective Retrieval Augmented Generation","summary":"  Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.\n","authors":["Shi-Qi Yan","Jia-Chen Gu","Yun Zhu","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2401.15884v3.pdf","comment":"Update results, add more analysis, and fix typos"},{"id":"http://arxiv.org/abs/2410.04691v1","updated":"2024-10-07T02:12:22Z","published":"2024-10-07T02:12:22Z","title":"Deeper Insights Without Updates: The Power of In-Context Learning Over\n  Fine-Tuning","summary":"  Fine-tuning and in-context learning (ICL) are two prevalent methods in\nimbuing large language models with task-specific knowledge. It is commonly\nbelieved that fine-tuning can surpass ICL given sufficient training samples as\nit allows the model to adjust its internal parameters based on the data.\nHowever, this paper presents a counterintuitive finding: For tasks with\nimplicit patterns, ICL captures these patterns significantly better than\nfine-tuning. We developed several datasets featuring implicit patterns, such as\nsequences determining answers through parity or identifying reducible terms in\ncalculations. We then evaluated the models' understanding of these patterns\nunder both fine-tuning and ICL across models ranging from 0.5B to 7B\nparameters. The results indicate that models employing ICL can quickly grasp\ndeep patterns and significantly improve accuracy. In contrast, fine-tuning,\ndespite utilizing thousands of times more training samples than ICL, achieved\nonly limited improvements. We also proposed circuit shift theory from a\nmechanistic interpretability's view to explain why ICL wins.\n","authors":["Qingyu Yin","Xuzheng He","Luoao Deng","Chak Tou Leong","Fan Wang","Yanzhao Yan","Xiaoyu Shen","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04691v1.pdf","comment":"EMNLP'24 Findings"},{"id":"http://arxiv.org/abs/2406.08464v2","updated":"2024-10-07T01:45:38Z","published":"2024-06-12T17:52:30Z","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing","summary":"  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n","authors":["Zhangchen Xu","Fengqing Jiang","Luyao Niu","Yuntian Deng","Radha Poovendran","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2406.08464v2.pdf","comment":"Link: https://magpie-align.github.io/"},{"id":"http://arxiv.org/abs/2405.16406v3","updated":"2024-10-07T01:27:59Z","published":"2024-05-26T02:15:49Z","title":"SpinQuant: LLM quantization with learned rotations","summary":"  Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\n","authors":["Zechun Liu","Changsheng Zhao","Igor Fedorov","Bilge Soran","Dhruv Choudhary","Raghuraman Krishnamoorthi","Vikas Chandra","Yuandong Tian","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2405.16406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12832v3","updated":"2024-10-07T01:26:23Z","published":"2024-09-19T15:07:35Z","title":"FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists","summary":"  Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.\n","authors":["Tenghao Huang","Donghee Lee","John Sweeney","Jiatong Shi","Emily Steliotes","Matthew Lange","Jonathan May","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12832v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04663v1","updated":"2024-10-07T00:22:07Z","published":"2024-10-07T00:22:07Z","title":"Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates","summary":"  This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.\n","authors":["Chaithanya Bandi","Hari Bandi","Abir Harrasse"],"pdf_url":"https://arxiv.org/pdf/2410.04663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08760v3","updated":"2024-10-07T00:16:54Z","published":"2024-04-12T18:36:20Z","title":"The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models","summary":"  We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}\n","authors":["Siyang Liu","Trish Maturi","Bowen Yi","Siqi Shen","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2404.08760v3.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2410.04657v1","updated":"2024-10-07T00:09:50Z","published":"2024-10-07T00:09:50Z","title":"Contrastive Learning to Improve Retrieval for Real-world Fact Checking","summary":"  Recent work on fact-checking addresses a realistic setting where models\nincorporate evidence retrieved from the web to decide the veracity of claims. A\nbottleneck in this pipeline is in retrieving relevant evidence: traditional\nmethods may surface documents directly related to a claim, but fact-checking\ncomplex claims requires more inferences. For instance, a document about how a\nvaccine was developed is relevant to addressing claims about what it might\ncontain, even if it does not address them directly. We present Contrastive\nFact-Checking Reranker (CFR), an improved retriever for this setting. By\nleveraging the AVeriTeC dataset, which annotates subquestions for claims with\nhuman written answers from evidence documents, we fine-tune Contriever with a\ncontrastive objective based on multiple training signals, including\ndistillation from GPT-4, evaluating subquestion answers, and gold labels in the\ndataset. We evaluate our model on both retrieval and end-to-end veracity\njudgments about claims. On the AVeriTeC dataset, we find a 6\\% improvement in\nveracity classification accuracy. We also show our gains can be transferred to\nFEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to\nmake inferences.\n","authors":["Aniruddh Sriram","Fangyuan Xu","Eunsol Choi","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2410.04657v1.pdf","comment":"EMNLP 2024 FEVER Workshop"}],"Sound":[{"id":"http://arxiv.org/abs/2410.05167v1","updated":"2024-10-07T16:24:18Z","published":"2024-10-07T16:24:18Z","title":"Presto! Distilling Steps and Layers for Accelerating Music Generation","summary":"  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/.\n","authors":["Zachary Novack","Ge Zhu","Jonah Casebeer","Julian McAuley","Taylor Berg-Kirkpatrick","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2410.05167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05151v1","updated":"2024-10-07T16:04:49Z","published":"2024-10-07T16:04:49Z","title":"Editing Music with Melody and Text: Using ControlNet for Diffusion\n  Transformer","summary":"  Despite the significant progress in controllable music generation and\nediting, challenges remain in the quality and length of generated music due to\nthe use of Mel-spectrogram representations and UNet-based model structures. To\naddress these limitations, we propose a novel approach using a Diffusion\nTransformer (DiT) augmented with an additional control branch using ControlNet.\nThis allows for long-form and variable-length music generation and editing\ncontrolled by text and melody prompts. For more precise and fine-grained melody\ncontrol, we introduce a novel top-$k$ constant-Q Transform representation as\nthe melody prompt, reducing ambiguity compared to previous representations\n(e.g., chroma), particularly for music with multiple tracks or a wide range of\npitch values. To effectively balance the control signals from text and melody\nprompts, we adopt a curriculum learning strategy that progressively masks the\nmelody prompt, resulting in a more stable training process. Experiments have\nbeen performed on text-to-music generation and music-style transfer tasks using\nopen-source instrumental recording data. The results demonstrate that by\nextending StableAudio, a pre-trained text-controlled DiT model, our approach\nenables superior melody-controlled editing while retaining good text-to-music\ngeneration performance. These results outperform a strong MusicGen baseline in\nterms of both text-based generation and melody preservation for editing. Audio\nexamples can be found at https://stable-audio-control.github.io/web/.\n","authors":["Siyuan Hou","Shansong Liu","Ruibin Yuan","Wei Xue","Ying Shan","Mangsuo Zhao","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05151v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.12132v2","updated":"2024-10-07T15:28:18Z","published":"2024-04-18T12:33:57Z","title":"Non-Invasive Suicide Risk Prediction Through Speech Analysis","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05101v1","updated":"2024-10-07T14:56:07Z","published":"2024-10-07T14:56:07Z","title":"CR-CTC: Consistency regularization on CTC for improved speech\n  recognition","summary":"  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance compared to transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized\nCTC (CR-CTC), which enforces consistency between two CTC distributions obtained\nfrom different augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC, which achieves\nperformance comparable to, or even slightly better than, that of transducer and\nCTC/AED.\n","authors":["Zengwei Yao","Wei Kang","Xiaoyu Yang","Fangjun Kuang","Liyong Guo","Han Zhu","Zengrui Jin","Zhaoqing Li","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2410.05101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18213v2","updated":"2024-10-07T13:52:00Z","published":"2024-05-28T14:17:41Z","title":"NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields","summary":"  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n","authors":["Amandine Brunetto","Sascha Hornauer","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2405.18213v2.pdf","comment":"Project Page: https://amandinebtto.github.io/NeRAF"},{"id":"http://arxiv.org/abs/2410.05037v1","updated":"2024-10-07T13:49:45Z","published":"2024-10-07T13:49:45Z","title":"Improving Speaker Representations Using Contrastive Losses on\n  Multi-scale Features","summary":"  Speaker verification systems have seen significant advancements with the\nintroduction of Multi-scale Feature Aggregation (MFA) architectures, such as\nMFA-Conformer and ECAPA-TDNN. These models leverage information from various\nnetwork depths by concatenating intermediate feature maps before the pooling\nand projection layers, demonstrating that even shallower feature maps encode\nvaluable speaker-specific information. Building upon this foundation, we\npropose a Multi-scale Feature Contrastive (MFCon) loss that directly enhances\nthe quality of these intermediate representations. Our MFCon loss applies\ncontrastive learning to all feature maps within the network, encouraging the\nmodel to learn more discriminative representations at the intermediate stage\nitself. By enforcing better feature map learning, we show that the resulting\nspeaker embeddings exhibit increased discriminative power. Our method achieves\na 9.05% improvement in equal error rate (EER) compared to the standard\nMFA-Conformer on the VoxCeleb-1O test set.\n","authors":["Satvik Dixit","Massa Baali","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2410.05037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05019v1","updated":"2024-10-07T13:19:10Z","published":"2024-10-07T13:19:10Z","title":"RelUNet: Relative Channel Fusion U-Net for Multichannel Speech\n  Enhancement","summary":"  Neural multi-channel speech enhancement models, in particular those based on\nthe U-Net architecture, demonstrate promising performance and generalization\npotential. These models typically encode input channels independently, and\nintegrate the channels during later stages of the network. In this paper, we\npropose a novel modification of these models by incorporating relative\ninformation from the outset, where each channel is processed in conjunction\nwith a reference channel through stacking. This input strategy exploits\ncomparative differences to adaptively fuse information between channels,\nthereby capturing crucial spatial information and enhancing the overall\nperformance. The experiments conducted on the CHiME-3 dataset demonstrate\nimprovements in speech enhancement metrics across various architectures.\n","authors":["Ibrahim Aldarmaki","Thamar Solorio","Bhiksha Raj","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2410.05019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04990v1","updated":"2024-10-07T12:45:20Z","published":"2024-10-07T12:45:20Z","title":"Stage-Wise and Prior-Aware Neural Speech Phase Prediction","summary":"  This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase\nPrediction (SP-NSPP) model, which predicts the phase spectrum from input\namplitude spectrum by two-stage neural networks. In the initial\nprior-construction stage, we preliminarily predict a rough prior phase spectrum\nfrom the amplitude spectrum. The subsequent refinement stage transforms the\namplitude spectrum into a refined high-quality phase spectrum conditioned on\nthe prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone\nand adopt adversarial training by innovatively introducing a phase spectrum\ndiscriminator (PSD). To further improve the continuity of the refined phase, we\nalso incorporate a time-frequency integrated difference (TFID) loss in the\nrefinement stage. Experimental results confirm that, compared to neural\nnetwork-based no-prior phase prediction methods, the proposed SP-NSPP achieves\nhigher phase prediction accuracy, thanks to introducing the coarse phase priors\nand diverse training criteria. Compared to iterative phase estimation\nalgorithms, our proposed SP-NSPP does not require multiple rounds of staged\niterations, resulting in higher generation efficiency.\n","authors":["Fei Liu","Yang Ai","Hui-Peng Du","Ye-Xin Lu","Rui-Chen Zheng","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2410.04990v1.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.04951v1","updated":"2024-10-07T11:47:59Z","published":"2024-10-07T11:47:59Z","title":"A decade of DCASE: Achievements, practices, evaluations and future\n  challenges","summary":"  This paper introduces briefly the history and growth of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) challenge, workshop,\nresearch area and research community. Created in 2013 as a data evaluation\nchallenge, DCASE has become a major research topic in the Audio and Acoustic\nSignal Processing area. Its success comes from a combination of factors: the\nchallenge offers a large variety of tasks that are renewed each year; and the\nworkshop offers a channel for dissemination of related work, engaging a young\nand dynamic community. At the same time, DCASE faces its own challenges,\ngrowing and expanding to different areas. One of the core principles of DCASE\nis open science and reproducibility: publicly available datasets, baseline\nsystems, technical reports and workshop publications. While the DCASE challenge\nand workshop are independent of IEEE SPS, the challenge receives annual\nendorsement from the AASP TC, and the DCASE community contributes significantly\nto the ICASSP flagship conference and the success of SPS in many of its\nactivities.\n","authors":["Annamaria Mesaros","Romain Serizel","Toni Heittola","Tuomas Virtanen","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2410.04951v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.11439v2","updated":"2024-10-07T11:44:38Z","published":"2024-09-16T09:19:19Z","title":"Machine listening in a neonatal intensive care unit","summary":"  Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.\n","authors":["Modan Tailleur","Vincent Lostanlen","Jean-Philippe Rivière","Pierre Aumond"],"pdf_url":"https://arxiv.org/pdf/2409.11439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04906v1","updated":"2024-10-07T10:48:08Z","published":"2024-10-07T10:48:08Z","title":"Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation","summary":"  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n","authors":["Ivan Rinaldi","Nicola Fanelli","Giovanna Castellano","Gennaro Vessio"],"pdf_url":"https://arxiv.org/pdf/2410.04906v1.pdf","comment":"Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00016v3","updated":"2024-10-07T07:54:37Z","published":"2024-07-28T15:45:08Z","title":"Towards a Universal Method for Meaningful Signal Detection","summary":"  It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.\n","authors":["Louis Mahon"],"pdf_url":"https://arxiv.org/pdf/2408.00016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.10561v4","updated":"2024-10-07T07:48:10Z","published":"2021-09-22T07:44:01Z","title":"A Few-Shot Learning Approach for Sound Source Distance Estimation Using\n  Relation Networks","summary":"  In this paper, we study the performance of few-shot learning, specifically\nmeta learning empowered few-shot relation networks, over supervised deep\nlearning and conventional machine learning approaches in the problem of Sound\nSource Distance Estimation (SSDE). In previous research on deep supervised\nSSDE, low accuracies have often resulted from the mismatch between the training\ndata (from known environments) and the test data (from unknown environments).\nBy performing comparative experiments on a sufficient amount of data, we show\nthat the few-shot relation network outperforms other competitors including\neXtreme Gradient Boosting (XGBoost), Support Vector Machine (SVM),\nConvolutional Neural Network (CNN), and MultiLayer Perceptron (MLP). Hence it\nis possible to calibrate a microphone-equipped system, with a few labeled\nsamples of audio recorded in a particular unknown environment to adjust and\ngeneralize our classifier to the possible input data and gain higher\naccuracies.\n","authors":["Amirreza Sobhdel","Roozbeh Razavi-Far","Vasile Palade"],"pdf_url":"https://arxiv.org/pdf/2109.10561v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04797v1","updated":"2024-10-07T07:16:29Z","published":"2024-10-07T07:16:29Z","title":"Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis","summary":"  Voice disorders negatively impact the quality of daily life in various ways.\nHowever, accurately recognizing the category of pathological features from raw\naudio remains a considerable challenge due to the limited dataset. A promising\nmethod to handle this issue is extracting multi-level pathological information\nfrom speech in a comprehensive manner by fusing features in the latent space.\nIn this paper, a novel framework is designed to explore the way of high-quality\nfeature fusion for effective and generalized detection performance.\nSpecifically, the proposed model follows a two-stage training paradigm: (1)\nECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various\ndomains are employed to learn the universal pathological information from raw\naudio; (2) An attentive fusion module is dedicatedly designed to establish the\ninteraction between pathological features projected by EcapTdnn and Wav2vec 2.0\nrespectively and guide the multi-layer fusion, the entire model is jointly\nfine-tuned from pre-trained features by the automatic voice pathology detection\ntask. Finally, comprehensive experiments on the FEMH and SVD datasets\ndemonstrate that the proposed framework outperforms the competitive baselines,\nand achieves the accuracy of 90.51% and 87.68%.\n","authors":["Lipeng Shen","Yifan Xiong","Dongyue Guo","Wei Mo","Lingyu Yu","Hui Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04785v1","updated":"2024-10-07T06:50:19Z","published":"2024-10-07T06:50:19Z","title":"Towards Ultra-Low-Power Neuromorphic Speech Enhancement with\n  Spiking-FullSubNet","summary":"  Speech enhancement is critical for improving speech intelligibility and\nquality in various audio devices. In recent years, deep learning-based methods\nhave significantly improved speech enhancement performance, but they often come\nwith a high computational cost, which is prohibitive for a large number of edge\ndevices, such as headsets and hearing aids. This work proposes an\nultra-low-power speech enhancement system based on the brain-inspired spiking\nneural network (SNN) called Spiking-FullSubNet. Spiking-FullSubNet follows a\nfull-band and sub-band fusioned approach to effectively capture both global and\nlocal spectral information. To enhance the efficiency of computationally\nexpensive sub-band modeling, we introduce a frequency partitioning method\ninspired by the sensitivity profile of the human peripheral auditory system.\nFurthermore, we introduce a novel spiking neuron model that can dynamically\ncontrol the input information integration and forgetting, enhancing the\nmulti-scale temporal processing capability of SNN, which is critical for speech\ndenoising. Experiments conducted on the recent Intel Neuromorphic Deep Noise\nSuppression (N-DNS) Challenge dataset show that the Spiking-FullSubNet\nsurpasses state-of-the-art methods by large margins in terms of both speech\nquality and energy efficiency metrics. Notably, our system won the championship\nof the Intel N-DNS Challenge (Algorithmic Track), opening up a myriad of\nopportunities for ultra-low-power speech enhancement at the edge. Our source\ncode and model checkpoints are publicly available at\nhttps://github.com/haoxiangsnr/spiking-fullsubnet.\n","authors":["Xiang Hao","Chenxiang Ma","Qu Yang","Jibin Wu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2410.04785v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.05356v2","updated":"2024-10-07T05:29:01Z","published":"2024-09-09T06:28:47Z","title":"IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS","summary":"  Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.\n","authors":["Ashwin Sankar","Srija Anand","Praveen Srinivasa Varadhan","Sherry Thomas","Mehak Singal","Shridhar Kumar","Deovrat Mehendale","Aditi Krishana","Giri Raju","Mitesh Khapra"],"pdf_url":"https://arxiv.org/pdf/2409.05356v2.pdf","comment":"Accepted to NeurIPS 2024 Datasets and Benchmarks track"},{"id":"http://arxiv.org/abs/2410.04704v1","updated":"2024-10-07T02:48:18Z","published":"2024-10-07T02:48:18Z","title":"Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model","summary":"  Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.\n","authors":["Kai Lia","Masato Akagia","Yongwei Lib","Masashi Unokia"],"pdf_url":"https://arxiv.org/pdf/2410.04704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04702v1","updated":"2024-10-07T02:38:58Z","published":"2024-10-07T02:38:58Z","title":"Demo of Zero-Shot Guitar Amplifier Modelling: Enhancing Modeling with\n  Hyper Neural Networks","summary":"  Electric guitar tone modeling typically focuses on the non-linear\ntransformation from clean to amplifier-rendered audio. Traditional methods rely\non one-to-one mappings, incorporating device parameters into neural models to\nreplicate specific amplifiers. However, these methods are limited by the need\nfor specific training data. In this paper, we adapt a model based on the\nprevious work, which leverages a tone embedding encoder and a feature wise\nlinear modulation (FiLM) condition method. In this work, we altered\nconditioning method using a hypernetwork-based gated convolutional network\n(GCN) to generate audio that blends clean input with the tone characteristics\nof reference audio. By extending the training data to cover a wider variety of\namplifier tones, our model is able to capture a broader range of tones.\nAdditionally, we developed a real-time plugin to demonstrate the system's\npractical application, allowing users to experience its performance\ninteractively. Our results indicate that the proposed system achieves superior\ntone modeling versatility compared to traditional methods.\n","authors":["Yu-Hua Chen","Yuan-Chiao Cheng","Yen-Tung Yeh","Jui-Te Wu","Yu-Hsiang Ho","Jyh-Shing Roger Jang","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04702v1.pdf","comment":"demo of the ISMIR paper"},{"id":"http://arxiv.org/abs/2403.05010v3","updated":"2024-10-07T02:08:05Z","published":"2024-03-08T03:16:47Z","title":"RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction","summary":"  Recent advancements in generative modeling have significantly enhanced the\nreconstruction of audio waveforms from various representations. While diffusion\nmodels are adept at this task, they are hindered by latency issues due to their\noperation at the individual sample point level and the need for numerous\nsampling steps. In this study, we introduce RFWave, a cutting-edge multi-band\nRectified Flow approach designed to reconstruct high-fidelity audio waveforms\nfrom Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates\ncomplex spectrograms and operates at the frame level, processing all subbands\nsimultaneously to boost efficiency. Leveraging Rectified Flow, which targets a\nstraight transport trajectory, RFWave achieves reconstruction with just 10\nsampling steps. Our empirical evaluations show that RFWave not only provides\noutstanding reconstruction quality but also offers vastly superior\ncomputational efficiency, enabling audio generation at speeds up to 160 times\nfaster than real-time on a GPU. An online demonstration is available at:\nhttps://rfwave-demo.github.io/rfwave/.\n","authors":["Peng Liu","Dongyang Dai","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05010v3.pdf","comment":null}],"Speech Processing":[{"id":"http://arxiv.org/abs/2410.05167v1","updated":"2024-10-07T16:24:18Z","published":"2024-10-07T16:24:18Z","title":"Presto! Distilling Steps and Layers for Accelerating Music Generation","summary":"  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/.\n","authors":["Zachary Novack","Ge Zhu","Jonah Casebeer","Julian McAuley","Taylor Berg-Kirkpatrick","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2410.05167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05151v1","updated":"2024-10-07T16:04:49Z","published":"2024-10-07T16:04:49Z","title":"Editing Music with Melody and Text: Using ControlNet for Diffusion\n  Transformer","summary":"  Despite the significant progress in controllable music generation and\nediting, challenges remain in the quality and length of generated music due to\nthe use of Mel-spectrogram representations and UNet-based model structures. To\naddress these limitations, we propose a novel approach using a Diffusion\nTransformer (DiT) augmented with an additional control branch using ControlNet.\nThis allows for long-form and variable-length music generation and editing\ncontrolled by text and melody prompts. For more precise and fine-grained melody\ncontrol, we introduce a novel top-$k$ constant-Q Transform representation as\nthe melody prompt, reducing ambiguity compared to previous representations\n(e.g., chroma), particularly for music with multiple tracks or a wide range of\npitch values. To effectively balance the control signals from text and melody\nprompts, we adopt a curriculum learning strategy that progressively masks the\nmelody prompt, resulting in a more stable training process. Experiments have\nbeen performed on text-to-music generation and music-style transfer tasks using\nopen-source instrumental recording data. The results demonstrate that by\nextending StableAudio, a pre-trained text-controlled DiT model, our approach\nenables superior melody-controlled editing while retaining good text-to-music\ngeneration performance. These results outperform a strong MusicGen baseline in\nterms of both text-based generation and melody preservation for editing. Audio\nexamples can be found at https://stable-audio-control.github.io/web/.\n","authors":["Siyuan Hou","Shansong Liu","Ruibin Yuan","Wei Xue","Ying Shan","Mangsuo Zhao","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05151v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.05146v1","updated":"2024-10-07T15:58:03Z","published":"2024-10-07T15:58:03Z","title":"CTC-GMM: CTC guided modality matching for fast and accurate streaming\n  speech translation","summary":"  Models for streaming speech translation (ST) can achieve high accuracy and\nlow latency if they're developed with vast amounts of paired audio in the\nsource language and written text in the target language. Yet, these text labels\nfor the target language are often pseudo labels due to the prohibitive cost of\nmanual ST data labeling. In this paper, we introduce a methodology named\nConnectionist Temporal Classification guided modality matching (CTC-GMM) that\nenhances the streaming ST model by leveraging extensive machine translation\n(MT) text data. This technique employs CTC to compress the speech sequence into\na compact embedding sequence that matches the corresponding text sequence,\nallowing us to utilize matched {source-target} language text pairs from the MT\ncorpora to refine the streaming ST model further. Our evaluations with FLEURS\nand CoVoST2 show that the CTC-GMM approach can increase translation accuracy\nrelatively by 13.9% and 6.4% respectively, while also boosting decoding speed\nby 59.7% on GPU.\n","authors":["Rui Zhao","Jinyu Li","Ruchao Fan","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2410.05146v1.pdf","comment":"Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)"},{"id":"http://arxiv.org/abs/2404.12132v2","updated":"2024-10-07T15:28:18Z","published":"2024-04-18T12:33:57Z","title":"Non-Invasive Suicide Risk Prediction Through Speech Analysis","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05101v1","updated":"2024-10-07T14:56:07Z","published":"2024-10-07T14:56:07Z","title":"CR-CTC: Consistency regularization on CTC for improved speech\n  recognition","summary":"  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance compared to transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized\nCTC (CR-CTC), which enforces consistency between two CTC distributions obtained\nfrom different augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC, which achieves\nperformance comparable to, or even slightly better than, that of transducer and\nCTC/AED.\n","authors":["Zengwei Yao","Wei Kang","Xiaoyu Yang","Fangjun Kuang","Liyong Guo","Han Zhu","Zengrui Jin","Zhaoqing Li","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2410.05101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13895v2","updated":"2024-10-07T14:07:47Z","published":"2024-07-18T20:48:33Z","title":"Improving Robustness and Clinical Applicability of Automatic Respiratory\n  Sound Classification Using Deep Learning-Based Audio Enhancement: Algorithm\n  Development and Validation Study","summary":"  Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions poses challenges for clinical deployment.\nAdditionally, predicting signals with only background noise could undermine\nuser trust in the system. This paper aims to investigate the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement\npreprocessing step into automatic respiratory sound classification systems to\nimprove robustness and clinical applicability. Multiple experiments were\nconducted using different audio enhancement model structures and classification\nmodels. The classification performance was compared to the baseline method of\nnoise injection data augmentation. Experiments were performed on two datasets:\nthe ICBHI respiratory sound dataset, which includes 5.5 hours of recordings,\nand the Formosa Archive of Breath Sounds (FABS) dataset, comprising 14.6 hours\nof recordings. Additionally, a physician validation study was conducted by 7\nsenior physicians to assess the clinical utility of the system.The integration\nof the audio enhancement pipeline resulted in a 21.88% increase in the ICBHI\nclassification score on the ICBHI dataset and a 4.10% improvement on the FABS\ndataset in multi-class noisy scenarios. Quantitative analysis from the\nphysician validation study revealed improvements in efficiency, diagnostic\nconfidence, and trust during model-assisted diagnosis, with workflows\nintegrating enhanced audio leading to an 11.61% increase in diagnostic\nsensitivity and facilitating high-confidence diagnoses. Incorporating an audio\nenhancement algorithm significantly enhances the robustness and clinical\nutility of automatic respiratory sound classification systems, improving\nperformance in noisy environments and fostering greater trust among medical\nprofessionals.\n","authors":["Jing-Tong Tzeng","Jeng-Lin Li","Huan-Yu Chen","Chun-Hsiang Huang","Chi-Hsin Chen","Cheng-Yi Fan","Edward Pei-Chuan Huang","Chi-Chun Lee"],"pdf_url":"https://arxiv.org/pdf/2407.13895v2.pdf","comment":"Demo website: https://rogertzeng.github.io/ReSC-AE/"},{"id":"http://arxiv.org/abs/2405.18213v2","updated":"2024-10-07T13:52:00Z","published":"2024-05-28T14:17:41Z","title":"NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields","summary":"  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n","authors":["Amandine Brunetto","Sascha Hornauer","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2405.18213v2.pdf","comment":"Project Page: https://amandinebtto.github.io/NeRAF"},{"id":"http://arxiv.org/abs/2410.05037v1","updated":"2024-10-07T13:49:45Z","published":"2024-10-07T13:49:45Z","title":"Improving Speaker Representations Using Contrastive Losses on\n  Multi-scale Features","summary":"  Speaker verification systems have seen significant advancements with the\nintroduction of Multi-scale Feature Aggregation (MFA) architectures, such as\nMFA-Conformer and ECAPA-TDNN. These models leverage information from various\nnetwork depths by concatenating intermediate feature maps before the pooling\nand projection layers, demonstrating that even shallower feature maps encode\nvaluable speaker-specific information. Building upon this foundation, we\npropose a Multi-scale Feature Contrastive (MFCon) loss that directly enhances\nthe quality of these intermediate representations. Our MFCon loss applies\ncontrastive learning to all feature maps within the network, encouraging the\nmodel to learn more discriminative representations at the intermediate stage\nitself. By enforcing better feature map learning, we show that the resulting\nspeaker embeddings exhibit increased discriminative power. Our method achieves\na 9.05% improvement in equal error rate (EER) compared to the standard\nMFA-Conformer on the VoxCeleb-1O test set.\n","authors":["Satvik Dixit","Massa Baali","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2410.05037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05019v1","updated":"2024-10-07T13:19:10Z","published":"2024-10-07T13:19:10Z","title":"RelUNet: Relative Channel Fusion U-Net for Multichannel Speech\n  Enhancement","summary":"  Neural multi-channel speech enhancement models, in particular those based on\nthe U-Net architecture, demonstrate promising performance and generalization\npotential. These models typically encode input channels independently, and\nintegrate the channels during later stages of the network. In this paper, we\npropose a novel modification of these models by incorporating relative\ninformation from the outset, where each channel is processed in conjunction\nwith a reference channel through stacking. This input strategy exploits\ncomparative differences to adaptively fuse information between channels,\nthereby capturing crucial spatial information and enhancing the overall\nperformance. The experiments conducted on the CHiME-3 dataset demonstrate\nimprovements in speech enhancement metrics across various architectures.\n","authors":["Ibrahim Aldarmaki","Thamar Solorio","Bhiksha Raj","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2410.05019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04990v1","updated":"2024-10-07T12:45:20Z","published":"2024-10-07T12:45:20Z","title":"Stage-Wise and Prior-Aware Neural Speech Phase Prediction","summary":"  This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase\nPrediction (SP-NSPP) model, which predicts the phase spectrum from input\namplitude spectrum by two-stage neural networks. In the initial\nprior-construction stage, we preliminarily predict a rough prior phase spectrum\nfrom the amplitude spectrum. The subsequent refinement stage transforms the\namplitude spectrum into a refined high-quality phase spectrum conditioned on\nthe prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone\nand adopt adversarial training by innovatively introducing a phase spectrum\ndiscriminator (PSD). To further improve the continuity of the refined phase, we\nalso incorporate a time-frequency integrated difference (TFID) loss in the\nrefinement stage. Experimental results confirm that, compared to neural\nnetwork-based no-prior phase prediction methods, the proposed SP-NSPP achieves\nhigher phase prediction accuracy, thanks to introducing the coarse phase priors\nand diverse training criteria. Compared to iterative phase estimation\nalgorithms, our proposed SP-NSPP does not require multiple rounds of staged\niterations, resulting in higher generation efficiency.\n","authors":["Fei Liu","Yang Ai","Hui-Peng Du","Ye-Xin Lu","Rui-Chen Zheng","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2410.04990v1.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.04951v1","updated":"2024-10-07T11:47:59Z","published":"2024-10-07T11:47:59Z","title":"A decade of DCASE: Achievements, practices, evaluations and future\n  challenges","summary":"  This paper introduces briefly the history and growth of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) challenge, workshop,\nresearch area and research community. Created in 2013 as a data evaluation\nchallenge, DCASE has become a major research topic in the Audio and Acoustic\nSignal Processing area. Its success comes from a combination of factors: the\nchallenge offers a large variety of tasks that are renewed each year; and the\nworkshop offers a channel for dissemination of related work, engaging a young\nand dynamic community. At the same time, DCASE faces its own challenges,\ngrowing and expanding to different areas. One of the core principles of DCASE\nis open science and reproducibility: publicly available datasets, baseline\nsystems, technical reports and workshop publications. While the DCASE challenge\nand workshop are independent of IEEE SPS, the challenge receives annual\nendorsement from the AASP TC, and the DCASE community contributes significantly\nto the ICASSP flagship conference and the success of SPS in many of its\nactivities.\n","authors":["Annamaria Mesaros","Romain Serizel","Toni Heittola","Tuomas Virtanen","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2410.04951v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.11439v2","updated":"2024-10-07T11:44:38Z","published":"2024-09-16T09:19:19Z","title":"Machine listening in a neonatal intensive care unit","summary":"  Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.\n","authors":["Modan Tailleur","Vincent Lostanlen","Jean-Philippe Rivière","Pierre Aumond"],"pdf_url":"https://arxiv.org/pdf/2409.11439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04906v1","updated":"2024-10-07T10:48:08Z","published":"2024-10-07T10:48:08Z","title":"Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation","summary":"  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n","authors":["Ivan Rinaldi","Nicola Fanelli","Giovanna Castellano","Gennaro Vessio"],"pdf_url":"https://arxiv.org/pdf/2410.04906v1.pdf","comment":"Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00016v3","updated":"2024-10-07T07:54:37Z","published":"2024-07-28T15:45:08Z","title":"Towards a Universal Method for Meaningful Signal Detection","summary":"  It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.\n","authors":["Louis Mahon"],"pdf_url":"https://arxiv.org/pdf/2408.00016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.10561v4","updated":"2024-10-07T07:48:10Z","published":"2021-09-22T07:44:01Z","title":"A Few-Shot Learning Approach for Sound Source Distance Estimation Using\n  Relation Networks","summary":"  In this paper, we study the performance of few-shot learning, specifically\nmeta learning empowered few-shot relation networks, over supervised deep\nlearning and conventional machine learning approaches in the problem of Sound\nSource Distance Estimation (SSDE). In previous research on deep supervised\nSSDE, low accuracies have often resulted from the mismatch between the training\ndata (from known environments) and the test data (from unknown environments).\nBy performing comparative experiments on a sufficient amount of data, we show\nthat the few-shot relation network outperforms other competitors including\neXtreme Gradient Boosting (XGBoost), Support Vector Machine (SVM),\nConvolutional Neural Network (CNN), and MultiLayer Perceptron (MLP). Hence it\nis possible to calibrate a microphone-equipped system, with a few labeled\nsamples of audio recorded in a particular unknown environment to adjust and\ngeneralize our classifier to the possible input data and gain higher\naccuracies.\n","authors":["Amirreza Sobhdel","Roozbeh Razavi-Far","Vasile Palade"],"pdf_url":"https://arxiv.org/pdf/2109.10561v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04797v1","updated":"2024-10-07T07:16:29Z","published":"2024-10-07T07:16:29Z","title":"Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis","summary":"  Voice disorders negatively impact the quality of daily life in various ways.\nHowever, accurately recognizing the category of pathological features from raw\naudio remains a considerable challenge due to the limited dataset. A promising\nmethod to handle this issue is extracting multi-level pathological information\nfrom speech in a comprehensive manner by fusing features in the latent space.\nIn this paper, a novel framework is designed to explore the way of high-quality\nfeature fusion for effective and generalized detection performance.\nSpecifically, the proposed model follows a two-stage training paradigm: (1)\nECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various\ndomains are employed to learn the universal pathological information from raw\naudio; (2) An attentive fusion module is dedicatedly designed to establish the\ninteraction between pathological features projected by EcapTdnn and Wav2vec 2.0\nrespectively and guide the multi-layer fusion, the entire model is jointly\nfine-tuned from pre-trained features by the automatic voice pathology detection\ntask. Finally, comprehensive experiments on the FEMH and SVD datasets\ndemonstrate that the proposed framework outperforms the competitive baselines,\nand achieves the accuracy of 90.51% and 87.68%.\n","authors":["Lipeng Shen","Yifan Xiong","Dongyue Guo","Wei Mo","Lingyu Yu","Hui Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07284v4","updated":"2024-10-07T06:54:30Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v4.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2410.04785v1","updated":"2024-10-07T06:50:19Z","published":"2024-10-07T06:50:19Z","title":"Towards Ultra-Low-Power Neuromorphic Speech Enhancement with\n  Spiking-FullSubNet","summary":"  Speech enhancement is critical for improving speech intelligibility and\nquality in various audio devices. In recent years, deep learning-based methods\nhave significantly improved speech enhancement performance, but they often come\nwith a high computational cost, which is prohibitive for a large number of edge\ndevices, such as headsets and hearing aids. This work proposes an\nultra-low-power speech enhancement system based on the brain-inspired spiking\nneural network (SNN) called Spiking-FullSubNet. Spiking-FullSubNet follows a\nfull-band and sub-band fusioned approach to effectively capture both global and\nlocal spectral information. To enhance the efficiency of computationally\nexpensive sub-band modeling, we introduce a frequency partitioning method\ninspired by the sensitivity profile of the human peripheral auditory system.\nFurthermore, we introduce a novel spiking neuron model that can dynamically\ncontrol the input information integration and forgetting, enhancing the\nmulti-scale temporal processing capability of SNN, which is critical for speech\ndenoising. Experiments conducted on the recent Intel Neuromorphic Deep Noise\nSuppression (N-DNS) Challenge dataset show that the Spiking-FullSubNet\nsurpasses state-of-the-art methods by large margins in terms of both speech\nquality and energy efficiency metrics. Notably, our system won the championship\nof the Intel N-DNS Challenge (Algorithmic Track), opening up a myriad of\nopportunities for ultra-low-power speech enhancement at the edge. Our source\ncode and model checkpoints are publicly available at\nhttps://github.com/haoxiangsnr/spiking-fullsubnet.\n","authors":["Xiang Hao","Chenxiang Ma","Qu Yang","Jibin Wu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2410.04785v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.04704v1","updated":"2024-10-07T02:48:18Z","published":"2024-10-07T02:48:18Z","title":"Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model","summary":"  Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.\n","authors":["Kai Lia","Masato Akagia","Yongwei Lib","Masashi Unokia"],"pdf_url":"https://arxiv.org/pdf/2410.04704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04702v1","updated":"2024-10-07T02:38:58Z","published":"2024-10-07T02:38:58Z","title":"Demo of Zero-Shot Guitar Amplifier Modelling: Enhancing Modeling with\n  Hyper Neural Networks","summary":"  Electric guitar tone modeling typically focuses on the non-linear\ntransformation from clean to amplifier-rendered audio. Traditional methods rely\non one-to-one mappings, incorporating device parameters into neural models to\nreplicate specific amplifiers. However, these methods are limited by the need\nfor specific training data. In this paper, we adapt a model based on the\nprevious work, which leverages a tone embedding encoder and a feature wise\nlinear modulation (FiLM) condition method. In this work, we altered\nconditioning method using a hypernetwork-based gated convolutional network\n(GCN) to generate audio that blends clean input with the tone characteristics\nof reference audio. By extending the training data to cover a wider variety of\namplifier tones, our model is able to capture a broader range of tones.\nAdditionally, we developed a real-time plugin to demonstrate the system's\npractical application, allowing users to experience its performance\ninteractively. Our results indicate that the proposed system achieves superior\ntone modeling versatility compared to traditional methods.\n","authors":["Yu-Hua Chen","Yuan-Chiao Cheng","Yen-Tung Yeh","Jui-Te Wu","Yu-Hsiang Ho","Jyh-Shing Roger Jang","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04702v1.pdf","comment":"demo of the ISMIR paper"},{"id":"http://arxiv.org/abs/2403.05010v3","updated":"2024-10-07T02:08:05Z","published":"2024-03-08T03:16:47Z","title":"RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction","summary":"  Recent advancements in generative modeling have significantly enhanced the\nreconstruction of audio waveforms from various representations. While diffusion\nmodels are adept at this task, they are hindered by latency issues due to their\noperation at the individual sample point level and the need for numerous\nsampling steps. In this study, we introduce RFWave, a cutting-edge multi-band\nRectified Flow approach designed to reconstruct high-fidelity audio waveforms\nfrom Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates\ncomplex spectrograms and operates at the frame level, processing all subbands\nsimultaneously to boost efficiency. Leveraging Rectified Flow, which targets a\nstraight transport trajectory, RFWave achieves reconstruction with just 10\nsampling steps. Our empirical evaluations show that RFWave not only provides\noutstanding reconstruction quality but also offers vastly superior\ncomputational efficiency, enabling audio generation at speeds up to 160 times\nfaster than real-time on a GPU. An online demonstration is available at:\nhttps://rfwave-demo.github.io/rfwave/.\n","authors":["Peng Liu","Dongyang Dai","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05010v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04690v1","updated":"2024-10-07T02:04:58Z","published":"2024-10-07T02:04:58Z","title":"SegINR: Segment-wise Implicit Neural Representation for Sequence\n  Alignment in Neural Text-to-Speech","summary":"  We present SegINR, a novel approach to neural Text-to-Speech (TTS) that\naddresses sequence alignment without relying on an auxiliary duration predictor\nand complex autoregressive (AR) or non-autoregressive (NAR) frame-level\nsequence modeling. SegINR simplifies the process by converting text sequences\ndirectly into frame-level features. It leverages an optimal text encoder to\nextract embeddings, transforming each into a segment of frame-level features\nusing a conditional implicit neural representation (INR). This method, named\nsegment-wise INR (SegINR), models temporal dynamics within each segment and\nautonomously defines segment boundaries, reducing computational costs. We\nintegrate SegINR into a two-stage TTS framework, using it for semantic token\nprediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate\nthat SegINR outperforms conventional methods in speech quality with\ncomputational efficiency.\n","authors":["Minchan Kim","Myeonghun Jeong","Joun Yeop Lee","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04690v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"}]},"2024-10-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2211.11548v2","updated":"2024-10-06T23:54:24Z","published":"2022-09-17T05:34:32Z","title":"Survey of Query-based Text Summarization","summary":"  Query-based text summarization is an important real world problem that\nrequires to condense the prolix text data into a summary under the guidance of\nthe query information provided by users. The topic has been studied for a long\ntime and there are many existing interesting research related to query-based\ntext summarization. Yet much of the work is not systematically surveyed. This\nsurvey aims at summarizing some interesting work in query-based text\nsummarization methods as well as related generic text summarization methods.\nNot all taxonomies in this paper exist the related work to the best of our\nknowledge and some analysis will be presented.\n","authors":["Hang Yu","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2211.11548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07064v3","updated":"2024-10-06T23:53:34Z","published":"2023-11-13T04:08:49Z","title":"Prompts have evil twins","summary":"  We discover that many natural-language prompts can be replaced by\ncorresponding prompts that are unintelligible to humans but that provably\nelicit similar behavior in language models. We call these prompts \"evil twins\"\nbecause they are obfuscated and uninterpretable (evil), but at the same time\nmimic the functionality of the original natural-language prompts (twins).\nRemarkably, evil twins transfer between models. We find these prompts by\nsolving a maximum-likelihood problem which has applications of independent\ninterest.\n","authors":["Rimon Melamed","Lucas H. McCabe","Tanay Wakhare","Yejin Kim","H. Howie Huang","Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2311.07064v3.pdf","comment":"EMNLP 2024 Main, camera-ready"},{"id":"http://arxiv.org/abs/2409.18025v2","updated":"2024-10-06T23:30:44Z","published":"2024-09-26T16:32:19Z","title":"An Adversarial Perspective on Machine Unlearning for AI Safety","summary":"  Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.\n","authors":["Jakub Łucki","Boyi Wei","Yangsibo Huang","Peter Henderson","Florian Tramèr","Javier Rando"],"pdf_url":"https://arxiv.org/pdf/2409.18025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11823v2","updated":"2024-10-06T22:42:47Z","published":"2024-06-17T17:57:30Z","title":"On Efficient Language and Vision Assistants for Visually-Situated\n  Natural Language Understanding: What Matters in Reading and Reasoning","summary":"  Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva.\n","authors":["Geewook Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.11823v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2408.08926v2","updated":"2024-10-06T22:19:54Z","published":"2024-08-15T17:23:10Z","title":"Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks\n  of Language Models","summary":"  Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have the potential to cause\nreal-world impact. Policymakers, model providers, and other researchers in the\nAI and cybersecurity communities are interested in quantifying the capabilities\nof such agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute bash commands and\nobserve outputs. Since many tasks are beyond the capabilities of existing LM\nagents, we introduce subtasks for each task, which break down a task into\nintermediary steps for a more detailed evaluation. To evaluate agent\ncapabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o,\nOpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,\nGemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask\nguidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and\nClaude 3 Opus successfully solved complete tasks that took human teams up to 11\nminutes to solve. In comparison, the most difficult task took human teams 24\nhours and 54 minutes to solve. All code and data are publicly available at\nhttps://cybench.github.io\n","authors":["Andy K. Zhang","Neil Perry","Riya Dulepet","Joey Ji","Justin W. Lin","Eliot Jones","Celeste Menders","Gashon Hussein","Samantha Liu","Donovan Jasper","Pura Peetathawatchai","Ari Glenn","Vikram Sivashankar","Daniel Zamoshchin","Leo Glikbarg","Derek Askaryar","Mike Yang","Teddy Zhang","Rishi Alluri","Nathan Tran","Rinnara Sangpisit","Polycarpos Yiorkadjis","Kenny Osele","Gautham Raghupathi","Dan Boneh","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2408.08926v2.pdf","comment":"78 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.12074v2","updated":"2024-10-06T22:17:03Z","published":"2024-06-17T20:20:47Z","title":"COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities","summary":"  Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.\n","authors":["Zihao He","Minh Duc Chu","Rebecca Dorn","Siyi Guo","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.12074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18400v4","updated":"2024-10-06T22:13:16Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v4.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.04633v1","updated":"2024-10-06T21:33:51Z","published":"2024-10-06T21:33:51Z","title":"A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for\n  Speech Emotion Recognition","summary":"  Best-performing speech models are trained on large amounts of data in the\nlanguage they are meant to work for. However, most languages have sparse data,\nmaking training models challenging. This shortage of data is even more\nprevalent in speech emotion recognition. Our work explores the model's\nperformance in limited data, specifically for speech emotion recognition.\nMeta-learning specializes in improving the few-shot learning. As a result, we\nemploy meta-learning techniques on speech emotion recognition tasks, accent\nrecognition, and person identification. To this end, we propose a series of\nimprovements over the multistage meta-learning method. Unlike other works\nfocusing on smaller models due to the high computational cost of meta-learning\nalgorithms, we take a more practical approach. We incorporate a large\npre-trained backbone and a prototypical network, making our methods more\nfeasible and applicable. Our most notable contribution is an improved\nfine-tuning technique during meta-testing that significantly boosts the\nperformance on out-of-distribution datasets. This result, together with\nincremental improvements from several other works, helped us achieve accuracy\nscores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition\ndatasets not included in the training or validation splits in the context of\n4-way 5-shot learning.\n","authors":["David-Gabriel Ion","Răzvan-Alexandru Smădu","Dumitru-Clementin Cercel","Florin Pop","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2410.04633v1.pdf","comment":"16 pages, 1 figure, Accepted by WISE 2024"}],"Sound":[{"id":"http://arxiv.org/abs/2310.17162v3","updated":"2024-10-06T21:36:20Z","published":"2023-10-26T05:24:38Z","title":"Content-based Controls For Music Large Language Modeling","summary":"  Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and arrangement. Our source codes and demos are available online.\n","authors":["Liwei Lin","Gus Xia","Junyan Jiang","Yixiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.17162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09508v3","updated":"2024-10-06T21:26:48Z","published":"2024-02-14T19:00:01Z","title":"Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\n  and Editing via Content-based Controls","summary":"  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n","authors":["Liwei Lin","Gus Xia","Yixiao Zhang","Junyan Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.09508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02512v4","updated":"2024-10-06T20:00:46Z","published":"2022-06-06T11:51:22Z","title":"Unsupervised TTS Acoustic Modeling for TTS with Conditional Disentangled\n  Sequential VAE","summary":"  In this paper, we propose a novel unsupervised text-to-speech acoustic model\ntraining scheme, named UTTS, which does not require text-audio pairs. UTTS is a\nmulti-speaker speech synthesizer that supports zero-shot voice cloning, it is\ndeveloped from a perspective of disentangled speech representation learning.\nThe framework offers a flexible choice of a speaker's duration model, timbre\nfeature (identity) and content for TTS inference. We leverage recent\nadvancements in self-supervised speech representation learning as well as\nspeech synthesis front-end techniques for system development. Specifically, we\nemploy our recently formulated Conditional Disentangled Sequential Variational\nAuto-encoder (C-DSVAE) as the backbone UTTS AM, which offers well-structured\ncontent representations given unsupervised alignment (UA) as condition during\ntraining. For UTTS inference, we utilize a lexicon to map input text to the\nphoneme sequence, which is expanded to the frame-level forced alignment (FA)\nwith a speaker-dependent duration model. Then, we develop an alignment mapping\nmodule that converts FA to UA. Finally, the C-DSVAE, serving as the\nself-supervised TTS AM, takes the predicted UA and a target speaker embedding\nto generate the mel spectrogram, which is ultimately converted to waveform with\na neural vocoder. We show how our method enables speech synthesis without using\na paired TTS corpus in AM development stage. Experiments demonstrate that UTTS\ncan synthesize speech of high naturalness and intelligibility measured by human\nand objective evaluations. Audio samples are available at our demo page\nhttps://neurtts.github.io/utts\\_demo/.\n","authors":["Jiachen Lian","Chunlei Zhang","Gopala Krishna Anumanchipalli","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2206.02512v4.pdf","comment":"IEEE/ACM Transactions on Audio, Speech, and Language Processing (\n  Volume: 31)"},{"id":"http://arxiv.org/abs/2410.04534v1","updated":"2024-10-06T16:04:05Z","published":"2024-10-06T16:04:05Z","title":"UniMuMo: Unified Text, Music and Motion Generation","summary":"  We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\ntext, music, and motion data as input conditions to generate outputs across all\nthree modalities. To address the lack of time-synchronized data, we align\nunpaired music and motion data based on rhythmic patterns to leverage existing\nlarge-scale music-only and motion-only datasets. By converting music, motion,\nand text into token-based representation, our model bridges these modalities\nthrough a unified encoder-decoder transformer architecture. To support multiple\ngeneration tasks within a single framework, we introduce several architectural\nimprovements. We propose encoding motion with a music codebook, mapping motion\ninto the same feature space as music. We introduce a music-motion parallel\ngeneration scheme that unifies all music and motion generation tasks into a\nsingle transformer decoder architecture with a single training task of\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\nexisting pre-trained single-modality models, significantly reducing\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\ncompetitive results on all unidirectional generation benchmarks across music,\nmotion, and text modalities. Quantitative results are available in the\n\\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.\n","authors":["Han Yang","Kun Su","Yutong Zhang","Jiaben Chen","Kaizhi Qian","Gaowen Liu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2410.04534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04478v1","updated":"2024-10-06T13:39:15Z","published":"2024-10-06T13:39:15Z","title":"Configurable Multilingual ASR with Speech Summary Representations","summary":"  Approximately half of the world's population is multilingual, making\nmultilingual ASR (MASR) essential. Deploying multiple monolingual models is\nchallenging when the ground-truth language is unknown in advance. This\nmotivates research efforts on configurable multilingual MASR models that can be\nprompted manually or adapted automatically to recognise specific languages. In\nthis paper, we present the Configurable MASR model with Summary Vector\n(csvMASR), a novel architecture designed to enhance configurability. Our\napproach leverages adapters and introduces speech summary vector\nrepresentations, inspired by conversational summary representations in speech\ndiarization, to combine outputs from language-specific components at the\nutterance level. We also incorporate an auxiliary language classification loss\nto enhance configurability. Using data from 7 languages in the Multilingual\nLibrispeech (MLS) dataset, csvMASR outperforms existing MASR models and reduces\nthe word error rate (WER) from 10.33\\% to 9.95\\% when compared with the\nbaseline. Additionally, csvMASR demonstrates superior performance in language\nclassification and prompting tasks.\n","authors":["Harrison Zhu","Ivan Fung","Yingke Zhu","Lahiru Samarakoon"],"pdf_url":"https://arxiv.org/pdf/2410.04478v1.pdf","comment":"A preprint"},{"id":"http://arxiv.org/abs/2309.13874v2","updated":"2024-10-06T11:12:59Z","published":"2023-09-25T04:58:38Z","title":"DDTSE: Discriminative Diffusion Model for Target Speech Extraction","summary":"  Diffusion models have gained attention in speech enhancement tasks, providing\nan alternative to conventional discriminative methods. However, research on\ntarget speech extraction under multi-speaker noisy conditions remains\nrelatively unexplored. Moreover, the superior quality of diffusion methods\ntypically comes at the cost of slower inference speed. In this paper, we\nintroduce the Discriminative Diffusion model for Target Speech Extraction\n(DDTSE). We apply the same forward process as diffusion models and utilize the\nreconstruction loss similar to discriminative methods. Furthermore, we devise a\ntwo-stage training strategy to emulate the inference process during model\ntraining. DDTSE not only works as a standalone system, but also can further\nimprove the performance of discriminative models without additional retraining.\nExperimental results demonstrate that DDTSE not only achieves higher perceptual\nquality but also accelerates the inference process by 3 times compared to the\nconventional diffusion model.\n","authors":["Leying Zhang","Yao Qian","Linfeng Yu","Heming Wang","Hemin Yang","Long Zhou","Shujie Liu","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2309.13874v2.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2408.14080v3","updated":"2024-10-06T04:03:35Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these inventions democratize music creation, they also\nnecessitate the ability to distinguish between human-composed and synthetic\nsongs to safeguard artistic integrity and protect human musical artistry.\nExisting research and datasets in fake song detection only focus on singing\nvoice deepfake detection (SVDD), where the vocals are AI-generated but the\ninstrumental music is sourced from real songs. However, these approaches are\ninadequate for detecting contemporary end-to-end artificial songs where all\ncomponents (vocals, music, lyrics, and style) could be AI-generated.\nAdditionally, existing datasets lack music-lyrics diversity, long-duration\nsongs, and open-access fake songs. To address these gaps, we introduce SONICS,\na novel dataset for end-to-end Synthetic Song Detection (SSD), comprising over\n97k songs (4,751 hours) with over 49k synthetic songs from popular platforms\nlike Suno and Udio. Furthermore, we highlight the importance of modeling\nlong-range temporal dependencies in songs for effective authenticity detection,\nan aspect entirely overlooked in existing methods. To utilize long-range\npatterns, we introduce SpecTTTra, a novel architecture that significantly\nimproves time and memory efficiency over conventional CNN and Transformer-based\nmodels. In particular, for long audio samples, our top-performing variant\noutperforms ViT by 8% F1 score while being 38% faster and using 26% less\nmemory. Additionally, in comparison with ConvNeXt, our model achieves 1% gain\nin F1 score with 20% boost in speed and 67% reduction in memory usage. Other\nvariants of our model family provide even better speed and memory efficiency\nwith competitive performance.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v3.pdf","comment":"Updated with correction"},{"id":"http://arxiv.org/abs/2409.08103v2","updated":"2024-10-06T01:32:03Z","published":"2024-09-12T14:55:33Z","title":"The Faetar Benchmark: Speech Recognition in a Very Under-Resourced\n  Language","summary":"  We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark\ncorpus designed to push the limits of current approaches to low-resource speech\nrecognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy,\nhas no standard orthography, has virtually no existing textual or speech\nresources other than what is included in the benchmark, and is quite different\nfrom other forms of Franco-Proven\\c{c}al. The corpus comes from field\nrecordings, most of which are noisy, for which only 5 hrs have matching\ntranscriptions, and for which forced alignment is of variable quality. The\ncorpus contains an additional 20 hrs of unlabelled speech. We report baseline\nresults from state-of-the-art multilingual speech foundation models with a best\nphone error rate of 30.4%, using a pipeline that continues pre-training on the\nfoundation model using the unlabelled set.\n","authors":["Michael Ong","Sean Robertson","Leo Peckham","Alba Jorquera Jimenez de Aberasturi","Paula Arkhangorodsky","Robin Huo","Aman Sakhardande","Mark Hallap","Naomi Nagy","Ewan Dunbar"],"pdf_url":"https://arxiv.org/pdf/2409.08103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04324v1","updated":"2024-10-06T01:03:42Z","published":"2024-10-06T01:03:42Z","title":"SONAR: A Synthetic AI-Audio Detection Framework~and Benchmark","summary":"  Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using\ngenerative Artificial Intelligence (AI) technology have made it possible to\ngenerate high-quality and realistic human-like audio. This introduces\nsignificant challenges to distinguishing AI-synthesized speech from the\nauthentic human voice and could raise potential issues of misuse for malicious\npurposes such as impersonation and fraud, spreading misinformation, deepfakes,\nand scams. However, existing detection techniques for AI-synthesized audio have\nnot kept pace and often exhibit poor generalization across diverse datasets. In\nthis paper, we introduce SONAR, a synthetic AI-Audio Detection Framework and\nBenchmark, aiming to provide a comprehensive evaluation for distinguishing\ncutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation\ndataset sourced from 9 diverse audio synthesis platforms, including leading TTS\nproviders and state-of-the-art TTS models. It is the first framework to\nuniformly benchmark AI-audio detection across both traditional and foundation\nmodel-based deepfake detection systems. Through extensive experiments, we\nreveal the generalization limitations of existing detection methods and\ndemonstrate that foundation models exhibit stronger generalization\ncapabilities, which can be attributed to their model size and the scale and\nquality of pretraining data. Additionally, we explore the effectiveness and\nefficiency of few-shot fine-tuning in improving generalization, highlighting\nits potential for tailored applications, such as personalized detection systems\nfor specific entities or individuals. Code and dataset are available at\nhttps://github.com/Jessegator/SONAR.\n","authors":["Xiang Li","Pin-Yu Chen","Wenqi Wei"],"pdf_url":"https://arxiv.org/pdf/2410.04324v1.pdf","comment":null}],"Speech Processing":[{"id":"http://arxiv.org/abs/2310.17162v3","updated":"2024-10-06T21:36:20Z","published":"2023-10-26T05:24:38Z","title":"Content-based Controls For Music Large Language Modeling","summary":"  Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and arrangement. Our source codes and demos are available online.\n","authors":["Liwei Lin","Gus Xia","Junyan Jiang","Yixiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.17162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09508v3","updated":"2024-10-06T21:26:48Z","published":"2024-02-14T19:00:01Z","title":"Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\n  and Editing via Content-based Controls","summary":"  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n","authors":["Liwei Lin","Gus Xia","Yixiao Zhang","Junyan Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.09508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02512v4","updated":"2024-10-06T20:00:46Z","published":"2022-06-06T11:51:22Z","title":"Unsupervised TTS Acoustic Modeling for TTS with Conditional Disentangled\n  Sequential VAE","summary":"  In this paper, we propose a novel unsupervised text-to-speech acoustic model\ntraining scheme, named UTTS, which does not require text-audio pairs. UTTS is a\nmulti-speaker speech synthesizer that supports zero-shot voice cloning, it is\ndeveloped from a perspective of disentangled speech representation learning.\nThe framework offers a flexible choice of a speaker's duration model, timbre\nfeature (identity) and content for TTS inference. We leverage recent\nadvancements in self-supervised speech representation learning as well as\nspeech synthesis front-end techniques for system development. Specifically, we\nemploy our recently formulated Conditional Disentangled Sequential Variational\nAuto-encoder (C-DSVAE) as the backbone UTTS AM, which offers well-structured\ncontent representations given unsupervised alignment (UA) as condition during\ntraining. For UTTS inference, we utilize a lexicon to map input text to the\nphoneme sequence, which is expanded to the frame-level forced alignment (FA)\nwith a speaker-dependent duration model. Then, we develop an alignment mapping\nmodule that converts FA to UA. Finally, the C-DSVAE, serving as the\nself-supervised TTS AM, takes the predicted UA and a target speaker embedding\nto generate the mel spectrogram, which is ultimately converted to waveform with\na neural vocoder. We show how our method enables speech synthesis without using\na paired TTS corpus in AM development stage. Experiments demonstrate that UTTS\ncan synthesize speech of high naturalness and intelligibility measured by human\nand objective evaluations. Audio samples are available at our demo page\nhttps://neurtts.github.io/utts\\_demo/.\n","authors":["Jiachen Lian","Chunlei Zhang","Gopala Krishna Anumanchipalli","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2206.02512v4.pdf","comment":"IEEE/ACM Transactions on Audio, Speech, and Language Processing (\n  Volume: 31)"},{"id":"http://arxiv.org/abs/2410.04534v1","updated":"2024-10-06T16:04:05Z","published":"2024-10-06T16:04:05Z","title":"UniMuMo: Unified Text, Music and Motion Generation","summary":"  We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\ntext, music, and motion data as input conditions to generate outputs across all\nthree modalities. To address the lack of time-synchronized data, we align\nunpaired music and motion data based on rhythmic patterns to leverage existing\nlarge-scale music-only and motion-only datasets. By converting music, motion,\nand text into token-based representation, our model bridges these modalities\nthrough a unified encoder-decoder transformer architecture. To support multiple\ngeneration tasks within a single framework, we introduce several architectural\nimprovements. We propose encoding motion with a music codebook, mapping motion\ninto the same feature space as music. We introduce a music-motion parallel\ngeneration scheme that unifies all music and motion generation tasks into a\nsingle transformer decoder architecture with a single training task of\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\nexisting pre-trained single-modality models, significantly reducing\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\ncompetitive results on all unidirectional generation benchmarks across music,\nmotion, and text modalities. Quantitative results are available in the\n\\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.\n","authors":["Han Yang","Kun Su","Yutong Zhang","Jiaben Chen","Kaizhi Qian","Gaowen Liu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2410.04534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04478v1","updated":"2024-10-06T13:39:15Z","published":"2024-10-06T13:39:15Z","title":"Configurable Multilingual ASR with Speech Summary Representations","summary":"  Approximately half of the world's population is multilingual, making\nmultilingual ASR (MASR) essential. Deploying multiple monolingual models is\nchallenging when the ground-truth language is unknown in advance. This\nmotivates research efforts on configurable multilingual MASR models that can be\nprompted manually or adapted automatically to recognise specific languages. In\nthis paper, we present the Configurable MASR model with Summary Vector\n(csvMASR), a novel architecture designed to enhance configurability. Our\napproach leverages adapters and introduces speech summary vector\nrepresentations, inspired by conversational summary representations in speech\ndiarization, to combine outputs from language-specific components at the\nutterance level. We also incorporate an auxiliary language classification loss\nto enhance configurability. Using data from 7 languages in the Multilingual\nLibrispeech (MLS) dataset, csvMASR outperforms existing MASR models and reduces\nthe word error rate (WER) from 10.33\\% to 9.95\\% when compared with the\nbaseline. Additionally, csvMASR demonstrates superior performance in language\nclassification and prompting tasks.\n","authors":["Harrison Zhu","Ivan Fung","Yingke Zhu","Lahiru Samarakoon"],"pdf_url":"https://arxiv.org/pdf/2410.04478v1.pdf","comment":"A preprint"},{"id":"http://arxiv.org/abs/2309.13874v2","updated":"2024-10-06T11:12:59Z","published":"2023-09-25T04:58:38Z","title":"DDTSE: Discriminative Diffusion Model for Target Speech Extraction","summary":"  Diffusion models have gained attention in speech enhancement tasks, providing\nan alternative to conventional discriminative methods. However, research on\ntarget speech extraction under multi-speaker noisy conditions remains\nrelatively unexplored. Moreover, the superior quality of diffusion methods\ntypically comes at the cost of slower inference speed. In this paper, we\nintroduce the Discriminative Diffusion model for Target Speech Extraction\n(DDTSE). We apply the same forward process as diffusion models and utilize the\nreconstruction loss similar to discriminative methods. Furthermore, we devise a\ntwo-stage training strategy to emulate the inference process during model\ntraining. DDTSE not only works as a standalone system, but also can further\nimprove the performance of discriminative models without additional retraining.\nExperimental results demonstrate that DDTSE not only achieves higher perceptual\nquality but also accelerates the inference process by 3 times compared to the\nconventional diffusion model.\n","authors":["Leying Zhang","Yao Qian","Linfeng Yu","Heming Wang","Hemin Yang","Long Zhou","Shujie Liu","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2309.13874v2.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.04380v1","updated":"2024-10-06T07:20:58Z","published":"2024-10-06T07:20:58Z","title":"HALL-E: Hierarchical Neural Codec Language Model for Minute-Long\n  Zero-Shot Text-to-Speech Synthesis","summary":"  Recently, Text-to-speech (TTS) models based on large language models (LLMs)\nthat translate natural language text into sequences of discrete audio tokens\nhave gained great research attention, with advances in neural audio codec (NAC)\nmodels using residual vector quantization (RVQ). However, long-form speech\nsynthesis remains a significant challenge due to the high frame rate, which\nincreases the length of audio tokens and makes it difficult for autoregressive\nlanguage models to generate audio tokens for even a minute of speech. To\naddress this challenge, this paper introduces two novel post-training\napproaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a\nframework to reduce the frame rate of pre-trained NAC models. Specifically, it\nincorporates multi-resolution residual vector quantization (MRVQ) module that\nhierarchically reorganizes discrete audio tokens through teacher-student\ndistillation. HALL-E is an LLM-based TTS model designed to predict hierarchical\ntokens of MReQ. Specifically, it incorporates the technique of using MRVQ\nsub-modules and continues training from a pre-trained LLM-based TTS model.\nFurthermore, to promote TTS research, we create MinutesSpeech, a new benchmark\ndataset consisting of 40k hours of filtered speech data for training and\nevaluating speech synthesis ranging from 3s up to 180s. In experiments, we\ndemonstrated the effectiveness of our approaches by applying our post-training\nframework to VALL-E. We achieved the frame rate down to as low as 8 Hz,\nenabling the stable minitue-long speech synthesis in a single inference step.\nAudio samples, dataset, codes and pre-trained models are available at\nhttps://yutonishimura-v2.github.io/HALL-E_DEMO/.\n","authors":["Yuto Nishimura","Takumi Hirose","Masanari Ohi","Hideki Nakayama","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2410.04380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14080v3","updated":"2024-10-06T04:03:35Z","published":"2024-08-26T08:02:57Z","title":"SONICS: Synthetic Or Not -- Identifying Counterfeit Songs","summary":"  The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. While these inventions democratize music creation, they also\nnecessitate the ability to distinguish between human-composed and synthetic\nsongs to safeguard artistic integrity and protect human musical artistry.\nExisting research and datasets in fake song detection only focus on singing\nvoice deepfake detection (SVDD), where the vocals are AI-generated but the\ninstrumental music is sourced from real songs. However, these approaches are\ninadequate for detecting contemporary end-to-end artificial songs where all\ncomponents (vocals, music, lyrics, and style) could be AI-generated.\nAdditionally, existing datasets lack music-lyrics diversity, long-duration\nsongs, and open-access fake songs. To address these gaps, we introduce SONICS,\na novel dataset for end-to-end Synthetic Song Detection (SSD), comprising over\n97k songs (4,751 hours) with over 49k synthetic songs from popular platforms\nlike Suno and Udio. Furthermore, we highlight the importance of modeling\nlong-range temporal dependencies in songs for effective authenticity detection,\nan aspect entirely overlooked in existing methods. To utilize long-range\npatterns, we introduce SpecTTTra, a novel architecture that significantly\nimproves time and memory efficiency over conventional CNN and Transformer-based\nmodels. In particular, for long audio samples, our top-performing variant\noutperforms ViT by 8% F1 score while being 38% faster and using 26% less\nmemory. Additionally, in comparison with ConvNeXt, our model achieves 1% gain\nin F1 score with 20% boost in speed and 67% reduction in memory usage. Other\nvariants of our model family provide even better speed and memory efficiency\nwith competitive performance.\n","authors":["Md Awsafur Rahman","Zaber Ibn Abdul Hakim","Najibul Haque Sarker","Bishmoy Paul","Shaikh Anowarul Fattah"],"pdf_url":"https://arxiv.org/pdf/2408.14080v3.pdf","comment":"Updated with correction"},{"id":"http://arxiv.org/abs/2409.08103v2","updated":"2024-10-06T01:32:03Z","published":"2024-09-12T14:55:33Z","title":"The Faetar Benchmark: Speech Recognition in a Very Under-Resourced\n  Language","summary":"  We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark\ncorpus designed to push the limits of current approaches to low-resource speech\nrecognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy,\nhas no standard orthography, has virtually no existing textual or speech\nresources other than what is included in the benchmark, and is quite different\nfrom other forms of Franco-Proven\\c{c}al. The corpus comes from field\nrecordings, most of which are noisy, for which only 5 hrs have matching\ntranscriptions, and for which forced alignment is of variable quality. The\ncorpus contains an additional 20 hrs of unlabelled speech. We report baseline\nresults from state-of-the-art multilingual speech foundation models with a best\nphone error rate of 30.4%, using a pipeline that continues pre-training on the\nfoundation model using the unlabelled set.\n","authors":["Michael Ong","Sean Robertson","Leo Peckham","Alba Jorquera Jimenez de Aberasturi","Paula Arkhangorodsky","Robin Huo","Aman Sakhardande","Mark Hallap","Naomi Nagy","Ewan Dunbar"],"pdf_url":"https://arxiv.org/pdf/2409.08103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04324v1","updated":"2024-10-06T01:03:42Z","published":"2024-10-06T01:03:42Z","title":"SONAR: A Synthetic AI-Audio Detection Framework~and Benchmark","summary":"  Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using\ngenerative Artificial Intelligence (AI) technology have made it possible to\ngenerate high-quality and realistic human-like audio. This introduces\nsignificant challenges to distinguishing AI-synthesized speech from the\nauthentic human voice and could raise potential issues of misuse for malicious\npurposes such as impersonation and fraud, spreading misinformation, deepfakes,\nand scams. However, existing detection techniques for AI-synthesized audio have\nnot kept pace and often exhibit poor generalization across diverse datasets. In\nthis paper, we introduce SONAR, a synthetic AI-Audio Detection Framework and\nBenchmark, aiming to provide a comprehensive evaluation for distinguishing\ncutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation\ndataset sourced from 9 diverse audio synthesis platforms, including leading TTS\nproviders and state-of-the-art TTS models. It is the first framework to\nuniformly benchmark AI-audio detection across both traditional and foundation\nmodel-based deepfake detection systems. Through extensive experiments, we\nreveal the generalization limitations of existing detection methods and\ndemonstrate that foundation models exhibit stronger generalization\ncapabilities, which can be attributed to their model size and the scale and\nquality of pretraining data. Additionally, we explore the effectiveness and\nefficiency of few-shot fine-tuning in improving generalization, highlighting\nits potential for tailored applications, such as personalized detection systems\nfor specific entities or individuals. Code and dataset are available at\nhttps://github.com/Jessegator/SONAR.\n","authors":["Xiang Li","Pin-Yu Chen","Wenqi Wei"],"pdf_url":"https://arxiv.org/pdf/2410.04324v1.pdf","comment":null}]},"2024-10-05T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2403.10493v4","updated":"2024-10-05T22:31:38Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly perform generation\nby constructing an image representation of audio (e.g., a mel-spectrogram) and\nthen convert it to audio using a phase reconstruction model or vocoder. Typical\nvocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24\nkHz), which limits their usefulness. We propose MusicHiFi -- an efficient\nhigh-fidelity stereophonic vocoder. Our method employs a cascade of three\ngenerative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nextension, and upmixes to stereophonic audio. Compared to past work, we propose\n1) a unified GAN-based generator and discriminator architecture and training\nprocedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using objective and\nsubjective listening tests and find our approach yields comparable or better\naudio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\n\\url{https://MusicHiFi.github.io/web/}.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v4.pdf","comment":"Accepted to IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2310.06930v2","updated":"2024-10-05T21:07:38Z","published":"2023-10-10T18:33:47Z","title":"Prosody Analysis of Audiobooks","summary":"  Recent advances in text-to-speech have made it possible to generate\nnatural-sounding audio from text. However, audiobook narrations involve\ndramatic vocalizations and intonations by the reader, with greater reliance on\nemotions, dialogues, and descriptions in the narrative. Using our dataset of 93\naligned book-audiobook pairs, we present improved models for prosody prediction\nproperties (pitch, volume, and rate of speech) from narrative text using\nlanguage modeling. Our predicted prosody attributes correlate much better with\nhuman audiobook readings than results from a state-of-the-art commercial TTS\nsystem: our predicted pitch shows a higher correlation with human reading for\n22 out of the 24 books, while our predicted volume attribute proves more\nsimilar to human reading for 23 out of the 24 books. Finally, we present a\nhuman evaluation study to quantify the extent that people prefer\nprosody-enhanced audiobook readings over commercial text-to-speech systems.\n","authors":["Charuta Pethe","Bach Pham","Felix D Childress","Yunting Yin","Steven Skiena"],"pdf_url":"https://arxiv.org/pdf/2310.06930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00344v3","updated":"2024-10-05T19:31:33Z","published":"2024-10-01T02:43:14Z","title":"Integrating Text-to-Music Models with Language Models: Composing Long\n  Structured Music Pieces","summary":"  Recent music generation methods based on transformers have a context window\nof up to a minute. The music generated by these methods is largely unstructured\nbeyond the context window. With a longer context window, learning long-scale\nstructures from musical data is a prohibitively challenging problem. This paper\nproposes integrating a text-to-music model with a large language model to\ngenerate music with form. The papers discusses the solutions to the challenges\nof such integration. The experimental results show that the proposed method can\ngenerate 2.5-minute-long music that is highly structured, strongly organized,\nand cohesive.\n","authors":["Lilac Atassi"],"pdf_url":"https://arxiv.org/pdf/2410.00344v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.11976"},{"id":"http://arxiv.org/abs/2405.08096v2","updated":"2024-10-05T18:08:53Z","published":"2024-05-13T18:22:02Z","title":"Semantic MIMO Systems for Speech-to-Text Transmission","summary":"  Semantic communications have been utilized to execute numerous intelligent\ntasks by transmitting task-related semantic information instead of bits. In\nthis article, we propose a semantic-aware speech-to-text transmission system\nfor the single-user multiple-input multiple-output (MIMO) and multi-user MIMO\ncommunication scenarios, named SAC-ST. Particularly, a semantic communication\nsystem to serve the speech-to-text task at the receiver is first designed,\nwhich compresses the semantic information and generates the low-dimensional\nsemantic features by leveraging the transformer module. In addition, a novel\nsemantic-aware network is proposed to facilitate transmission with high\nsemantic fidelity by identifying the critical semantic information and\nguaranteeing its accurate recovery. Furthermore, we extend the SAC-ST with a\nneural network-enabled channel estimation network to mitigate the dependence on\naccurate channel state information and validate the feasibility of SAC-ST in\npractical communication environments. Simulation results will show that the\nproposed SAC-ST outperforms the communication framework without the\nsemantic-aware network for speech-to-text transmission over the MIMO channels\nin terms of the speech-to-text metrics, especially in the low signal-to-noise\nregime. Moreover, the SAC-ST with the developed channel estimation network is\ncomparable to the SAC-ST with perfect channel state information.\n","authors":["Zhenzi Weng","Zhijin Qin","Huiqiang Xie","Xiaoming Tao","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2405.08096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04159v1","updated":"2024-10-05T13:46:01Z","published":"2024-10-05T13:46:01Z","title":"Efficient and Robust Long-Form Speech Recognition with Hybrid\n  H3-Conformer","summary":"  Recently, Conformer has achieved state-of-the-art performance in many speech\nrecognition tasks. However, the Transformer-based models show significant\ndeterioration for long-form speech, such as lectures, because the\nself-attention mechanism becomes unreliable with the computation of the square\norder of the input length. To solve the problem, we incorporate a kind of\nstate-space model, Hungry Hungry Hippos (H3), to replace or complement the\nmulti-head self-attention (MHSA). H3 allows for efficient modeling of long-form\nsequences with a linear-order computation. In experiments using two datasets of\nCSJ and LibriSpeech, our proposed H3-Conformer model performs efficient and\nrobust recognition of long-form speech. Moreover, we propose a hybrid of H3 and\nMHSA and show that using H3 in higher layers and MHSA in lower layers provides\nsignificant improvement in online recognition. We also investigate a parallel\nuse of H3 and MHSA in all layers, resulting in the best performance.\n","authors":["Tomoki Honda","Shinsuke Sakai","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.04159v1.pdf","comment":"Submitted to InterSpeech2024, Sample code is available at\n  https://github.com/mirrormouse/Hybrid-H3-Conformer"},{"id":"http://arxiv.org/abs/2312.16422v3","updated":"2024-10-05T12:23:05Z","published":"2023-12-27T06:02:50Z","title":"Selective-Memory Meta-Learning with Environment Representations for\n  Sound Event Localization and Detection","summary":"  Environment shifts and conflicts present significant challenges for\nlearning-based sound event localization and detection (SELD) methods. SELD\nsystems, when trained in particular acoustic settings, often show restricted\ngeneralization capabilities for diverse acoustic environments. Furthermore,\nobtaining annotated samples for spatial sound events is notably costly.\nDeploying a SELD system in a new environment requires extensive time for\nre-training and fine-tuning. To overcome these challenges, we propose\nenvironment-adaptive Meta-SELD, designed for efficient adaptation to new\nenvironments using minimal data. Our method specifically utilizes\ncomputationally synthesized spatial data and employs Model-Agnostic\nMeta-Learning (MAML) on a pre-trained, environment-independent model. The\nmethod then utilizes fast adaptation to unseen real-world environments using\nlimited samples from the respective environments. Inspired by the\nLearning-to-Forget approach, we introduce the concept of selective memory as a\nstrategy for resolving conflicts across environments. This approach involves\nselectively memorizing target-environment-relevant information and adapting to\nthe new environments through the selective attenuation of model parameters. In\naddition, we introduce environment representations to characterize different\nacoustic settings, enhancing the adaptability of our attenuation approach to\nvarious environments. We evaluate our proposed method on the development set of\nthe Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) dataset and\ncomputationally synthesized scenes. Experimental results demonstrate the\nsuperior performance of the proposed method compared to conventional supervised\nlearning methods, particularly in localization.\n","authors":["Jinbo Hu","Yin Cao","Ming Wu","Qiuqiang Kong","Feiran Yang","Mark D. Plumbley","Jun Yang"],"pdf_url":"https://arxiv.org/pdf/2312.16422v3.pdf","comment":"14 pages, 11 figures, accepted by IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing (TASLP)"},{"id":"http://arxiv.org/abs/2410.04098v1","updated":"2024-10-05T09:47:54Z","published":"2024-10-05T09:47:54Z","title":"The OCON model: an old but green solution for distributable supervised\n  classification for acoustic monitoring in smart cities","summary":"  This paper explores a structured application of the One-Class approach and\nthe One-Class-One-Network model for supervised classification tasks, focusing\non vowel phonemes classification and speakers recognition for the Automatic\nSpeech Recognition (ASR) domain. For our case-study, the ASR model runs on a\nproprietary sensing and lightning system, exploited to monitor acoustic and air\npollution on urban streets. We formalize combinations of pseudo-Neural\nArchitecture Search and Hyper-Parameters Tuning experiments, using an informed\ngrid-search methodology, to achieve classification accuracy comparable to\nnowadays most complex architectures, delving into the speaker recognition and\nenergy efficiency aspects. Despite its simplicity, our model proposal has a\nvery good chance to generalize the language and speaker genders context for\nwidespread applicability in computational constrained contexts, proved by\nrelevant statistical and performance metrics. Our experiments code is openly\naccessible on our GitHub.\n","authors":["Stefano Giacomelli","Marco Giordano","Claudia Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2410.04098v1.pdf","comment":"Accepted at \"IEEE 5th International Symposium on the Internet of\n  Sounds, 30 Sep / 2 Oct 2024, Erlangen, Germany\""},{"id":"http://arxiv.org/abs/2410.04091v1","updated":"2024-10-05T09:19:29Z","published":"2024-10-05T09:19:29Z","title":"Cross-Lingual Query-by-Example Spoken Term Detection: A\n  Transformer-Based Approach","summary":"  Query-by-example spoken term detection (QbE-STD) is typically constrained by\ntranscribed data scarcity and language specificity. This paper introduces a\nnovel, language-agnostic QbE-STD model leveraging image processing techniques\nand transformer architecture. By employing a pre-trained XLSR-53 network for\nfeature extraction and a Hough transform for detection, our model effectively\nsearches for user-defined spoken terms within any audio file. Experimental\nresults across four languages demonstrate significant performance gains\n(19-54%) over a CNN-based baseline. While processing time is improved compared\nto DTW, accuracy remains inferior. Notably, our model offers the advantage of\naccurately counting query term repetitions within the target audio.\n","authors":["Allahdadi Fatemeh","Mahdian Toroghi Rahil","Zareian Hassan"],"pdf_url":"https://arxiv.org/pdf/2410.04091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02925v3","updated":"2024-10-05T09:06:11Z","published":"2024-06-05T04:25:56Z","title":"Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech\n  Recognition","summary":"  Synthetic data is widely used in speech recognition due to the availability\nof text-to-speech models, which facilitate adapting models to previously unseen\ntext domains. However, existing methods suffer in performance when they\nfine-tune an automatic speech recognition (ASR) model on synthetic data as they\nsuffer from the distributional shift commonly referred to as the\nsynthetic-to-real gap. In this paper, we find that task vector arithmetic is\neffective at mitigating this gap. Our proposed method, SYN2REAL task vector,\nshows an average improvement of 10.03\\% improvement in word error rate over\nbaselines on the SLURP dataset. Additionally, we show that an average of\nSYN2REAL task vectors, when we have real speeches from multiple different\ndomains, can further adapt the original ASR model to perform better on the\ntarget text domain.\n","authors":["Hsuan Su","Hua Farn","Fan-Yun Sun","Shang-Tse Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.02925v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2310.09505v2","updated":"2024-10-05T08:00:47Z","published":"2023-10-14T06:22:08Z","title":"Advancing Test-Time Adaptation in Wild Acoustic Test Settings","summary":"  Acoustic foundation models, fine-tuned for Automatic Speech Recognition\n(ASR), suffer from performance degradation in wild acoustic test settings when\ndeployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA)\nunder these conditions remains an open and unexplored question. Existing wild\nvision TTA methods often fail to handle speech data effectively due to the\nunique characteristics of high-entropy speech frames, which are unreliably\nfiltered out even when containing crucial semantic content. Furthermore, unlike\nstatic vision data, speech signals follow short-term consistency, requiring\nspecialized adaptation strategies. In this work, we propose a novel wild\nacoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our\nmethod, Confidence-Enhanced Adaptation, performs frame-level adaptation using a\nconfidence-aware weight scheme to avoid filtering out essential information in\nhigh-entropy frames. Additionally, we apply consistency regularization during\ntest-time optimization to leverage the inherent short-term consistency of\nspeech signals. Our experiments on both synthetic and real-world datasets\ndemonstrate that our approach outperforms existing baselines under various wild\nacoustic test settings, including Gaussian noise, environmental sounds, accent\nvariations, and sung speech.\n","authors":["Hongfu Liu","Hengguan Huang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09505v2.pdf","comment":"Accepted to the EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.06995v2","updated":"2024-10-05T07:32:03Z","published":"2024-05-11T12:06:31Z","title":"Benchmarking Cross-Domain Audio-Visual Deception Detection","summary":"  Automated deception detection is crucial for assisting humans in accurately\nassessing truthfulness and identifying deceptive behavior. Conventional\ncontact-based techniques, like polygraph devices, rely on physiological signals\nto determine the authenticity of an individual's statements. Nevertheless,\nrecent developments in automated deception detection have demonstrated that\nmultimodal features derived from both audio and video modalities may outperform\nhuman observers on publicly available datasets. Despite these positive\nfindings, the generalizability of existing audio-visual deception detection\napproaches across different scenarios remains largely unexplored. To close this\ngap, we present the first cross-domain audio-visual deception detection\nbenchmark, that enables us to assess how well these methods generalize for use\nin real-world scenarios. We used widely adopted audio and visual features and\ndifferent architectures for benchmarking, comparing single-to-single and\nmulti-to-single domain generalization performance. To further exploit the\nimpacts using data from multiple source domains for training, we investigate\nthree types of domain sampling strategies, including domain-simultaneous,\ndomain-alternating, and domain-by-domain for multi-to-single domain\ngeneralization evaluation. We also propose an algorithm to enhance the\ngeneralization performance by maximizing the gradient inner products between\nmodality encoders, named ``MM-IDGM\". Furthermore, we proposed the\nAttention-Mixer fusion method to improve performance, and we believe that this\nnew cross-domain benchmark will facilitate future research in audio-visual\ndeception detection.\n","authors":["Xiaobao Guo","Zitong Yu","Nithish Muthuchamy Selvaraj","Bingquan Shen","Adams Wai-Kin Kong","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2405.06995v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.00811v2","updated":"2024-10-05T06:42:40Z","published":"2024-10-01T15:57:35Z","title":"Improving curriculum learning for target speaker extraction with\n  synthetic speakers","summary":"  Target speaker extraction (TSE) aims to isolate individual speaker voices\nfrom complex speech environments. The effectiveness of TSE systems is often\ncompromised when the speaker characteristics are similar to each other. Recent\nresearch has introduced curriculum learning (CL), in which TSE models are\ntrained incrementally on speech samples of increasing complexity. In CL\ntraining, the model is first trained on samples with low speaker similarity\nbetween the target and interference speakers, and then on samples with high\nspeaker similarity. To further improve CL, this paper uses a $k$-nearest\nneighbor-based voice conversion method to simulate and generate speech of\ndiverse interference speakers, and then uses the generated data as part of the\nCL. Experiments demonstrate that training data based on synthetic speakers can\neffectively enhance the model's capabilities and significantly improve the\nperformance of multiple TSE systems.\n","authors":["Yun Liu","Xuechen Liu","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2410.00811v2.pdf","comment":"Accepted by SLT2024"}],"Speech Processing":[{"id":"http://arxiv.org/abs/2403.10493v4","updated":"2024-10-05T22:31:38Z","published":"2024-03-15T17:27:42Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"  Diffusion-based audio and music generation models commonly perform generation\nby constructing an image representation of audio (e.g., a mel-spectrogram) and\nthen convert it to audio using a phase reconstruction model or vocoder. Typical\nvocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24\nkHz), which limits their usefulness. We propose MusicHiFi -- an efficient\nhigh-fidelity stereophonic vocoder. Our method employs a cascade of three\ngenerative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nextension, and upmixes to stereophonic audio. Compared to past work, we propose\n1) a unified GAN-based generator and discriminator architecture and training\nprocedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using objective and\nsubjective listening tests and find our approach yields comparable or better\naudio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\n\\url{https://MusicHiFi.github.io/web/}.\n","authors":["Ge Zhu","Juan-Pablo Caceres","Zhiyao Duan","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2403.10493v4.pdf","comment":"Accepted to IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2310.06930v2","updated":"2024-10-05T21:07:38Z","published":"2023-10-10T18:33:47Z","title":"Prosody Analysis of Audiobooks","summary":"  Recent advances in text-to-speech have made it possible to generate\nnatural-sounding audio from text. However, audiobook narrations involve\ndramatic vocalizations and intonations by the reader, with greater reliance on\nemotions, dialogues, and descriptions in the narrative. Using our dataset of 93\naligned book-audiobook pairs, we present improved models for prosody prediction\nproperties (pitch, volume, and rate of speech) from narrative text using\nlanguage modeling. Our predicted prosody attributes correlate much better with\nhuman audiobook readings than results from a state-of-the-art commercial TTS\nsystem: our predicted pitch shows a higher correlation with human reading for\n22 out of the 24 books, while our predicted volume attribute proves more\nsimilar to human reading for 23 out of the 24 books. Finally, we present a\nhuman evaluation study to quantify the extent that people prefer\nprosody-enhanced audiobook readings over commercial text-to-speech systems.\n","authors":["Charuta Pethe","Bach Pham","Felix D Childress","Yunting Yin","Steven Skiena"],"pdf_url":"https://arxiv.org/pdf/2310.06930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00344v3","updated":"2024-10-05T19:31:33Z","published":"2024-10-01T02:43:14Z","title":"Integrating Text-to-Music Models with Language Models: Composing Long\n  Structured Music Pieces","summary":"  Recent music generation methods based on transformers have a context window\nof up to a minute. The music generated by these methods is largely unstructured\nbeyond the context window. With a longer context window, learning long-scale\nstructures from musical data is a prohibitively challenging problem. This paper\nproposes integrating a text-to-music model with a large language model to\ngenerate music with form. The papers discusses the solutions to the challenges\nof such integration. The experimental results show that the proposed method can\ngenerate 2.5-minute-long music that is highly structured, strongly organized,\nand cohesive.\n","authors":["Lilac Atassi"],"pdf_url":"https://arxiv.org/pdf/2410.00344v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.11976"},{"id":"http://arxiv.org/abs/2405.08096v2","updated":"2024-10-05T18:08:53Z","published":"2024-05-13T18:22:02Z","title":"Semantic MIMO Systems for Speech-to-Text Transmission","summary":"  Semantic communications have been utilized to execute numerous intelligent\ntasks by transmitting task-related semantic information instead of bits. In\nthis article, we propose a semantic-aware speech-to-text transmission system\nfor the single-user multiple-input multiple-output (MIMO) and multi-user MIMO\ncommunication scenarios, named SAC-ST. Particularly, a semantic communication\nsystem to serve the speech-to-text task at the receiver is first designed,\nwhich compresses the semantic information and generates the low-dimensional\nsemantic features by leveraging the transformer module. In addition, a novel\nsemantic-aware network is proposed to facilitate transmission with high\nsemantic fidelity by identifying the critical semantic information and\nguaranteeing its accurate recovery. Furthermore, we extend the SAC-ST with a\nneural network-enabled channel estimation network to mitigate the dependence on\naccurate channel state information and validate the feasibility of SAC-ST in\npractical communication environments. Simulation results will show that the\nproposed SAC-ST outperforms the communication framework without the\nsemantic-aware network for speech-to-text transmission over the MIMO channels\nin terms of the speech-to-text metrics, especially in the low signal-to-noise\nregime. Moreover, the SAC-ST with the developed channel estimation network is\ncomparable to the SAC-ST with perfect channel state information.\n","authors":["Zhenzi Weng","Zhijin Qin","Huiqiang Xie","Xiaoming Tao","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2405.08096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04198v1","updated":"2024-10-05T15:31:48Z","published":"2024-10-05T15:31:48Z","title":"DJ Mix Transcription with Multi-Pass Non-Negative Matrix Factorization","summary":"  DJ mix transcription is a crucial step towards DJ mix reverse engineering,\nwhich estimates the set of parameters and audio effects applied to a set of\nexisting tracks to produce a performative DJ mix. We introduce a new approach\nbased on a multi-pass NMF algorithm where the dictionary matrix corresponds to\na set of spectrogram slices of the source tracks present in the mix.\n  The multi-pass strategy is motivated by the high computational cost resulting\nfrom the use of a large NMF dictionary. The proposed method uses inter-pass\nfiltering to favor temporal continuity and sparseness and is evaluated on a\npublicly available dataset.\n  Our comparative results considering a baseline method based on dynamic time\nwarping (DTW) are promising and pave the way of future NMF-based applications.\n","authors":["Étienne Paul André","Dominique Fourer","Diemo Schwarz"],"pdf_url":"https://arxiv.org/pdf/2410.04198v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.04159v1","updated":"2024-10-05T13:46:01Z","published":"2024-10-05T13:46:01Z","title":"Efficient and Robust Long-Form Speech Recognition with Hybrid\n  H3-Conformer","summary":"  Recently, Conformer has achieved state-of-the-art performance in many speech\nrecognition tasks. However, the Transformer-based models show significant\ndeterioration for long-form speech, such as lectures, because the\nself-attention mechanism becomes unreliable with the computation of the square\norder of the input length. To solve the problem, we incorporate a kind of\nstate-space model, Hungry Hungry Hippos (H3), to replace or complement the\nmulti-head self-attention (MHSA). H3 allows for efficient modeling of long-form\nsequences with a linear-order computation. In experiments using two datasets of\nCSJ and LibriSpeech, our proposed H3-Conformer model performs efficient and\nrobust recognition of long-form speech. Moreover, we propose a hybrid of H3 and\nMHSA and show that using H3 in higher layers and MHSA in lower layers provides\nsignificant improvement in online recognition. We also investigate a parallel\nuse of H3 and MHSA in all layers, resulting in the best performance.\n","authors":["Tomoki Honda","Shinsuke Sakai","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.04159v1.pdf","comment":"Submitted to InterSpeech2024, Sample code is available at\n  https://github.com/mirrormouse/Hybrid-H3-Conformer"},{"id":"http://arxiv.org/abs/2312.16422v3","updated":"2024-10-05T12:23:05Z","published":"2023-12-27T06:02:50Z","title":"Selective-Memory Meta-Learning with Environment Representations for\n  Sound Event Localization and Detection","summary":"  Environment shifts and conflicts present significant challenges for\nlearning-based sound event localization and detection (SELD) methods. SELD\nsystems, when trained in particular acoustic settings, often show restricted\ngeneralization capabilities for diverse acoustic environments. Furthermore,\nobtaining annotated samples for spatial sound events is notably costly.\nDeploying a SELD system in a new environment requires extensive time for\nre-training and fine-tuning. To overcome these challenges, we propose\nenvironment-adaptive Meta-SELD, designed for efficient adaptation to new\nenvironments using minimal data. Our method specifically utilizes\ncomputationally synthesized spatial data and employs Model-Agnostic\nMeta-Learning (MAML) on a pre-trained, environment-independent model. The\nmethod then utilizes fast adaptation to unseen real-world environments using\nlimited samples from the respective environments. Inspired by the\nLearning-to-Forget approach, we introduce the concept of selective memory as a\nstrategy for resolving conflicts across environments. This approach involves\nselectively memorizing target-environment-relevant information and adapting to\nthe new environments through the selective attenuation of model parameters. In\naddition, we introduce environment representations to characterize different\nacoustic settings, enhancing the adaptability of our attenuation approach to\nvarious environments. We evaluate our proposed method on the development set of\nthe Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) dataset and\ncomputationally synthesized scenes. Experimental results demonstrate the\nsuperior performance of the proposed method compared to conventional supervised\nlearning methods, particularly in localization.\n","authors":["Jinbo Hu","Yin Cao","Ming Wu","Qiuqiang Kong","Feiran Yang","Mark D. Plumbley","Jun Yang"],"pdf_url":"https://arxiv.org/pdf/2312.16422v3.pdf","comment":"14 pages, 11 figures, accepted by IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing (TASLP)"},{"id":"http://arxiv.org/abs/2410.04098v1","updated":"2024-10-05T09:47:54Z","published":"2024-10-05T09:47:54Z","title":"The OCON model: an old but green solution for distributable supervised\n  classification for acoustic monitoring in smart cities","summary":"  This paper explores a structured application of the One-Class approach and\nthe One-Class-One-Network model for supervised classification tasks, focusing\non vowel phonemes classification and speakers recognition for the Automatic\nSpeech Recognition (ASR) domain. For our case-study, the ASR model runs on a\nproprietary sensing and lightning system, exploited to monitor acoustic and air\npollution on urban streets. We formalize combinations of pseudo-Neural\nArchitecture Search and Hyper-Parameters Tuning experiments, using an informed\ngrid-search methodology, to achieve classification accuracy comparable to\nnowadays most complex architectures, delving into the speaker recognition and\nenergy efficiency aspects. Despite its simplicity, our model proposal has a\nvery good chance to generalize the language and speaker genders context for\nwidespread applicability in computational constrained contexts, proved by\nrelevant statistical and performance metrics. Our experiments code is openly\naccessible on our GitHub.\n","authors":["Stefano Giacomelli","Marco Giordano","Claudia Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2410.04098v1.pdf","comment":"Accepted at \"IEEE 5th International Symposium on the Internet of\n  Sounds, 30 Sep / 2 Oct 2024, Erlangen, Germany\""},{"id":"http://arxiv.org/abs/2410.04092v1","updated":"2024-10-05T09:23:46Z","published":"2024-10-05T09:23:46Z","title":"Enhancement of Dysarthric Speech Reconstruction by Contrastive Learning","summary":"  Dysarthric speech reconstruction is challenging due to its pathological sound\npatterns. Preserving speaker identity, especially without access to normal\nspeech, is a key challenge. Our proposed approach uses contrastive learning to\nextract speaker embedding for reconstruction, while employing XLS-R\nrepresentations instead of filter banks. The results show improved speech\nquality, naturalness, intelligibility, speaker identity preservation, and\ngender consistency for female speakers. Reconstructed speech exhibits 1.51 and\n2.12 MOS score improvements and reduces word error rates by 25.45% and 32.1%\nfor moderate and moderate-severe dysarthria speakers using Jasper speech\nrecognition system, respectively. This approach offers promising advancements\nin dysarthric speech reconstruction.\n","authors":["Keshvari Fatemeh","Mahdian Toroghi Rahil","Zareian Hassan"],"pdf_url":"https://arxiv.org/pdf/2410.04092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04091v1","updated":"2024-10-05T09:19:29Z","published":"2024-10-05T09:19:29Z","title":"Cross-Lingual Query-by-Example Spoken Term Detection: A\n  Transformer-Based Approach","summary":"  Query-by-example spoken term detection (QbE-STD) is typically constrained by\ntranscribed data scarcity and language specificity. This paper introduces a\nnovel, language-agnostic QbE-STD model leveraging image processing techniques\nand transformer architecture. By employing a pre-trained XLSR-53 network for\nfeature extraction and a Hough transform for detection, our model effectively\nsearches for user-defined spoken terms within any audio file. Experimental\nresults across four languages demonstrate significant performance gains\n(19-54%) over a CNN-based baseline. While processing time is improved compared\nto DTW, accuracy remains inferior. Notably, our model offers the advantage of\naccurately counting query term repetitions within the target audio.\n","authors":["Allahdadi Fatemeh","Mahdian Toroghi Rahil","Zareian Hassan"],"pdf_url":"https://arxiv.org/pdf/2410.04091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02925v3","updated":"2024-10-05T09:06:11Z","published":"2024-06-05T04:25:56Z","title":"Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech\n  Recognition","summary":"  Synthetic data is widely used in speech recognition due to the availability\nof text-to-speech models, which facilitate adapting models to previously unseen\ntext domains. However, existing methods suffer in performance when they\nfine-tune an automatic speech recognition (ASR) model on synthetic data as they\nsuffer from the distributional shift commonly referred to as the\nsynthetic-to-real gap. In this paper, we find that task vector arithmetic is\neffective at mitigating this gap. Our proposed method, SYN2REAL task vector,\nshows an average improvement of 10.03\\% improvement in word error rate over\nbaselines on the SLURP dataset. Additionally, we show that an average of\nSYN2REAL task vectors, when we have real speeches from multiple different\ndomains, can further adapt the original ASR model to perform better on the\ntarget text domain.\n","authors":["Hsuan Su","Hua Farn","Fan-Yun Sun","Shang-Tse Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.02925v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2310.09505v2","updated":"2024-10-05T08:00:47Z","published":"2023-10-14T06:22:08Z","title":"Advancing Test-Time Adaptation in Wild Acoustic Test Settings","summary":"  Acoustic foundation models, fine-tuned for Automatic Speech Recognition\n(ASR), suffer from performance degradation in wild acoustic test settings when\ndeployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA)\nunder these conditions remains an open and unexplored question. Existing wild\nvision TTA methods often fail to handle speech data effectively due to the\nunique characteristics of high-entropy speech frames, which are unreliably\nfiltered out even when containing crucial semantic content. Furthermore, unlike\nstatic vision data, speech signals follow short-term consistency, requiring\nspecialized adaptation strategies. In this work, we propose a novel wild\nacoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our\nmethod, Confidence-Enhanced Adaptation, performs frame-level adaptation using a\nconfidence-aware weight scheme to avoid filtering out essential information in\nhigh-entropy frames. Additionally, we apply consistency regularization during\ntest-time optimization to leverage the inherent short-term consistency of\nspeech signals. Our experiments on both synthetic and real-world datasets\ndemonstrate that our approach outperforms existing baselines under various wild\nacoustic test settings, including Gaussian noise, environmental sounds, accent\nvariations, and sung speech.\n","authors":["Hongfu Liu","Hengguan Huang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09505v2.pdf","comment":"Accepted to the EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.06995v2","updated":"2024-10-05T07:32:03Z","published":"2024-05-11T12:06:31Z","title":"Benchmarking Cross-Domain Audio-Visual Deception Detection","summary":"  Automated deception detection is crucial for assisting humans in accurately\nassessing truthfulness and identifying deceptive behavior. Conventional\ncontact-based techniques, like polygraph devices, rely on physiological signals\nto determine the authenticity of an individual's statements. Nevertheless,\nrecent developments in automated deception detection have demonstrated that\nmultimodal features derived from both audio and video modalities may outperform\nhuman observers on publicly available datasets. Despite these positive\nfindings, the generalizability of existing audio-visual deception detection\napproaches across different scenarios remains largely unexplored. To close this\ngap, we present the first cross-domain audio-visual deception detection\nbenchmark, that enables us to assess how well these methods generalize for use\nin real-world scenarios. We used widely adopted audio and visual features and\ndifferent architectures for benchmarking, comparing single-to-single and\nmulti-to-single domain generalization performance. To further exploit the\nimpacts using data from multiple source domains for training, we investigate\nthree types of domain sampling strategies, including domain-simultaneous,\ndomain-alternating, and domain-by-domain for multi-to-single domain\ngeneralization evaluation. We also propose an algorithm to enhance the\ngeneralization performance by maximizing the gradient inner products between\nmodality encoders, named ``MM-IDGM\". Furthermore, we proposed the\nAttention-Mixer fusion method to improve performance, and we believe that this\nnew cross-domain benchmark will facilitate future research in audio-visual\ndeception detection.\n","authors":["Xiaobao Guo","Zitong Yu","Nithish Muthuchamy Selvaraj","Bingquan Shen","Adams Wai-Kin Kong","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2405.06995v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.00811v2","updated":"2024-10-05T06:42:40Z","published":"2024-10-01T15:57:35Z","title":"Improving curriculum learning for target speaker extraction with\n  synthetic speakers","summary":"  Target speaker extraction (TSE) aims to isolate individual speaker voices\nfrom complex speech environments. The effectiveness of TSE systems is often\ncompromised when the speaker characteristics are similar to each other. Recent\nresearch has introduced curriculum learning (CL), in which TSE models are\ntrained incrementally on speech samples of increasing complexity. In CL\ntraining, the model is first trained on samples with low speaker similarity\nbetween the target and interference speakers, and then on samples with high\nspeaker similarity. To further improve CL, this paper uses a $k$-nearest\nneighbor-based voice conversion method to simulate and generate speech of\ndiverse interference speakers, and then uses the generated data as part of the\nCL. Experiments demonstrate that training data based on synthetic speakers can\neffectively enhance the model's capabilities and significantly improve the\nperformance of multiple TSE systems.\n","authors":["Yun Liu","Xuechen Liu","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2410.00811v2.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2410.04029v1","updated":"2024-10-05T04:29:55Z","published":"2024-10-05T04:29:55Z","title":"SyllableLM: Learning Coarse Semantic Units for Speech Language Models","summary":"  Language models require tokenized inputs. However, tokenization strategies\nfor continuous data like audio and vision are often based on simple heuristics\nsuch as fixed sized convolutions or discrete clustering, which do not\nnecessarily align with the semantic structure of the data. For speech in\nparticular, the high resolution of waveforms (16,000 samples/second or more)\npresents a significant challenge as speech-based language models have had to\nuse several times more tokens per word than text-based language models. In this\nwork, we introduce a controllable self-supervised technique to merge speech\nrepresentations into coarser syllable-like units while still preserving\nsemantic information. We do this by 1) extracting noisy boundaries through\nanalyzing correlations in pretrained encoder losses and 2) iteratively\nimproving model representations with a novel distillation technique. Our method\nproduces controllable-rate semantic units at as low as 5Hz and 60bps and\nachieves SotA in syllabic segmentation and clustering. Using these coarse\ntokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM)\nthat matches or outperforms current SotA SpeechLMs on a range of spoken\nlanguage modeling tasks. SyllableLM also achieves significant improvements in\nefficiency with a 30x reduction in training compute and a 4x wall-clock\ninference speedup.\n","authors":["Alan Baade","Puyuan Peng","David Harwath"],"pdf_url":"https://arxiv.org/pdf/2410.04029v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.04951v2","updated":"2024-10-05T04:01:02Z","published":"2024-06-07T14:13:20Z","title":"The Database and Benchmark for the Source Speaker Tracing Challenge 2024","summary":"  Voice conversion (VC) systems can transform audio to mimic another speaker's\nvoice, thereby attacking speaker verification (SV) systems. However, ongoing\nstudies on source speaker verification (SSV) are hindered by limited data\navailability and methodological constraints. This paper presents the Source\nSpeaker Tracking Challenge (SSTC) on STL 2024, which aims to fill the gap in\nthe database and benchmark for the SSV task. In this study, we generate a\nlarge-scale converted speech database with 16 common VC methods and train a\nbatch of baseline systems based on the MFA-Conformer architecture. In addition,\nwe introduced a related task called conversion method recognition, with the aim\nof assisting the SSV task. We expect SSTC to be a platform for advancing the\ndevelopment of the SSV task and provide further insights into the performance\nand limitations of current SV systems against VC attacks. Further details about\nSSTC can be found in https://sstc-challenge.github.io/.\n","authors":["Ze Li","Yuke Lin","Tian Yao","Hongbin Suo","Pengyuan Zhang","Yanzhen Ren","Zexin Cai","Hiromitsu Nishizaki","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2406.04951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04017v1","updated":"2024-10-05T03:20:20Z","published":"2024-10-05T03:20:20Z","title":"Adversarial Attacks and Robust Defenses in Speaker Embedding based\n  Zero-Shot Text-to-Speech System","summary":"  Speaker embedding based zero-shot Text-to-Speech (TTS) systems enable\nhigh-quality speech synthesis for unseen speakers using minimal data. However,\nthese systems are vulnerable to adversarial attacks, where an attacker\nintroduces imperceptible perturbations to the original speaker's audio\nwaveform, leading to synthesized speech sounds like another person. This\nvulnerability poses significant security risks, including speaker identity\nspoofing and unauthorized voice manipulation. This paper investigates two\nprimary defense strategies to address these threats: adversarial training and\nadversarial purification. Adversarial training enhances the model's robustness\nby integrating adversarial examples during the training process, thereby\nimproving resistance to such attacks. Adversarial purification, on the other\nhand, employs diffusion probabilistic models to revert adversarially perturbed\naudio to its clean form. Experimental results demonstrate that these defense\nmechanisms can significantly reduce the impact of adversarial perturbations,\nenhancing the security and reliability of speaker embedding based zero-shot TTS\nsystems in adversarial environments.\n","authors":["Ze Li","Yao Shi","Yunfei Xu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2410.04017v1.pdf","comment":null}]}}